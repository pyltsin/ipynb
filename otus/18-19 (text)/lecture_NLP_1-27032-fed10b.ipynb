{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='otus.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Анализ текстовых данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Какие задачи можно решать, обрабатывая текст?\n",
    "\"Мама мыла раму, и теперь она блестит\"  \n",
    "\"Мама мыла раму, и теперь она сильно устала\"  \n",
    "\n",
    "\"Кубок не помещался в чемодан, потому что он был слишком велик. Что именно было слишком велико, чемодан или кубок?\"\n",
    "\n",
    "http://commonsensereasoning.org/winograd.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. синтаксические задачи\n",
    "  * разметка по частям речи и по морфологическим признакам\n",
    "  * деление слов в тексте на морфемы (суффикс, приставка и пр.)\n",
    "  * стемминг, лемматизация (?)\n",
    "  * деление на предложения (инициалы и сокращения) и слова (китайский язык)\n",
    "  * поиск имен и названий в тексте - сущностей\n",
    "  * разрешение смысла слов в заданном контексте (замок)\n",
    "  * построить синтаксическое дерево\n",
    "  * определение того, к каким другим объектам относится слово\n",
    "2. задачи на понимание текста, в которых есть \"учитель\"\n",
    "  * предсказание следующего символа\n",
    "  * информационный поиск\n",
    "  * анализ тональности\n",
    "  * выделение отношений и фактов\n",
    "  * ответы на вопросы\n",
    "3. понимание и порождение текста (оценка качества?)\n",
    "  * порождение текста\n",
    "  * автоматическое реферирование\n",
    "  * машинный перевод\n",
    "  * диалоговые модели (чат-бот)\n",
    "  \n",
    "Косвенные задачи:\n",
    "  * описание изображения\n",
    "  * распознавание речи\n",
    "  \n",
    "**Задачи бизнеса**:\n",
    "  * распознавание речи (помощник)\n",
    "  * чат-бот (замена техподдержки в решении большинства вопросов)\n",
    "  * поиск точного ответа на вопрос в базе документов (например, база стандартов)\n",
    "  * оценка мнения в социальных сетях о продукте\n",
    "  * ... (ваши варианты?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download()  # download lots of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# От текста к простым моделям"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разбиение на токены\n",
    "**Def.**  \n",
    "разбиение последовательности символов на части (токены), возможно, исключая из рассмотрения некоторые символы  \n",
    "Наивный подход: разделить строку пробелами и выкинуть знаки препинания  \n",
    "\n",
    "\n",
    "*Трисия любила Нью-Йорк, поскольку любовь к Нью-Йорку могла положительно повлиять на ее карьеру.*  \n",
    "\n",
    "\n",
    "**Проблемы:**  \n",
    "* my.email@mail.ru, 127.0.0.1\n",
    "* С++, C#\n",
    "* York University vs New York University\n",
    "* Зависимость от языка (“Lebensversicherungsgesellschaftsangestellter”, “l’amour”)\n",
    "Альтернатива: n-граммы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.nltk.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Трисия ::\n",
      "любила ::\n",
      "Нью ::\n",
      "- ::\n",
      "Йорк ::\n",
      ", ::\n",
      "поскольку ::\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer('\\w+|[^\\w\\s]+')\n",
    "s = u'Трисия любила Нью-Йорк, поскольку любовь к Нью-Йорку могла положительно повлиять на ее карьеру.'\n",
    "for t in tokenizer.tokenize(s)[:7]: \n",
    "    print(t + \" ::\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ftfy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-891253d0469b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mftfy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfix_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfix_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'\\001\\033[36;44mI&#x92;m blue, da ba dee da ba doo&#133;\\033[0m'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'NFKC'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ftfy'"
     ]
    }
   ],
   "source": [
    "from ftfy import fix_text\n",
    "print(fix_text(u'\\001\\033[36;44mI&#x92;m blue, da ba dee da ba doo&#133;\\033[0m', normalization='NFKC'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стоп-слова\n",
    "**Def.**  \n",
    "Наиболее частые слова в языке, не содержащие никакой информации о содержании текста\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "и в во не что он на я с со как а то все она так его но да ты\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(' '.join(stopwords.words('russian')[:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблема: “To be or not to be\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нормализация\n",
    "**Def.**  \n",
    "Приведение токенов к единому виду для того, чтобы избавиться от поверхностной разницы в написании  \n",
    "\n",
    "Подходы  \n",
    "* сформулировать набор правил, по которым преобразуется токен  \n",
    "Нью-Йорк → нью-йорк → ньюйорк → ньюиорк\n",
    "* явно хранить связи между токенами (WordNet – Princeton)  \n",
    "машина → автомобиль, Windows 6→ window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "нью-йорк\n"
     ]
    }
   ],
   "source": [
    "s = u'Нью-Йорк'\n",
    "s1 = s.lower()\n",
    "print(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "нью йорк\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "s2 = re.sub(r\"\\W\", \" \", s1, flags=re.U)\n",
    "print(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "нью иорк\n"
     ]
    }
   ],
   "source": [
    "s3 = re.sub(r\"й\", u\"и\", s2, flags=re.U)\n",
    "print(s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стемминг и Лемматизация\n",
    "**Def.**  \n",
    "Приведение грамматических форм слова и однокоренных слов к единой основе (lemma):\n",
    "* Stemming – с помощью простых эвристических правил\n",
    "  * Porter (Cambridge – 1980)\n",
    "        5 этапов, на каждом применяется набор правил, таких как\n",
    "            sses → ss (caresses → caress)\n",
    "            ies → i (ponies → poni)\n",
    "\n",
    "  * Lovins (1968)\n",
    "  * Paice (1990)\n",
    "  * другие\n",
    "* Lemmatization – с использованием словарей и морфологического анализа преобразование к нормальной форме\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стемминг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token\n",
      "stem\n",
      "авиац\n",
      "национальн\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import PorterStemmer\n",
    "s = PorterStemmer()\n",
    "print(s.stem('Tokenization'))\n",
    "print( s.stem('stemming'))\n",
    "\n",
    "from nltk.stem.snowball import RussianStemmer\n",
    "r = RussianStemmer()\n",
    "print (r.stem(u'Авиация'))\n",
    "print (r.stem(u'национальный'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Наблюдение**  \n",
    "для сложных языков лучше подходит лемматизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse(word='замок', tag=OpencorporaTag('NOUN,inan,masc sing,nomn'), normal_form='замок', score=0.3333333333333333, methods_stack=((<DictionaryAnalyzer>, 'замок', 139, 0),))\n",
      "замок замок\n",
      "Parse(word='замок', tag=OpencorporaTag('NOUN,inan,masc sing,accs'), normal_form='замок', score=0.3333333333333333, methods_stack=((<DictionaryAnalyzer>, 'замок', 139, 3),))\n",
      "замок замок\n",
      "Parse(word='замок', tag=OpencorporaTag('VERB,perf,intr masc,sing,past,indc'), normal_form='замокнуть', score=0.3333333333333333, methods_stack=((<DictionaryAnalyzer>, 'замок', 730, 1),))\n",
      "замок замокнуть\n"
     ]
    }
   ],
   "source": [
    "import pymorphy2\n",
    "\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "for i in morph.parse(u'замок'):\n",
    "    print(i)\n",
    "    print(i.word, i.normal_form)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Представление документов\n",
    "**Boolean Model.** Присутствие или отсутствие слова в документе  \n",
    "**Bag of Words.** Порядок токенов не важен - матрица док-ты vs токены, в ячейках - встречаемость токена в док-те \n",
    "\n",
    "*Погода была ужасная, принцесса была прекрасная.\n",
    "Или все было наоборот?*\n",
    "\n",
    "Координаты\n",
    "* Мультиномиальные: количество токенов в документе\n",
    "* Числовые: взвешенное количество токенов в документе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 0., 1.],\n",
       "       [0., 1., 3.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = DictVectorizer(sparse=False)\n",
    "D = [{'foo': 1, 'bar': 2}, {'foo': 3, 'baz': 1}]\n",
    "v.fit(D)\n",
    "X = v.transform(D)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bar': 2.0, 'foo': 1.0}, {'baz': 1.0, 'foo': 3.0}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.inverse_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 4.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.transform({'foo': 4, 'unseen_feature': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Counter({'thank': 1, '40': 1, ',': 1, 'mr': 1, 'president': 1, '.': 1}),\n",
       " Counter({'madam': 1,\n",
       "          'president': 1,\n",
       "          ',': 3,\n",
       "          'agree': 1,\n",
       "          'recognise': 1,\n",
       "          'turkey': 2,\n",
       "          \"'\": 1,\n",
       "          'european': 1,\n",
       "          'prospects': 2,\n",
       "          'auspicious': 1,\n",
       "          'outcome': 1,\n",
       "          'needs': 1,\n",
       "          ':': 1}),\n",
       " Counter({'madam': 1,\n",
       "          'president': 1,\n",
       "          ',': 2,\n",
       "          'firstly': 1,\n",
       "          'would': 1,\n",
       "          'like': 1,\n",
       "          'express': 1,\n",
       "          'sincerest': 1,\n",
       "          'thanks': 1,\n",
       "          'high': 1,\n",
       "          'representative': 1,\n",
       "          'including': 1,\n",
       "          'important': 1,\n",
       "          'issue': 1,\n",
       "          'agenda': 1,\n",
       "          'early': 1,\n",
       "          'stage': 1,\n",
       "          '.': 1})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "docs = [\n",
    "    \"Thank 40 you, Mr President.\",\n",
    "    \"Madam President, I agree and recognise Turkey's European prospects, but if these prospects are to have an auspicious outcome, Turkey needs to:\",\n",
    "    \"Madam President, firstly, I would like to express my sincerest thanks to the High Representative for including this important issue in the agenda at such an early stage.\",\n",
    "]\n",
    "\n",
    "tokenizer = RegexpTokenizer('\\w+|[^\\w\\s]+')\n",
    "stopwords_eng = stopwords.words()\n",
    "\n",
    "document_bags = list()\n",
    "\n",
    "for d in docs:\n",
    "    bag = Counter()\n",
    "    text = d.lower()\n",
    "\n",
    "    for t in tokenizer.tokenize(text):     \n",
    "        if t in stopwords_eng:\n",
    "            continue\n",
    "            \n",
    "        bag[t] += 1\n",
    "    document_bags.append(bag)\n",
    "    \n",
    "document_bags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 31)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = DictVectorizer(sparse=False)\n",
    "X = v.fit_transform(document_bags)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'\",\n",
       " ',',\n",
       " '.',\n",
       " '40',\n",
       " ':',\n",
       " 'agenda',\n",
       " 'agree',\n",
       " 'auspicious',\n",
       " 'early',\n",
       " 'european',\n",
       " 'express',\n",
       " 'firstly',\n",
       " 'high',\n",
       " 'important',\n",
       " 'including',\n",
       " 'issue',\n",
       " 'like',\n",
       " 'madam',\n",
       " 'mr',\n",
       " 'needs',\n",
       " 'outcome',\n",
       " 'president',\n",
       " 'prospects',\n",
       " 'recognise',\n",
       " 'representative',\n",
       " 'sincerest',\n",
       " 'stage',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'turkey',\n",
       " 'would']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         1],\n",
       "        [0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "         0, 1, 0, 0, 1, 1, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 2, 0,\n",
       "         0],\n",
       "        [0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "         1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 2, 0, 1, 2, 0, 1,\n",
       "         0]], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit_transform(docs).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'40': 0,\n",
       " 'agenda': 1,\n",
       " 'agree': 2,\n",
       " 'an': 3,\n",
       " 'and': 4,\n",
       " 'are': 5,\n",
       " 'at': 6,\n",
       " 'auspicious': 7,\n",
       " 'but': 8,\n",
       " 'early': 9,\n",
       " 'european': 10,\n",
       " 'express': 11,\n",
       " 'firstly': 12,\n",
       " 'for': 13,\n",
       " 'have': 14,\n",
       " 'high': 15,\n",
       " 'if': 16,\n",
       " 'important': 17,\n",
       " 'in': 18,\n",
       " 'including': 19,\n",
       " 'issue': 20,\n",
       " 'like': 21,\n",
       " 'madam': 22,\n",
       " 'mr': 23,\n",
       " 'my': 24,\n",
       " 'needs': 25,\n",
       " 'outcome': 26,\n",
       " 'president': 27,\n",
       " 'prospects': 28,\n",
       " 'recognise': 29,\n",
       " 'representative': 30,\n",
       " 'sincerest': 31,\n",
       " 'stage': 32,\n",
       " 'such': 33,\n",
       " 'thank': 34,\n",
       " 'thanks': 35,\n",
       " 'the': 36,\n",
       " 'these': 37,\n",
       " 'this': 38,\n",
       " 'to': 39,\n",
       " 'turkey': 40,\n",
       " 'would': 41,\n",
       " 'you': 42}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "Количество вхождений слова $t$ в документе $d$\n",
    "$$\n",
    "TF_{t,d} = term\\!\\!-\\!\\!frequency(t, d)\n",
    "$$\n",
    "Количество документов из $N$ возможных, где встречается $t$\n",
    "$$\n",
    "DF_t = document\\!\\!-\\!\\!fequency(t)\n",
    "$$\n",
    "$$\n",
    "IDF_t = inverse\\!\\!-\\!\\!document\\!\\!-\\!\\!frequency(t) = \\log \\frac{N}{DF_t}\n",
    "$$\n",
    "TF-IDF\n",
    "$$\n",
    "TF\\!\\!-\\!\\!IDF_{t,d} = TF_{t,d} \\times IDF_t\n",
    "$$\n",
    "\n",
    "Оценивает важность слова в контексте документа, являющегося частью корпуса\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.47952794, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.47952794, 0.        ,\n",
       "         0.        , 0.        , 0.28321692, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.47952794,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.47952794],\n",
       "        [0.        , 0.        , 0.20489728, 0.15582966, 0.20489728,\n",
       "         0.20489728, 0.        , 0.20489728, 0.20489728, 0.        ,\n",
       "         0.20489728, 0.        , 0.        , 0.        , 0.20489728,\n",
       "         0.        , 0.20489728, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.15582966, 0.        , 0.        ,\n",
       "         0.20489728, 0.20489728, 0.12101563, 0.40979456, 0.20489728,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.20489728, 0.        , 0.31165933,\n",
       "         0.40979456, 0.        , 0.        ],\n",
       "        [0.        , 0.18959527, 0.        , 0.14419209, 0.        ,\n",
       "         0.        , 0.18959527, 0.        , 0.        , 0.18959527,\n",
       "         0.        , 0.18959527, 0.18959527, 0.18959527, 0.        ,\n",
       "         0.18959527, 0.        , 0.18959527, 0.18959527, 0.18959527,\n",
       "         0.18959527, 0.18959527, 0.14419209, 0.        , 0.18959527,\n",
       "         0.        , 0.        , 0.11197802, 0.        , 0.        ,\n",
       "         0.18959527, 0.18959527, 0.18959527, 0.18959527, 0.        ,\n",
       "         0.18959527, 0.37919054, 0.        , 0.18959527, 0.28838418,\n",
       "         0.        , 0.18959527, 0.        ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "features = vectorizer.fit_transform(docs).todense()\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thank': 34,\n",
       " '40': 0,\n",
       " 'you': 42,\n",
       " 'mr': 23,\n",
       " 'president': 27,\n",
       " 'madam': 22,\n",
       " 'agree': 2,\n",
       " 'and': 4,\n",
       " 'recognise': 29,\n",
       " 'turkey': 40,\n",
       " 'european': 10,\n",
       " 'prospects': 28,\n",
       " 'but': 8,\n",
       " 'if': 16,\n",
       " 'these': 37,\n",
       " 'are': 5,\n",
       " 'to': 39,\n",
       " 'have': 14,\n",
       " 'an': 3,\n",
       " 'auspicious': 7,\n",
       " 'outcome': 26,\n",
       " 'needs': 25,\n",
       " 'firstly': 12,\n",
       " 'would': 41,\n",
       " 'like': 21,\n",
       " 'express': 11,\n",
       " 'my': 24,\n",
       " 'sincerest': 31,\n",
       " 'thanks': 35,\n",
       " 'the': 36,\n",
       " 'high': 15,\n",
       " 'representative': 30,\n",
       " 'for': 13,\n",
       " 'including': 19,\n",
       " 'this': 38,\n",
       " 'important': 17,\n",
       " 'issue': 20,\n",
       " 'in': 18,\n",
       " 'agenda': 1,\n",
       " 'at': 6,\n",
       " 'such': 33,\n",
       " 'early': 9,\n",
       " 'stage': 32}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.stop_words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thank 40 you, Mr President.',\n",
       " \"Madam President, I agree and recognise Turkey's European prospects, but if these prospects are to have an auspicious outcome, Turkey needs to:\",\n",
       " 'Madam President, firstly, I would like to express my sincerest thanks to the High Representative for including this important issue in the agenda at such an early stage.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5), (23, 0.5), (34, 0.5), (42, 0.5)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# https://radimrehurek.com/gensim/\n",
    "import gensim\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "x = vectorizer.fit_transform(docs).todense()\n",
    "\n",
    "corpus = [list(filter(lambda x: x[1] != 0, enumerate(np.asarray(row)[0]))) for row in x]\n",
    "tfidf = TfidfModel(corpus)\n",
    "print(tfidf[corpus[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Байесовский классификатор\n",
    "\n",
    "Дано\n",
    "\n",
    "$\\mathbf{x} \\in X$ - описание документа $d$ из коллекции $D$  \n",
    "$C_k \\in C, \\; k = 1,\\ldots,K$ - целевая переменная\n",
    "\n",
    "Теорема Байеса\n",
    "$$\n",
    "P(C_k \\mid \\mathbf{x}) = \\frac{p(\\mathbf{x} \\mid C_k) p(C_k)}{p(\\mathbf{x})} \\propto p(\\mathbf{x} \\mid C_k) p(C_k)\n",
    "$$\n",
    "\n",
    "Принцип Maximum A-Posteriori\n",
    "$$\n",
    "C_{MAP} = \\arg \\max_k p(C_k | \\mathbf{x})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Байесовский классификатор — широкий класс алгоритмов классификации, основанный на принципе максимума апостериорной вероятности.  \n",
    "Для классифицируемого объекта вычисляются функции правдоподобия каждого из классов, по ним вычисляются апостериорные вероятности классов.  \n",
    "Объект относится к тому классу, для которого апостериорная вероятность максимальна."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Naive Bayes\n",
    "\n",
    "$x_j$ - слово на $j$-м месте в документе $\\mathbf{x}$,  \n",
    "$w^i \\in V$ - слово из словаря $V$\n",
    "\n",
    "\n",
    "Предположения\n",
    "* conditional independence - слова внутри документа независимы\n",
    "$$\n",
    "p(x_i=w^s, x_j=w^r | C_k) = p(x_i=w^s | C_k) p(x_j=w^r | C_k)\n",
    "$$\n",
    "* postional independence - результат не зависит от позиции слова в документе\n",
    "$$\n",
    "P(x_i=w^s | C_k) = P(x_j=w^s | C_k) = P(x = w^s | C_k)\n",
    "$$\n",
    "\n",
    "Получаем\n",
    "$$\n",
    "p(\\mathbf{x} | C_k) = p(x_1=w^{s_1}, \\ldots, x_{|\\mathbf{x}|}=w^{s_{|\\mathbf{x}|}} | C_k) = \\prod_{i=1}^{|\\mathbf{x}|} p(x = w^{s_i} | C_k)\n",
    "$$\n",
    "\n",
    "**Почему NB хорошо работает?**  \n",
    "Корректная оценка дает правильное предсказание, но правильное предсказание *не требует* корректной оценки\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Варианты NB\n",
    "\n",
    "MAP\n",
    "$$\n",
    "C_{MAP} = \\arg \\max_k p(C_k) \\prod_{i=1}^{|\\mathbf{x}|} p(x = w^{s_i} | C_k)  = \n",
    "$$\n",
    "$$\n",
    "= \\arg \\max_k \\left[ \\log p(C_k) + \\sum_{i=1}^{|\\mathbf{x}|} \\log p(x = w^{s_i} | C_k) \\right]\n",
    "$$\n",
    "Априорные вероятности\n",
    "$$\n",
    "p(C_k) = N_{C_k}/{N}\n",
    "$$\n",
    "Likelihood $p(x = w^{s_i} | C_k)$\n",
    "* BernoulliNB $p(x = w^{s_i} | C_k) = D_{w^{s_i}, C_k} / D_{C_k}$, $D$ - кол-во документов\n",
    "* MultinomialNB $p(x = w^{s_i} | C_k) = T_{w^{s_i}, C_k} / T_{C_k}$, $T$ - кол-во токенов\n",
    "* GaussianNB $p(x = w^{s_i} | C_k) = \\mathcal{N}(\\mu_k, \\sigma_k^2)$, параметры из MLE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение NB\n",
    "\n",
    "```\n",
    "function nb_train(D,C):\n",
    "\tV = dictionary of tokens\n",
    "\tN = number of documents\n",
    "\tfor Ck in C: # iterate over all classes\n",
    "\t\tN_Ck = number of documents in class Ck\n",
    "\t\tp(Ck) = N_Ck / N # Class prior\n",
    "\t\tD_Ck = Documents in class Ck\t\t\n",
    "\t\tfor w_i in V:\t\t\t\n",
    "\t\t\t# multinomial, bernoulli, gaussian\n",
    "\t\t\tp(w_i|Ck) = count_likelihood(...)\n",
    "\treturn V, p(Ck), p(w_i|Ck)\n",
    "```\n",
    "\n",
    "Алгоритмическая сложность: $O(|D| \\langle |\\mathbf{x}| \\rangle + |C||V|)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Применение MultinomialNB\n",
    "\n",
    "\n",
    "```\n",
    "function nb_apply(d, C, V, p(Ck), p(w_i|Ck)):\n",
    "\tx = tokenize(d) # somehow\t\n",
    "\tfor Ck in C: # iterate over all classes\n",
    "\t\tscore(Ck|x) = log p(Ck) # use class prior\n",
    "\t\t# use likelihoods\n",
    "\t\tfor i in 1..|x|:\t\t\n",
    "\t\t\tscore(Ck|x) += log p(x_i|Ck)\n",
    "\treturn arg max score(Ck|x)\n",
    "```\n",
    "\n",
    "Алгоритмическая сложность: $O(|C||\\mathbf{x}|)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сглаживание\n",
    "\n",
    "Проблема: $p(свинки|мимими) = 0$\n",
    "\n",
    "Решение:\n",
    "\n",
    "$$ p(x=w_{s_i}|C_k) = \\frac{ T_{w^{s_i}, C_k} + \\alpha }{ T_{C_k} + \\alpha|V|} $$\n",
    "\n",
    "\n",
    "если $\\alpha \\geq 0$ - сглаживание Лапласа, если $0 \\leq \\alpha \\leq 1$ - Лидстоуна\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**+** (Удивительно) неплохо работает  \n",
    "**+** Стабилен при смещении выборки  \n",
    "**+** Оптимальный по производительности  \n",
    "\n",
    "**-** Наивные предположения  \n",
    "**-** Требует отбора признаков  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMS Spam  detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/uciml/sms-spam-collection-dataset/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'spam.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-6fd3948980f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'spam.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'latin-1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\javasdk\\anakonda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\javasdk\\anakonda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\javasdk\\anakonda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\javasdk\\anakonda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\javasdk\\anakonda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'spam.csv' does not exist"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('spam.csv', usecols=[0, 1], encoding='latin-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "y = pd.get_dummies(df['v1'])['spam']\n",
    "X = vectorizer.fit_transform(df['v2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method IndexOpsMixin.value_counts of 0        ham\n",
       "1        ham\n",
       "2       spam\n",
       "3        ham\n",
       "4        ham\n",
       "5       spam\n",
       "6        ham\n",
       "7        ham\n",
       "8       spam\n",
       "9       spam\n",
       "10       ham\n",
       "11      spam\n",
       "12      spam\n",
       "13       ham\n",
       "14       ham\n",
       "15      spam\n",
       "16       ham\n",
       "17       ham\n",
       "18       ham\n",
       "19      spam\n",
       "20       ham\n",
       "21       ham\n",
       "22       ham\n",
       "23       ham\n",
       "24       ham\n",
       "25       ham\n",
       "26       ham\n",
       "27       ham\n",
       "28       ham\n",
       "29       ham\n",
       "        ... \n",
       "5542     ham\n",
       "5543     ham\n",
       "5544     ham\n",
       "5545     ham\n",
       "5546     ham\n",
       "5547    spam\n",
       "5548     ham\n",
       "5549     ham\n",
       "5550     ham\n",
       "5551     ham\n",
       "5552     ham\n",
       "5553     ham\n",
       "5554     ham\n",
       "5555     ham\n",
       "5556     ham\n",
       "5557     ham\n",
       "5558     ham\n",
       "5559     ham\n",
       "5560     ham\n",
       "5561     ham\n",
       "5562     ham\n",
       "5563     ham\n",
       "5564     ham\n",
       "5565     ham\n",
       "5566    spam\n",
       "5567    spam\n",
       "5568     ham\n",
       "5569     ham\n",
       "5570     ham\n",
       "5571     ham\n",
       "Name: v1, Length: 5572, dtype: object>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['v1'].value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomized_cv(model, param_grid, x_train, y_train):\n",
    "    grid_search = RandomizedSearchCV(model, param_grid, cv=5, scoring='accuracy', n_iter=10)\n",
    "    t_start = time.time()\n",
    "    grid_search.fit(x_train, y_train)\n",
    "    t_end = time.time()\n",
    "    print('model {} best accuracy score is {}'.format(model.__class__.__name__, grid_search.best_score_))\n",
    "    print('time for training is {} seconds'.format(t_end - t_start))\n",
    "    return grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model MultinomialNB best accuracy score is 0.981516206804179\n",
      "time for training is 0.2098085880279541 seconds\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'alpha':[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.5, 2, 5]}\n",
    "model = MultinomialNB()\n",
    "best_model = randomized_cv(model, param_grid, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.980967917346\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 1, 0, 0, 0, 0, 0], dtype=uint8)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.predict(X_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['no', 'just', 'teaches', 'choose', 'funny', 'wife', 'tsunamis',\n",
       "        'nobody', 'fact', 'hw', 'natural', 'happens', 'volcanoes', 'erupt',\n",
       "        'arise', 'hurricanes', 'sway', 'aroundn', 'disasters'], \n",
       "       dtype='<U34'),\n",
       " array(['to', 'think', 'and', 'you', 'for', 'my', 'is', 'me', 'had', 'the',\n",
       "        'on', 'cost', 'if', 'are', 'do', 'of', 'her', 'joke', 'one',\n",
       "        'research', 'contact', 'school', 'sent', 'also', 'few', 'thinking',\n",
       "        'less', 'schools', 'ones', 'scores', 'sophas', 'secondary',\n",
       "        'application', 'applying', 'ogunrinde', 'expensive'], \n",
       "       dtype='<U34'),\n",
       " array(['to', 'you', 'call', '150p', 'pobox', 'that', 'we', 'know', 'out',\n",
       "        'who', 'find', 'someone', 'fancies', '09058097218', 'ls15hb'], \n",
       "       dtype='<U34'),\n",
       " array(['only', 'in', 'ok', 'to', 'text', 'it', 'and', 'you', 'me', 'as',\n",
       "        'your', 'the', 'soon', 'promise', 'if', 'can', 'getting', 'll',\n",
       "        'let', 'know', 'out', 'morning', 'made'], \n",
       "       dtype='<U34'),\n",
       " array(['free', 'entry', 'to', 'txt', '100', 'our', 'www', 'ur', 'of',\n",
       "        'awarded', 'draw', '500', 'congratulations', 'cd', 'vouchers',\n",
       "        'music', '87066', 'tncs', 'ldew', 'com1win150ppmx3age16', 'weekly',\n",
       "        'either', 'gift'], \n",
       "       dtype='<U34'),\n",
       " array(['text', 'and', 'you', 'on', 'll', 'let', 'know', 'carlos', 'hang'], \n",
       "       dtype='<U34'),\n",
       " array(['now', 'you', 'did', 'are', 'see', 'where'], \n",
       "       dtype='<U34'),\n",
       " array(['no', 'message', 'what', 'responce', 'happend'], \n",
       "       dtype='<U34'),\n",
       " array(['in', 'to', 'and', 'right', 'at', 'first', 'down', 'turn', 'lt',\n",
       "        'gt', 'get', 'cut', 'gandhipuram', 'walk', 'cross', 'road', 'side',\n",
       "        'street'], \n",
       "       dtype='<U34'),\n",
       " array(['you', 'your', 'yet', 'shit', 'flippin'], \n",
       "       dtype='<U34')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.inverse_transform(X_test[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://code.google.com/archive/p/word2vec/#Pre-trained_word_and_phrase_vectors\n",
    "\n",
    "https://www.youtube.com/watch?v=EpJzLN8LL7Q&t=43s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'freebase-vectors-skipgram1000-en.bin.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-42106550b8fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"freebase-vectors-skipgram1000-en.bin.gz\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'vacation'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\javasdk\\anakonda\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m   1474\u001b[0m         return _load_word2vec_format(\n\u001b[0;32m   1475\u001b[0m             \u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1476\u001b[1;33m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[0;32m   1477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1478\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\javasdk\\anakonda\\lib\\site-packages\\gensim\\models\\utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loading projection weights from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m         \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# throws for invalid file format\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\javasdk\\anakonda\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[1;34m(uri, mode, **kw)\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m         \u001b[0mbinary_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m     \u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open_binary_stream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muri\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary_mode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mignore_extension\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[0mdecompressed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\javasdk\\anakonda\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36m_open_binary_stream\u001b[1;34m(uri, mode, **kw)\u001b[0m\n\u001b[0;32m    333\u001b[0m             \u001b[1;31m# local files -- both read & write supported\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m             \u001b[1;31m# compression, if any, is determined by the filename extension (.gz, .bz2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m             \u001b[0mfobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    336\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mparsed_uri\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscheme\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msmart_open_s3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSUPPORTED_SCHEMES\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'freebase-vectors-skipgram1000-en.bin.gz'"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "fn = \"freebase-vectors-skipgram1000-en.bin.gz\"\n",
    "model = KeyedVectors.load_word2vec_format(fn)\n",
    "model.most_similar('vacation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://dumps.wikimedia.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ruwiki-20171220-pages-articles-multistream.xml.bz2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-fa8582bb6e5d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwikicorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWikiCorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mwiki\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWikiCorpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ruwiki-20171220-pages-articles-multistream.xml.bz2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\javasdk\\anakonda\\lib\\site-packages\\gensim\\corpora\\wikicorpus.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fname, processes, lemmatize, dictionary, filter_namespaces, tokenizer_func, article_min_tokens, token_min_len, token_max_len, lower, filter_articles)\u001b[0m\n\u001b[0;32m    634\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 636\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_texts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    637\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\javasdk\\anakonda\\lib\\site-packages\\gensim\\corpora\\dictionary.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, documents, prune_at)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_documents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprune_at\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprune_at\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\javasdk\\anakonda\\lib\\site-packages\\gensim\\corpora\\dictionary.py\u001b[0m in \u001b[0;36madd_documents\u001b[1;34m(self, documents, prune_at)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \"\"\"\n\u001b[1;32m--> 197\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mdocno\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocument\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m             \u001b[1;31m# log progress & run a regular check for pruning, once every 10k docs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdocno\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m10000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\javasdk\\anakonda\\lib\\site-packages\\gensim\\corpora\\wikicorpus.py\u001b[0m in \u001b[0;36mget_texts\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m             ((text, self.lemmatize, title, pageid, tokenization_params)\n\u001b[0;32m    677\u001b[0m              \u001b[1;32mfor\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpageid\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m              in extract_pages(bz2.BZ2File(self.fname), self.filter_namespaces, self.filter_articles))\n\u001b[0m\u001b[0;32m    679\u001b[0m         \u001b[0mpool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_to_ignore_interrupt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\javasdk\\anakonda\\lib\\bz2.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filename, mode, buffering, compresslevel)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_closefp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode_code\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ruwiki-20171220-pages-articles-multistream.xml.bz2'"
     ]
    }
   ],
   "source": [
    "from gensim.corpora.wikicorpus import WikiCorpus\n",
    "wiki = WikiCorpus('ruwiki-20171220-pages-articles-multistream.xml.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phraser, Phrases\n",
    "bigram = Phrases(wiki.get_texts())\n",
    "bigram_transformer = Phraser(bigram)\n",
    "\n",
    "\n",
    "def text_generator_bigram():\n",
    "    for text in wiki.get_texts():\n",
    "        yield bigram_transformer[[word.decode('utf8') for word in text]]\n",
    "        \n",
    "        \n",
    "def text_generator_trigram():\n",
    "    for text in wiki.get_texts():\n",
    "        yield trigram_transformer[bigram_transformer[[word.decode('utf8') for word in text]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "model = Word2Vec(size=100, window=7, min_count=10, workers=10)\n",
    "model.build_vocab(text_generator_trigram())\n",
    "model.train(text_generator_trigram())\n",
    "\n",
    "fname = 'w2v_model_wiki'\n",
    "model.save(fname)\n",
    "model = Word2Vec.load(fname)\n",
    "model.most_similar('токен')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработка текста для Java:  \n",
    "https://stanfordnlp.github.io/CoreNLP/index.html  \n",
    "https://opennlp.apache.org/docs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpaCy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free PERSON\n",
      "2 CARDINAL\n",
      "FA Cup EVENT\n",
      "21st ORDINAL\n",
      "May 2005 DATE\n",
      "87121 DATE\n",
      "rate)T&C ORG\n",
      "08452810075over18 PERSON\n",
      "Nah PERSON\n",
      "3 week's DATE\n",
      "Melle Melle PERSON\n",
      "Oru Minnaminunginte Nurungu Vettam PERSON\n",
      "9 CARDINAL\n",
      "KL341 CARDINAL\n",
      "Valid FAC\n",
      "12 hours TIME\n",
      "11 months DATE\n",
      "U R ORG\n",
      "Update GPE\n",
      "The Mobile Update Co FREE ORG\n",
      "08002986030 DATE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "#python -m spacy download en\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# Process whole documents\n",
    "\n",
    "text = '. '.join(df['v2'][:10])\n",
    "doc = nlp(text)\n",
    "\n",
    "# Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat.... Ok lar... Joking wif u oni.... Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's. U dun say so early hor... U c already then say.... Nah I don't think he goes to usf, he lives around here though. FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, å£1.50 to rcv. Even my brother is not like to speak with me. They treat me like aids patent.. As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune. WINNER!! As a valued network customer you have been selected to receivea å£900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.. Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42931574228979374"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine semantic similarities\n",
    "doc1 = nlp(u'the fries were gross')\n",
    "doc2 = nlp(u'worst potato ever')\n",
    "doc1.similarity(doc2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86696618402938308"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine semantic similarities\n",
    "doc1 = nlp(u'men')\n",
    "doc2 = nlp(u'women')\n",
    "doc1.similarity(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
