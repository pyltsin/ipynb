{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<img src='otus.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Анализ текстовых данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Николенко, Кадурин, Архангельская. **Глубокое обучение. Погружение в мир нейронных сетей**. Глава 7.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Какие задачи можно решать, обрабатывая текст?\n",
    "\"Мама мыла раму, и теперь она блестит\"  \n",
    "\"Мама мыла раму, и теперь она сильно устала\"  \n",
    "\n",
    "\"Кубок не помещался в чемодан, потому что он был слишком велик. Что именно было слишком велико, чемодан или кубок?\"\n",
    "\n",
    "http://commonsensereasoning.org/winograd.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. синтаксические задачи\n",
    "  * разметка по частям речи и по морфологическим признакам\n",
    "  * деление слов в тексте на морфемы (суффикс, приставка и пр.)\n",
    "  * стемминг, лемматизация (?)\n",
    "  * деление на предложения (инициалы и сокращения) и слова (китайский язык)\n",
    "  * поиск имен и названий в тексте - сущностей\n",
    "  * разрешение смысла слов в заданном контексте (замок)\n",
    "  * построить синтаксическое дерево\n",
    "  * определение того, к каким другим объектам относится слово\n",
    "2. задачи на понимание текста, в которых есть \"учитель\"\n",
    "  * предсказание следующего символа\n",
    "  * информационный поиск\n",
    "  * анализ тональности\n",
    "  * выделение отношений и фактов\n",
    "  * ответы на вопросы\n",
    "3. понимание и порождение текста (оценка качества?)\n",
    "  * порождение текста\n",
    "  * автоматическое реферирование\n",
    "  * машинный перевод\n",
    "  * диалоговые модели (чат-бот)\n",
    "  \n",
    "Косвенные задачи:\n",
    "  * описание изображения\n",
    "  * распознавание речи\n",
    "  \n",
    "**Задачи бизнеса**:\n",
    "  * распознавание речи (помощник)\n",
    "  * чат-бот (замена техподдержки в решении большинства вопросов)\n",
    "  * поиск точного ответа на вопрос в базе документов (например, база стандартов)\n",
    "  * оценка мнения в социальных сетях о продукте\n",
    "  * ... (ваши варианты?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тематическое моделирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тематическая модель автоматически определяет, к каким темам относится каждый документ из коллекции документов, а так же какие слова (термины) характеризуют каждую тему."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/d/d5/%D0%A2%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B0%D1%8F_%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 409.642s.\n"
     ]
    }
   ],
   "source": [
    "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "#         Lars Buitinck\n",
    "#         Chyi-Kwei Yau <chyikwei.yau@gmail.com>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from __future__ import print_function\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation # Non-negative Matrix Factorization & LDA\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 10\n",
    "n_top_words = 20\n",
    "\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "\n",
    "# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n",
    "# to filter out useless terms early on: the posts are stripped of headers,\n",
    "# footers and quoted replies, and common English words, words occurring in\n",
    "# only one document or in at least 95% of the documents are removed.\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "data_samples = dataset.data[:n_samples]\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Well i'm not sure about the story nad it did seem biased. What\\nI disagree with is your statement that the U.S. Media is out to\\nruin Israels reputation. That is rediculous. The U.S. media is\\nthe most pro-israeli media in the world. Having lived in Europe\\nI realize that incidences such as the one described in the\\nletter have occured. The U.S. media as a whole seem to try to\\nignore them. The U.S. is subsidizing Israels existance and the\\nEuropeans are not (at least not to the same degree). So I think\\nthat might be a reason they report more clearly on the\\natrocities.\\n\\tWhat is a shame is that in Austria, daily reports of\\nthe inhuman acts commited by Israeli soldiers and the blessing\\nreceived from the Government makes some of the Holocaust guilt\\ngo away. After all, look how the Jews are treating other races\\nwhen they got power. It is unfortunate.\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features for NMF...\n",
      "done in 1.357s.\n",
      "Extracting tf features for LDA...\n",
      "done in 1.443s.\n",
      "\n",
      "Fitting the NMF model (Frobenius norm) with tf-idf features, n_samples=2000 and n_features=1000...\n",
      "done in 1.655s.\n",
      "\n",
      "Topics in NMF model (Frobenius norm):\n",
      "Topic #0: just people don think like know time good make way really say right ve want did ll new use years\n",
      "Topic #1: windows use dos using window program os drivers application help software pc running ms screen files version card code work\n",
      "Topic #2: god jesus bible faith christian christ christians does heaven sin believe lord life church mary atheism belief human love religion\n",
      "Topic #3: thanks know does mail advance hi info interested email anybody looking card help like appreciated information send list video need\n",
      "Topic #4: car cars tires miles 00 new engine insurance price condition oil power speed good 000 brake year models used bought\n",
      "Topic #5: edu soon com send university internet mit ftp mail cc pub article information hope program mac email home contact blood\n",
      "Topic #6: file problem files format win sound ftp pub read save site help image available create copy running memory self version\n",
      "Topic #7: game team games year win play season players nhl runs goal hockey toronto division flyers player defense leafs bad teams\n",
      "Topic #8: drive drives hard disk floppy software card mac computer power scsi controller apple mb 00 pc rom sale problem internal\n",
      "Topic #9: key chip clipper keys encryption government public use secure enforcement phone nsa communications law encrypted security clinton used legal standard\n",
      "\n",
      "Fitting the NMF model (generalized Kullback-Leibler divergence) with tf-idf features, n_samples=2000 and n_features=1000...\n",
      "done in 6.161s.\n",
      "\n",
      "Topics in NMF model (generalized Kullback-Leibler divergence):\n",
      "Topic #0: people just like time don say really know way things make think right said did want ve probably work years\n",
      "Topic #1: windows thanks using help need hi work know use looking mail software does used pc video available running info advance\n",
      "Topic #2: god does true read know say believe subject says religion mean question point jesus people book christian mind understand matter\n",
      "Topic #3: thanks know like interested mail just want new send edu list does bike thing email reply post wondering hear heard\n",
      "Topic #4: time new 10 year sale old offer 20 16 15 great 30 weeks good test model condition 11 14 power\n",
      "Topic #5: use number com government new university data states information talk phone right including security provide control following long used research\n",
      "Topic #6: edu try file soon remember problem com program hope mike space article wrong library short include win little couldn sun\n",
      "Topic #7: year world team game play won win games season maybe case second does did series playing nhl fact said points\n",
      "Topic #8: think don drive need hard make people mac read going pretty try sure order means trying apple case bit drives\n",
      "Topic #9: just good use way got like ll doesn want sure don doing thought does wrong right better make stuff speed\n",
      "\n",
      "Fitting LDA models with tf features, n_samples=2000 and n_features=1000...\n",
      "done in 28.050s.\n",
      "\n",
      "Topics in LDA model:\n",
      "Topic #0: edu com mail send graphics ftp pub available contact university list faq ca information cs 1993 program sun uk mit\n",
      "Topic #1: don like just know think ve way use right good going make sure ll point got need really time doesn\n",
      "Topic #2: christian think atheism faith pittsburgh new bible radio games alt lot just religion like book read play time subject believe\n",
      "Topic #3: drive disk windows thanks use card drives hard version pc software file using scsi help does new dos controller 16\n",
      "Topic #4: hiv health aids disease april medical care research 1993 light information study national service test led 10 page new drug\n",
      "Topic #5: god people does just good don jesus say israel way life know true fact time law want believe make think\n",
      "Topic #6: 55 10 11 18 15 team game 19 period play 23 12 13 flyers 20 25 22 17 24 16\n",
      "Topic #7: car year just cars new engine like bike good oil insurance better tires 000 thing speed model brake driving performance\n",
      "Topic #8: people said did just didn know time like went think children came come don took years say dead told started\n",
      "Topic #9: key space law government public use encryption earth section security moon probe enforcement keys states lunar military crime surface technology\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (generalized Kullback-Leibler divergence) with \"\n",
    "      \"tf-idf features, n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n",
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.96604155,  4.3537397 , 21.42539886, ...,  1.57926488,\n",
       "         1.33933502,  1.20988436],\n",
       "       [ 0.48391034,  1.85845783, 14.04720958, ..., 74.59501615,\n",
       "        59.36116266,  0.27698642],\n",
       "       [ 0.18708486,  0.13728929,  0.31409364, ...,  1.02679042,\n",
       "         2.56259123,  0.13662652],\n",
       "       ...,\n",
       "       [ 3.22343848, 39.1368944 , 11.24910558, ..., 23.37779481,\n",
       "         3.06315114,  0.15230766],\n",
       "       [ 1.41871388, 47.53082031, 16.14390001, ..., 82.46751192,\n",
       "        16.51319941, 28.11660323],\n",
       "       [ 4.02759659,  1.24781464, 13.26101699, ..., 29.0225105 ,\n",
       "         0.24834416,  0.13033208]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Наивный Байес\n",
    "* Знаем метку каждого документа\n",
    "* У каждого документа только одна метка  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что можно сделать, если нет информации о метках?\n",
    "#### Проблема кластеризации  \n",
    "Можно решать с помощью EM-алгоритма:\n",
    "* E-шаг - вычислить ожидания того, какой документ к какой теме относится\n",
    "* M-шаг - с помощью Наивного Байеса определить вероятности $p(w|t)$ при фиксированных метках\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM-алгоритм (Expectation-maximization)\n",
    "\n",
    "Решает задачу кластеризации.  \n",
    "Подбирает некоторые параметры модели для данных в которых неизвестен ответ.  \n",
    "\n",
    "Expectation шаг:\n",
    "* зафиксировать параметры модели\n",
    "* посчитать значения скрытых переменных\n",
    "Maximization шаг:\n",
    "* зафиксировать скрытые переменные\n",
    "* посчитать параметры модели\n",
    "\n",
    "Повторять до сходимости.\n",
    "\n",
    "Есть математическое обоснование того, что метод сходится к локальному экстремуму, на каждом шаге значение функции правдоподобия не убывает (правдоподобие $p(\\theta | \\mathcal{X})$ - насколько правдоподобна модель при данных параметрах, насколдько она хорошо описывает данные)\n",
    "\n",
    "Частный случай EM-алгоритма - **k-means**.  \n",
    "Метки кластеров - скрытые переменные Z  (latent variables)  \n",
    "Параметры модели - центры кластеров  \n",
    "\n",
    "<img src=\"kmeans.png\">\n",
    "\n",
    "Еще вариант EM-алгоритма - разделение смеси гауссиан (Gaussian Mixture Model, GMM)\n",
    "\n",
    "Параметры модели - центр кластера и матрица ковариаций (здесь описывает форму могомерного нормального распраделения, или гауссианы)\n",
    "Скрытые переменные - вероятность пренадлежности к каждой гауссиане (метка кластера выбирается как наиболее вероятный кластер)\n",
    "\n",
    "\n",
    "<img src=\"gauss.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLSA (Probabilistic latent semantic analysis)\n",
    "\n",
    "Что если у каждого документа может быть много меток?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим модель:\n",
    "* Каждое слово в документе $d$ сгенерировано из некоторой темы $t \\in T$\n",
    "* Документ сгенерирован некоторым распределением над темами $p(t|d)$\n",
    "* Слово сгенерировано из темы (не из документа) $p(w|d, t) = p(w|d)$\n",
    "* Получаем правдоподобие: $$p(w|d) = \\sum_{t \\in T}p(w|t)p(t|d) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полученная модель - probabilistic latent semantic analysis, pLSA, Вероятностный латентно-семантический анализ\n",
    "\n",
    "http://www.machinelearning.ru/wiki/index.php?title=%D0%92%D0%B5%D1%80%D0%BE%D1%8F%D1%82%D0%BD%D0%BE%D1%81%D1%82%D0%BD%D1%8B%D0%B9_%D0%BB%D0%B0%D1%82%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B9_%D1%81%D0%B5%D0%BC%D0%B0%D0%BD%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B9_%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение:  \n",
    "Нам нужны величины:\n",
    "* $p(w|t)$ - вероятности слов в темах, обозначим $\\phi_{wt}$\n",
    "\n",
    "* $p(t|d)$ - вероятности тем в документах, обозначим $\\theta_{td}$\n",
    "\n",
    "E-шаг:\n",
    "* фиксируем $\\phi_{wt}$ и $\\theta_{td}$\n",
    "* вычисляем $$p(t|d,w) = \\frac{\\phi_{wt} \\theta_{td}}{\\sum_{s \\in T}\\phi_{ws} \\theta_{sd}}$$ для всех тем, для каждого документа, для каждого термина\n",
    "* вычисляем количество терминов, которое генерируется в документе $d$ темой $t$ $$n_{dwt} = n_{dw}p(t|d,w)$$\n",
    "\n",
    "М-шаг:\n",
    "* по вычисленным $p(t|d,w)$ обновить приближения модели $\\phi_{wt}$ и $\\theta_{td}$\n",
    "* $$n_{wt} = \\sum_d n_{dwt}$$ $$n_{td} = \\sum_{w \\in d} n_{dwt}$$ $$n_t=\\sum_w n_{wt}$$\n",
    "* $$\\theta_{td} = \\frac{n_{td}}{n_d}$$ $$\\phi_{wt} = \\frac{n_{wt}}{n_t}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Можно не хранить матрицу $n_{dwt}$, а итерироваться по документам и суммировать $n_{wt}$ и $n_{td}$\n",
    "* Много локальных экстремумов\n",
    "* Много параметров, модель переобучается\n",
    "* Нужно достичь не локальный минимум, а добиться интерпретируемости - найти \"хороший\" минимум"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA (Latent Dirichlet Allocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В общем случае, чтобы улучшить pLSA, в логарифм правдоподобия добавляют регуляризацию:\n",
    "\n",
    "$$\\sum_{d \\in D} \\sum_{w \\in d} n_{dw} ln \\sum_{t \\in T} \\phi_{wt} \\theta_{td} + \\sum_i \\tau_i R_i(\\Phi, \\Theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если добавить априорное распределение - распределение дирехле, получим алгоритм LDA - Latent Dirichlet Allocation\n",
    "\n",
    "В итоге получаем \"хорошее\" интерпретируемое решение (лучше, чем с pLSA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Один документ может содержать несколько тем.  \n",
    "Составляем иерархическую модель:  \n",
    "* первый уровень - смесь, компоненты которой отвечают за темы\n",
    "* второй уровень - мультиномиальная переменная с априорным распределением Дирихле, которая определяет \"распределение над темами\" в документе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение:\n",
    "* сэмплирование по Гибсу\n",
    "* online variational bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\"\n",
    "doc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\n",
    "doc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\n",
    "doc4 = \"Sometimes I feel pressure to perform well at school, but my father never seems to drive my sister to do better.\"\n",
    "doc5 = \"Health experts say that Sugar is not good for your lifestyle.\"\n",
    "\n",
    "# compile documents\n",
    "doc_complete = [doc1, doc2, doc3, doc4, doc5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kapmik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kapmik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(doc).split() for doc in doc_complete]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['sugar', 'bad', 'consume', 'sister', 'like', 'sugar', 'father'],\n",
       " ['father',\n",
       "  'spends',\n",
       "  'lot',\n",
       "  'time',\n",
       "  'driving',\n",
       "  'sister',\n",
       "  'around',\n",
       "  'dance',\n",
       "  'practice'],\n",
       " ['doctor',\n",
       "  'suggest',\n",
       "  'driving',\n",
       "  'may',\n",
       "  'cause',\n",
       "  'increased',\n",
       "  'stress',\n",
       "  'blood',\n",
       "  'pressure'],\n",
       " ['sometimes',\n",
       "  'feel',\n",
       "  'pressure',\n",
       "  'perform',\n",
       "  'well',\n",
       "  'school',\n",
       "  'father',\n",
       "  'never',\n",
       "  'seems',\n",
       "  'drive',\n",
       "  'sister',\n",
       "  'better'],\n",
       " ['health', 'expert', 'say', 'sugar', 'good', 'lifestyle']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  33,\n",
       "  34],\n",
       " ValuesView(<gensim.corpora.dictionary.Dictionary object at 0x000001A28C72B5C0>))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.keys(), dictionary.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 2)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.doc2bow(doc_clean[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word=dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.091*\"sugar\" + 0.064*\"sister\" + 0.064*\"father\"'), (1, '0.029*\"father\" + 0.029*\"sister\" + 0.029*\"pressure\"'), (2, '0.079*\"driving\" + 0.045*\"pressure\" + 0.045*\"suggest\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=3, num_words=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\javasdk\\anakonda\\lib\\site-packages\\pyLDAvis\\_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el932017977619330084794645172\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el932017977619330084794645172_data = {\"mdsDat\": {\"x\": [-0.08943503518130619, 0.0893154443993231, 0.00011959078198307997], \"y\": [0.0, 0.0, 0.0], \"topics\": [1, 2, 3], \"cluster\": [1, 1, 1], \"Freq\": [55.38682174682617, 41.09609603881836, 3.5170798301696777]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\"], \"Freq\": [1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.16505765914917, 0.8658237457275391, 0.8658239245414734, 0.8658239245414734, 0.8658237457275391, 0.8658237457275391, 0.8658239245414734, 0.8658237457275391, 0.8658239245414734, 0.8658239245414734, 0.8654147982597351, 0.8654147982597351, 0.8654147982597351, 0.8652491569519043, 0.8652492761611938, 0.8652490973472595, 0.8652490973472595, 0.8652490973472595, 1.5182487964630127, 1.5182487964630127, 0.8653688430786133, 0.21676351130008698, 0.21676351130008698, 0.21676351130008698, 0.21676351130008698, 0.21676351130008698, 0.21676351130008698, 0.2167397141456604, 0.2167397141456604, 0.2167397141456604, 0.2167397141456604, 0.2167433798313141, 1.3900822401046753, 0.7940922975540161, 0.7940922975540161, 0.7940922975540161, 0.7940922975540161, 0.7940922975540161, 0.7940922975540161, 0.7940922975540161, 0.794069766998291, 0.7940697073936462, 0.7940697073936462, 0.7940697073936462, 0.7940697073936462, 0.7940697073936462, 0.7947873473167419, 0.7918643355369568, 0.7918643355369568, 0.19894149899482727, 0.19894149899482727, 0.19894146919250488, 0.19894136488437653, 0.19894136488437653, 0.19891706109046936, 0.19891706109046936, 0.19891706109046936, 0.19881071150302887, 0.19881071150302887, 0.19881071150302887, 0.19881071150302887, 0.19881071150302887, 0.19881071150302887, 0.19890479743480682, 0.043185632675886154, 0.043185632675886154, 0.043185632675886154, 0.043185632675886154, 0.043185628950595856, 0.043185628950595856, 0.04318546503782272, 0.04318546503782272, 0.04318546503782272, 0.04318546503782272, 0.04318546503782272, 0.04318546503782272, 0.04318546503782272, 0.04326590895652771, 0.04326590895652771, 0.04326590895652771, 0.04326590895652771, 0.04326590895652771, 0.04323824867606163, 0.04323824867606163, 0.04323824867606163, 0.043179988861083984, 0.04317997768521309, 0.04317997768521309, 0.04317997768521309, 0.04317997023463249, 0.043179966509342194, 0.043179966509342194, 0.0431799553334713, 0.0431799590587616, 0.04329168051481247, 0.04329167306423187, 0.04326867312192917, 0.04323576018214226, 0.04318062588572502], \"Term\": [\"driving\", \"sugar\", \"may\", \"increased\", \"doctor\", \"cause\", \"blood\", \"stress\", \"suggest\", \"dance\", \"practice\", \"time\", \"spends\", \"around\", \"lot\", \"health\", \"good\", \"lifestyle\", \"expert\", \"say\", \"consume\", \"like\", \"bad\", \"school\", \"feel\", \"perform\", \"well\", \"drive\", \"never\", \"better\", \"sugar\", \"well\", \"sometimes\", \"seems\", \"school\", \"perform\", \"never\", \"feel\", \"drive\", \"better\", \"bad\", \"consume\", \"like\", \"health\", \"good\", \"lifestyle\", \"say\", \"expert\", \"father\", \"sister\", \"pressure\", \"time\", \"spends\", \"practice\", \"lot\", \"dance\", \"around\", \"stress\", \"suggest\", \"may\", \"increased\", \"driving\", \"driving\", \"may\", \"stress\", \"suggest\", \"doctor\", \"cause\", \"increased\", \"blood\", \"lot\", \"time\", \"practice\", \"spends\", \"dance\", \"around\", \"pressure\", \"sister\", \"father\", \"say\", \"lifestyle\", \"expert\", \"health\", \"good\", \"consume\", \"like\", \"bad\", \"well\", \"school\", \"perform\", \"feel\", \"seems\", \"better\", \"sugar\", \"dance\", \"lot\", \"practice\", \"time\", \"around\", \"spends\", \"may\", \"stress\", \"blood\", \"cause\", \"doctor\", \"increased\", \"suggest\", \"health\", \"lifestyle\", \"say\", \"good\", \"expert\", \"bad\", \"consume\", \"like\", \"school\", \"well\", \"perform\", \"feel\", \"never\", \"sometimes\", \"better\", \"seems\", \"drive\", \"father\", \"sister\", \"pressure\", \"sugar\", \"driving\"], \"Total\": [1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.407198190689087, 1.1078144311904907, 1.1078146696090698, 1.1078146696090698, 1.1078144311904907, 1.1078144311904907, 1.1078146696090698, 1.1078144311904907, 1.1078146696090698, 1.1078146696090698, 1.1075701713562012, 1.1075701713562012, 1.1075701713562012, 1.1074564456939697, 1.1074565649032593, 1.1074565649032593, 1.1074565649032593, 1.1074565649032593, 2.3534047603607178, 2.3534047603607178, 1.7034249305725098, 1.0540188550949097, 1.0540188550949097, 1.0540188550949097, 1.0540188550949097, 1.0540188550949097, 1.0540188550949097, 1.0540175437927246, 1.0540175437927246, 1.0540175437927246, 1.0540175437927246, 1.6500061750411987, 1.6500061750411987, 1.0540175437927246, 1.0540175437927246, 1.0540175437927246, 1.0540175437927246, 1.0540175437927246, 1.0540175437927246, 1.0540175437927246, 1.0540188550949097, 1.0540188550949097, 1.0540188550949097, 1.0540188550949097, 1.0540188550949097, 1.0540188550949097, 1.7034249305725098, 2.3534047603607178, 2.3534047603607178, 1.1074565649032593, 1.1074565649032593, 1.1074565649032593, 1.1074564456939697, 1.1074565649032593, 1.1075701713562012, 1.1075701713562012, 1.1075701713562012, 1.1078144311904907, 1.1078144311904907, 1.1078144311904907, 1.1078144311904907, 1.1078146696090698, 1.1078146696090698, 2.407198190689087, 1.0540188550949097, 1.0540188550949097, 1.0540188550949097, 1.0540188550949097, 1.0540188550949097, 1.0540188550949097, 1.0540175437927246, 1.0540175437927246, 1.0540175437927246, 1.0540175437927246, 1.0540175437927246, 1.0540175437927246, 1.0540175437927246, 1.1074564456939697, 1.1074565649032593, 1.1074565649032593, 1.1074565649032593, 1.1074565649032593, 1.1075701713562012, 1.1075701713562012, 1.1075701713562012, 1.1078144311904907, 1.1078144311904907, 1.1078144311904907, 1.1078144311904907, 1.1078146696090698, 1.1078146696090698, 1.1078146696090698, 1.1078146696090698, 1.1078146696090698, 2.3534047603607178, 2.3534047603607178, 1.7034249305725098, 2.407198190689087, 1.6500061750411987], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.4848000109195709, 0.34439998865127563, 0.34439998865127563, 0.34439998865127563, 0.34439998865127563, 0.34439998865127563, 0.34439998865127563, 0.34439998865127563, 0.34439998865127563, 0.34439998865127563, 0.3440999984741211, 0.3440999984741211, 0.3440999984741211, 0.3440000116825104, 0.3440000116825104, 0.3440000116825104, 0.3440000116825104, 0.3440000116825104, 0.1525000035762787, 0.1525000035762787, -0.08640000224113464, -0.9907000064849854, -0.9907000064849854, -0.9907000064849854, -0.9907000064849854, -0.9907000064849854, -0.9907000064849854, -0.9908000230789185, -0.9908000230789185, -0.9908000230789185, -0.9908000230789185, -1.4390000104904175, 0.7178000211715698, 0.6061000227928162, 0.6061000227928162, 0.6061000227928162, 0.6061000227928162, 0.6061000227928162, 0.6061000227928162, 0.6061000227928162, 0.6061000227928162, 0.6061000227928162, 0.6061000227928162, 0.6061000227928162, 0.6061000227928162, 0.6061000227928162, 0.12690000236034393, -0.20000000298023224, -0.20000000298023224, -0.8276000022888184, -0.8276000022888184, -0.8276000022888184, -0.8276000022888184, -0.8276000022888184, -0.8277999758720398, -0.8277999758720398, -0.8277999758720398, -0.828499972820282, -0.828499972820282, -0.828499972820282, -0.828499972820282, -0.828499972820282, -0.828499972820282, -1.604099988937378, 0.1527000069618225, 0.1527000069618225, 0.1527000069618225, 0.1527000069618225, 0.1527000069618225, 0.1527000069618225, 0.1527000069618225, 0.1527000069618225, 0.1527000069618225, 0.1527000069618225, 0.1527000069618225, 0.1527000069618225, 0.1527000069618225, 0.10509999841451645, 0.10509999841451645, 0.10509999841451645, 0.10509999841451645, 0.10509999841451645, 0.10429999977350235, 0.10429999977350235, 0.10429999977350235, 0.10279999673366547, 0.10279999673366547, 0.10279999673366547, 0.10279999673366547, 0.10279999673366547, 0.10279999673366547, 0.10279999673366547, 0.10279999673366547, 0.10279999673366547, -0.6481000185012817, -0.6481000185012817, -0.3253999948501587, -0.671999990940094, -0.2955999970436096], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -2.397900104522705, -3.3143999576568604, -3.3143999576568604, -3.3143999576568604, -3.3143999576568604, -3.3143999576568604, -3.3143999576568604, -3.3143999576568604, -3.3143999576568604, -3.3143999576568604, -3.3148999214172363, -3.3148999214172363, -3.3148999214172363, -3.3150999546051025, -3.3150999546051025, -3.3150999546051025, -3.3150999546051025, -3.3150999546051025, -2.7527999877929688, -2.7527999877929688, -3.315000057220459, -4.6992998123168945, -4.6992998123168945, -4.6992998123168945, -4.6992998123168945, -4.6992998123168945, -4.6992998123168945, -4.699399948120117, -4.699399948120117, -4.699399948120117, -4.699399948120117, -4.699399948120117, -2.54259991645813, -3.1024999618530273, -3.1024999618530273, -3.1024999618530273, -3.1024999618530273, -3.1024999618530273, -3.1024999618530273, -3.1024999618530273, -3.1024999618530273, -3.1024999618530273, -3.1024999618530273, -3.1024999618530273, -3.1024999618530273, -3.1024999618530273, -3.101599931716919, -3.105299949645996, -3.105299949645996, -4.486700057983398, -4.486700057983398, -4.486700057983398, -4.486700057983398, -4.486700057983398, -4.486800193786621, -4.486800193786621, -4.486800193786621, -4.487299919128418, -4.487299919128418, -4.487299919128418, -4.487299919128418, -4.487299919128418, -4.487299919128418, -4.4868998527526855, -3.5559000968933105, -3.5559000968933105, -3.5559000968933105, -3.5559000968933105, -3.5559000968933105, -3.5559000968933105, -3.5559000968933105, -3.5559000968933105, -3.5559000968933105, -3.5559000968933105, -3.5559000968933105, -3.5559000968933105, -3.5559000968933105, -3.5541000366210938, -3.5541000366210938, -3.5541000366210938, -3.5541000366210938, -3.5541000366210938, -3.5546998977661133, -3.5546998977661133, -3.5546998977661133, -3.555999994277954, -3.555999994277954, -3.555999994277954, -3.555999994277954, -3.555999994277954, -3.555999994277954, -3.555999994277954, -3.555999994277954, -3.555999994277954, -3.553499937057495, -3.553499937057495, -3.553999900817871, -3.5546998977661133, -3.555999994277954]}, \"token.table\": {\"Topic\": [2, 1, 1, 2, 2, 1, 2, 2, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 2, 1, 2, 2, 1, 2, 2, 1], \"Freq\": [0.9487496018409729, 0.9028773307800293, 0.9026780724525452, 0.9487507939338684, 0.9487507939338684, 0.9028773307800293, 0.9487496018409729, 0.9487507939338684, 0.9026780724525452, 0.6060583591461182, 0.9029699563980103, 0.8498325347900391, 0.42491626739501953, 0.9026782512664795, 0.9029699563980103, 0.9029700756072998, 0.9487507939338684, 0.9029699563980103, 0.9028773307800293, 0.9487496018409729, 0.9487507939338684, 0.9026780724525452, 0.9026782512664795, 0.9487496018409729, 0.587052583694458, 0.587052583694458, 0.9029699563980103, 0.9026782512664795, 0.9026780724525452, 0.8498325347900391, 0.42491626739501953, 0.9026780724525452, 0.9487496018409729, 0.9487507939338684, 0.8308414220809937, 0.9487507939338684, 0.9487496018409729, 0.9026782512664795], \"Term\": [\"around\", \"bad\", \"better\", \"blood\", \"cause\", \"consume\", \"dance\", \"doctor\", \"drive\", \"driving\", \"expert\", \"father\", \"father\", \"feel\", \"good\", \"health\", \"increased\", \"lifestyle\", \"like\", \"lot\", \"may\", \"never\", \"perform\", \"practice\", \"pressure\", \"pressure\", \"say\", \"school\", \"seems\", \"sister\", \"sister\", \"sometimes\", \"spends\", \"stress\", \"sugar\", \"suggest\", \"time\", \"well\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 3, 2]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el932017977619330084794645172\", ldavis_el932017977619330084794645172_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el932017977619330084794645172\", ldavis_el932017977619330084794645172_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el932017977619330084794645172\", ldavis_el932017977619330084794645172_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x    y  topics  cluster       Freq\n",
       "topic                                           \n",
       "0     -0.089435  0.0       1        1  55.386822\n",
       "2      0.089315  0.0       2        1  41.096096\n",
       "1      0.000120  0.0       3        1   3.517080, topic_info=     Category      Freq       Term     Total  loglift  logprob\n",
       "term                                                          \n",
       "8     Default  1.000000    driving  1.000000  30.0000  30.0000\n",
       "5     Default  2.000000      sugar  2.000000  29.0000  29.0000\n",
       "17    Default  1.000000        may  1.000000  28.0000  28.0000\n",
       "16    Default  1.000000  increased  1.000000  27.0000  27.0000\n",
       "15    Default  1.000000     doctor  1.000000  26.0000  26.0000\n",
       "14    Default  1.000000      cause  1.000000  25.0000  25.0000\n",
       "13    Default  1.000000      blood  1.000000  24.0000  24.0000\n",
       "19    Default  1.000000     stress  1.000000  23.0000  23.0000\n",
       "20    Default  1.000000    suggest  1.000000  22.0000  22.0000\n",
       "7     Default  1.000000      dance  1.000000  21.0000  21.0000\n",
       "10    Default  1.000000   practice  1.000000  20.0000  20.0000\n",
       "12    Default  1.000000       time  1.000000  19.0000  19.0000\n",
       "11    Default  1.000000     spends  1.000000  18.0000  18.0000\n",
       "6     Default  1.000000     around  1.000000  17.0000  17.0000\n",
       "9     Default  1.000000        lot  1.000000  16.0000  16.0000\n",
       "32    Default  1.000000     health  1.000000  15.0000  15.0000\n",
       "31    Default  1.000000       good  1.000000  14.0000  14.0000\n",
       "33    Default  1.000000  lifestyle  1.000000  13.0000  13.0000\n",
       "30    Default  1.000000     expert  1.000000  12.0000  12.0000\n",
       "34    Default  1.000000        say  1.000000  11.0000  11.0000\n",
       "1     Default  1.000000    consume  1.000000  10.0000  10.0000\n",
       "3     Default  1.000000       like  1.000000   9.0000   9.0000\n",
       "0     Default  1.000000        bad  1.000000   8.0000   8.0000\n",
       "26    Default  1.000000     school  1.000000   7.0000   7.0000\n",
       "23    Default  1.000000       feel  1.000000   6.0000   6.0000\n",
       "25    Default  1.000000    perform  1.000000   5.0000   5.0000\n",
       "29    Default  1.000000       well  1.000000   4.0000   4.0000\n",
       "22    Default  1.000000      drive  1.000000   3.0000   3.0000\n",
       "24    Default  1.000000      never  1.000000   2.0000   2.0000\n",
       "21    Default  1.000000     better  1.000000   1.0000   1.0000\n",
       "...       ...       ...        ...       ...      ...      ...\n",
       "11     Topic3  0.043186     spends  1.054019   0.1527  -3.5559\n",
       "17     Topic3  0.043185        may  1.054018   0.1527  -3.5559\n",
       "19     Topic3  0.043185     stress  1.054018   0.1527  -3.5559\n",
       "13     Topic3  0.043185      blood  1.054018   0.1527  -3.5559\n",
       "14     Topic3  0.043185      cause  1.054018   0.1527  -3.5559\n",
       "15     Topic3  0.043185     doctor  1.054018   0.1527  -3.5559\n",
       "16     Topic3  0.043185  increased  1.054018   0.1527  -3.5559\n",
       "20     Topic3  0.043185    suggest  1.054018   0.1527  -3.5559\n",
       "32     Topic3  0.043266     health  1.107456   0.1051  -3.5541\n",
       "33     Topic3  0.043266  lifestyle  1.107457   0.1051  -3.5541\n",
       "34     Topic3  0.043266        say  1.107457   0.1051  -3.5541\n",
       "31     Topic3  0.043266       good  1.107457   0.1051  -3.5541\n",
       "30     Topic3  0.043266     expert  1.107457   0.1051  -3.5541\n",
       "0      Topic3  0.043238        bad  1.107570   0.1043  -3.5547\n",
       "1      Topic3  0.043238    consume  1.107570   0.1043  -3.5547\n",
       "3      Topic3  0.043238       like  1.107570   0.1043  -3.5547\n",
       "26     Topic3  0.043180     school  1.107814   0.1028  -3.5560\n",
       "29     Topic3  0.043180       well  1.107814   0.1028  -3.5560\n",
       "25     Topic3  0.043180    perform  1.107814   0.1028  -3.5560\n",
       "23     Topic3  0.043180       feel  1.107814   0.1028  -3.5560\n",
       "24     Topic3  0.043180      never  1.107815   0.1028  -3.5560\n",
       "28     Topic3  0.043180  sometimes  1.107815   0.1028  -3.5560\n",
       "21     Topic3  0.043180     better  1.107815   0.1028  -3.5560\n",
       "27     Topic3  0.043180      seems  1.107815   0.1028  -3.5560\n",
       "22     Topic3  0.043180      drive  1.107815   0.1028  -3.5560\n",
       "2      Topic3  0.043292     father  2.353405  -0.6481  -3.5535\n",
       "4      Topic3  0.043292     sister  2.353405  -0.6481  -3.5535\n",
       "18     Topic3  0.043269   pressure  1.703425  -0.3254  -3.5540\n",
       "5      Topic3  0.043236      sugar  2.407198  -0.6720  -3.5547\n",
       "8      Topic3  0.043181    driving  1.650006  -0.2956  -3.5560\n",
       "\n",
       "[129 rows x 6 columns], token_table=      Topic      Freq       Term\n",
       "term                            \n",
       "6         2  0.948750     around\n",
       "0         1  0.902877        bad\n",
       "21        1  0.902678     better\n",
       "13        2  0.948751      blood\n",
       "14        2  0.948751      cause\n",
       "1         1  0.902877    consume\n",
       "7         2  0.948750      dance\n",
       "15        2  0.948751     doctor\n",
       "22        1  0.902678      drive\n",
       "8         2  0.606058    driving\n",
       "30        1  0.902970     expert\n",
       "2         1  0.849833     father\n",
       "2         2  0.424916     father\n",
       "23        1  0.902678       feel\n",
       "31        1  0.902970       good\n",
       "32        1  0.902970     health\n",
       "16        2  0.948751  increased\n",
       "33        1  0.902970  lifestyle\n",
       "3         1  0.902877       like\n",
       "9         2  0.948750        lot\n",
       "17        2  0.948751        may\n",
       "24        1  0.902678      never\n",
       "25        1  0.902678    perform\n",
       "10        2  0.948750   practice\n",
       "18        1  0.587053   pressure\n",
       "18        2  0.587053   pressure\n",
       "34        1  0.902970        say\n",
       "26        1  0.902678     school\n",
       "27        1  0.902678      seems\n",
       "4         1  0.849833     sister\n",
       "4         2  0.424916     sister\n",
       "28        1  0.902678  sometimes\n",
       "11        2  0.948750     spends\n",
       "19        2  0.948751     stress\n",
       "5         1  0.830841      sugar\n",
       "20        2  0.948751    suggest\n",
       "12        2  0.948750       time\n",
       "29        1  0.902678       well, R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[1, 3, 2])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.gensim.prepare(ldamodel, doc_term_matrix, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://www.kaggle.com/benhamner/nips-papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...\n",
       "1       683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...\n",
       "2       394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...\n",
       "3       Bayesian Query Construction for Neural\\nNetwor...\n",
       "4       Neural Network Ensembles, Cross\\nValidation, a...\n",
       "5       U sing a neural net to instantiate a\\ndeformab...\n",
       "6       Plasticity-Mediated Competitive Learning\\n\\nTe...\n",
       "7       ICEG Morphology Classification using an\\nAnalo...\n",
       "8       Real-Time Control of a Tokamak Plasma\\nUsing N...\n",
       "9       Real-Time Control of a Tokamak Plasma\\nUsing N...\n",
       "10      Learning To Play the Game of Chess\\n\\nSebastia...\n",
       "11      Multidimensional Scaling and Data Clustering\\n...\n",
       "12      An experimental comparison\\nof recurrent neura...\n",
       "13      133\\n\\nTRAINING MULTILAYER PERCEPTRONS WITH TH...\n",
       "14      Interference in Learning Internal\\nModels of I...\n",
       "15      Active Learning with Statistical Models\\n\\nDav...\n",
       "16      A Rapid Graph-based Method for\\nArbitrary Tran...\n",
       "17      Ocular Dominance and Patterned Lateral\\nConnec...\n",
       "18      Associative Decorrelation Dynamics:\\nA Theory ...\n",
       "19      A Connectionist Technique for Accelerated\\nTex...\n",
       "20      Connectionist Speaker Normalization\\nwith Gene...\n",
       "21      A Critical Comparison of Models for\\nOrientati...\n",
       "22      Generalization in Reinforcement Learning:\\nSaf...\n",
       "23      A Mixture Model System for Medical and\\nMachin...\n",
       "24      186\\n\\nAN APPLICATION OF THE PRINCIPLE OF\\nMAX...\n",
       "25      A Computational Model of Prefrontal\\nCortex Fu...\n",
       "26      The Gamma MLP for Speech Phoneme\\nRecognition\\...\n",
       "27      A Multiscale Attentional Framework for\\nRelaxa...\n",
       "28      Correlated Neuronal Response:\\nTime Scales and...\n",
       "29      Onset-based Sound Segmentation\\n\\nLeslie S. Sm...\n",
       "                              ...                        \n",
       "7211    Combining Estimators Using\\nNon-Constant Weigh...\n",
       "7212    A model of the hippocampus combining selforgan...\n",
       "7213    Catastrophic Interference in Human\\nMotor Lear...\n",
       "7214    Model of a Biological Neuron as a Temporal\\nNe...\n",
       "7215    Dynamic Cell Structures\\nJorg Bruske and Geral...\n",
       "7216    On-line Learning of Dichotomies\\n\\nN. Barkai\\n...\n",
       "7217    New Algorithms for\\n2D and 3D Point Matching:\\...\n",
       "7218    Pattern Playback in the '90s\\nMalcolm Slaney\\n...\n",
       "7219    Learning in large linear perceptrons and\\nwhy ...\n",
       "7220    553\\n\\nSPREADING ACTIVATION OVER\\nDISTRIBUTED ...\n",
       "7221    Interior Point Implementations of\\nAlternating...\n",
       "7222    Reinforcement Learning with Soft State\\nAggreg...\n",
       "7223    Coarse-to-Fine Image Search Using Neural\\nNetw...\n",
       "7224    On the Computational Utility of\\nConsciousness...\n",
       "7225    A Convolutional Neural Network\\nHand Tracker\\n...\n",
       "7226    Efficient Methods for Dealing with\\nMissing Da...\n",
       "7227    Grammar Learning by a Self-Organizing\\nNetwork...\n",
       "7228    Recurrent Networks:\\nSecond Order Properties a...\n",
       "7229    Comparing the prediction accuracy of\\nartifici...\n",
       "7230    Convergence Properties of the K-Means\\nAlgorit...\n",
       "7231    695\\n\\nANALOG IMPLEMENTATION OF SHUNTING\\nNEUR...\n",
       "7232    Comparing the prediction accuracy of\\nartifici...\n",
       "7233    Stochastic Dynamics of Three-State\\nNeural Net...\n",
       "7234    Grouping Components of\\n?\\nThree-Dimensional M...\n",
       "7235    Visual Speech Recognition with\\nStochastic Net...\n",
       "7236    Single Transistor Learning Synapses\\n\\nPaul Ha...\n",
       "7237    Bias, Variance and the Combination of\\nLeast S...\n",
       "7238    A Real Time Clustering CMOS\\nNeural Engine\\nT....\n",
       "7239    Learning direction in global motion: two\\nclas...\n",
       "7240    Correlation and Interpolation Networks for\\nRe...\n",
       "Name: paper_text, Length: 7241, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ds = pd.read_csv('papers.csv')\n",
    "ds['paper_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7241"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(doc).split() for doc in ds['paper_text']]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'767',\n",
       " u'selforganization',\n",
       " u'associative',\n",
       " u'database',\n",
       " u'application',\n",
       " u'hisashi',\n",
       " u'suzuki',\n",
       " u'suguru',\n",
       " u'arimoto',\n",
       " u'osaka',\n",
       " u'university',\n",
       " u'toyonaka',\n",
       " u'osaka',\n",
       " u'560',\n",
       " u'japan',\n",
       " u'abstract',\n",
       " u'efficient',\n",
       " u'method',\n",
       " u'selforganizing',\n",
       " u'associative',\n",
       " u'database',\n",
       " u'proposed',\n",
       " u'together',\n",
       " u'application',\n",
       " u'robot',\n",
       " u'eyesight',\n",
       " u'system',\n",
       " u'proposed',\n",
       " u'database',\n",
       " u'associate',\n",
       " u'input',\n",
       " u'output',\n",
       " u'first',\n",
       " u'half',\n",
       " u'part',\n",
       " u'discussion',\n",
       " u'algorithm',\n",
       " u'selforganization',\n",
       " u'proposed',\n",
       " u'aspect',\n",
       " u'hardware',\n",
       " u'produce',\n",
       " u'new',\n",
       " u'style',\n",
       " u'neural',\n",
       " u'network',\n",
       " u'latter',\n",
       " u'half',\n",
       " u'part',\n",
       " u'applicability',\n",
       " u'handwritten',\n",
       " u'letter',\n",
       " u'recognition',\n",
       " u'autonomous',\n",
       " u'mobile',\n",
       " u'robot',\n",
       " u'system',\n",
       " u'demonstrated',\n",
       " u'introduction',\n",
       " u'let',\n",
       " u'mapping',\n",
       " u'f',\n",
       " u'x',\n",
       " u'given',\n",
       " u'here',\n",
       " u'x',\n",
       " u'finite',\n",
       " u'infinite',\n",
       " u'set',\n",
       " u'another',\n",
       " u'finite',\n",
       " u'infinite',\n",
       " u'set',\n",
       " u'learning',\n",
       " u'machine',\n",
       " u'observes',\n",
       " u'set',\n",
       " u'pair',\n",
       " u'x',\n",
       " u'y',\n",
       " u'sampled',\n",
       " u'randomly',\n",
       " u'x',\n",
       " u'x',\n",
       " u'y',\n",
       " u'x',\n",
       " u'x',\n",
       " u'mean',\n",
       " u'cartesian',\n",
       " u'product',\n",
       " u'x',\n",
       " u'y',\n",
       " u'and',\n",
       " u'computes',\n",
       " u'estimate',\n",
       " u'j',\n",
       " u'x',\n",
       " u'f',\n",
       " u'make',\n",
       " u'small',\n",
       " u'estimation',\n",
       " u'error',\n",
       " u'measure',\n",
       " u'usually',\n",
       " u'say',\n",
       " u'that',\n",
       " u'faster',\n",
       " u'decrease',\n",
       " u'estimation',\n",
       " u'error',\n",
       " u'increase',\n",
       " u'number',\n",
       " u'sample',\n",
       " u'better',\n",
       " u'learning',\n",
       " u'machine',\n",
       " u'however',\n",
       " u'expression',\n",
       " u'performance',\n",
       " u'incomplete',\n",
       " u'since',\n",
       " u'lack',\n",
       " u'consideration',\n",
       " u'candidate',\n",
       " u'j',\n",
       " u'j',\n",
       " u'assumed',\n",
       " u'preliminarily',\n",
       " u'then',\n",
       " u'find',\n",
       " u'good',\n",
       " u'learning',\n",
       " u'machine',\n",
       " u'clarify',\n",
       " u'conception',\n",
       " u'let',\n",
       " u'u',\n",
       " u'discus',\n",
       " u'type',\n",
       " u'learning',\n",
       " u'machine',\n",
       " u'and',\n",
       " u'let',\n",
       " u'u',\n",
       " u'advance',\n",
       " u'understanding',\n",
       " u'selforganization',\n",
       " u'associative',\n",
       " u'database',\n",
       " u'parameter',\n",
       " u'type',\n",
       " u'ordinary',\n",
       " u'type',\n",
       " u'learning',\n",
       " u'machine',\n",
       " u'assumes',\n",
       " u'equation',\n",
       " u'relating',\n",
       " u'x',\n",
       " u'y',\n",
       " u'parameter',\n",
       " u'indefinite',\n",
       " u'namely',\n",
       " u'structure',\n",
       " u'f',\n",
       " u'equivalent',\n",
       " u'define',\n",
       " u'implicitly',\n",
       " u'set',\n",
       " u'f',\n",
       " u'candidate',\n",
       " u'f',\n",
       " u'subset',\n",
       " u'mapping',\n",
       " u'x',\n",
       " u'y',\n",
       " u'and',\n",
       " u'computes',\n",
       " u'value',\n",
       " u'parameter',\n",
       " u'based',\n",
       " u'observed',\n",
       " u'sample',\n",
       " u'call',\n",
       " u'type',\n",
       " u'parameter',\n",
       " u'type',\n",
       " u'learning',\n",
       " u'machine',\n",
       " u'defined',\n",
       " u'well',\n",
       " u'f',\n",
       " u'3',\n",
       " u'f',\n",
       " u'j',\n",
       " u'approach',\n",
       " u'f',\n",
       " u'number',\n",
       " u'sample',\n",
       " u'increase',\n",
       " u'alternative',\n",
       " u'case',\n",
       " u'however',\n",
       " u'estimation',\n",
       " u'error',\n",
       " u'remains',\n",
       " u'eternally',\n",
       " u'thus',\n",
       " u'problem',\n",
       " u'designing',\n",
       " u'learning',\n",
       " u'machine',\n",
       " u'return',\n",
       " u'find',\n",
       " u'proper',\n",
       " u'structure',\n",
       " u'f',\n",
       " u'sense',\n",
       " u'hand',\n",
       " u'assumed',\n",
       " u'structure',\n",
       " u'f',\n",
       " u'demanded',\n",
       " u'compact',\n",
       " u'possible',\n",
       " u'achieve',\n",
       " u'fast',\n",
       " u'learning',\n",
       " u'word',\n",
       " u'number',\n",
       " u'parameter',\n",
       " u'small',\n",
       " u'since',\n",
       " u'parameter',\n",
       " u'few',\n",
       " u'j',\n",
       " u'uniquely',\n",
       " u'determined',\n",
       " u'even',\n",
       " u'though',\n",
       " u'observed',\n",
       " u'sample',\n",
       " u'few',\n",
       " u'however',\n",
       " u'demand',\n",
       " u'proper',\n",
       " u'contradicts',\n",
       " u'compact',\n",
       " u'consequently',\n",
       " u'parameter',\n",
       " u'type',\n",
       " u'better',\n",
       " u'compactness',\n",
       " u'assumed',\n",
       " u'structure',\n",
       " u'proper',\n",
       " u'better',\n",
       " u'learning',\n",
       " u'machine',\n",
       " u'elementary',\n",
       " u'conception',\n",
       " u'design',\n",
       " u'learning',\n",
       " u'machine',\n",
       " u'1',\n",
       " u'universality',\n",
       " u'ordinary',\n",
       " u'neural',\n",
       " u'network',\n",
       " u'suppose',\n",
       " u'sufficient',\n",
       " u'knowledge',\n",
       " u'f',\n",
       " u'given',\n",
       " u'though',\n",
       " u'j',\n",
       " u'unknown',\n",
       " u'case',\n",
       " u'comparatively',\n",
       " u'easy',\n",
       " u'find',\n",
       " u'proper',\n",
       " u'compact',\n",
       " u'structure',\n",
       " u'j',\n",
       " u'alternative',\n",
       " u'case',\n",
       " u'however',\n",
       " u'sometimes',\n",
       " u'difficult',\n",
       " u'possible',\n",
       " u'solution',\n",
       " u'give',\n",
       " u'compactness',\n",
       " u'assume',\n",
       " u'almighty',\n",
       " u'structure',\n",
       " u'cover',\n",
       " u'various',\n",
       " u'1',\n",
       " u'combination',\n",
       " u'orthogonal',\n",
       " u'base',\n",
       " u'infinite',\n",
       " u'dimension',\n",
       " u'structure',\n",
       " u'neural',\n",
       " u'network',\n",
       " u'1',\n",
       " u'2',\n",
       " u'approximation',\n",
       " u'obtained',\n",
       " u'truncating',\n",
       " u'finitely',\n",
       " u'dimension',\n",
       " u'implementation',\n",
       " u'american',\n",
       " u'institute',\n",
       " u'physic',\n",
       " u'1988',\n",
       " u'768',\n",
       " u'main',\n",
       " u'topic',\n",
       " u'designing',\n",
       " u'neural',\n",
       " u'network',\n",
       " u'establish',\n",
       " u'desirable',\n",
       " u'structure',\n",
       " u'1',\n",
       " u'work',\n",
       " u'includes',\n",
       " u'developing',\n",
       " u'practical',\n",
       " u'procedure',\n",
       " u'compute',\n",
       " u'value',\n",
       " u'coefficient',\n",
       " u'observed',\n",
       " u'sample',\n",
       " u'discussion',\n",
       " u'flourishing',\n",
       " u'since',\n",
       " u'1980',\n",
       " u'many',\n",
       " u'efficient',\n",
       " u'method',\n",
       " u'proposed',\n",
       " u'recently',\n",
       " u'even',\n",
       " u'hardware',\n",
       " u'unit',\n",
       " u'computing',\n",
       " u'coefficient',\n",
       " u'parallel',\n",
       " u'speedup',\n",
       " u'sold',\n",
       " u'eg',\n",
       " u'anza',\n",
       " u'mark',\n",
       " u'iii',\n",
       " u'odyssey',\n",
       " u'e1',\n",
       " u'nevertheless',\n",
       " u'neural',\n",
       " u'network',\n",
       " u'always',\n",
       " u'exists',\n",
       " u'danger',\n",
       " u'error',\n",
       " u'remaining',\n",
       " u'eternally',\n",
       " u'estimating',\n",
       " u'precisely',\n",
       " u'speaking',\n",
       " u'suppose',\n",
       " u'combination',\n",
       " u'base',\n",
       " u'finite',\n",
       " u'number',\n",
       " u'define',\n",
       " u'structure',\n",
       " u'1',\n",
       " u'essentially',\n",
       " u'word',\n",
       " u'suppose',\n",
       " u'f',\n",
       " u'3',\n",
       " u'1',\n",
       " u'located',\n",
       " u'near',\n",
       " u'f',\n",
       " u'case',\n",
       " u'estimation',\n",
       " u'error',\n",
       " u'none',\n",
       " u'negligible',\n",
       " u'however',\n",
       " u'1',\n",
       " u'distant',\n",
       " u'f',\n",
       " u'estimation',\n",
       " u'error',\n",
       " u'never',\n",
       " u'becomes',\n",
       " u'negligible',\n",
       " u'indeed',\n",
       " u'many',\n",
       " u'research',\n",
       " u'report',\n",
       " u'following',\n",
       " u'situation',\n",
       " u'appears',\n",
       " u'1',\n",
       " u'complex',\n",
       " u'estimation',\n",
       " u'error',\n",
       " u'converges',\n",
       " u'value',\n",
       " u'0',\n",
       " u'number',\n",
       " u'sample',\n",
       " u'increase',\n",
       " u'decrease',\n",
       " u'hardly',\n",
       " u'even',\n",
       " u'though',\n",
       " u'dimension',\n",
       " u'heighten',\n",
       " u'property',\n",
       " u'sometimes',\n",
       " u'considerable',\n",
       " u'defect',\n",
       " u'neural',\n",
       " u'network',\n",
       " u'recursi',\n",
       " u'type',\n",
       " u'recursive',\n",
       " u'type',\n",
       " u'founded',\n",
       " u'another',\n",
       " u'methodology',\n",
       " u'learning',\n",
       " u'follows',\n",
       " u'initial',\n",
       " u'stage',\n",
       " u'sample',\n",
       " u'set',\n",
       " u'fa',\n",
       " u'instead',\n",
       " u'notation',\n",
       " u'f',\n",
       " u'candidate',\n",
       " u'equal',\n",
       " u'set',\n",
       " u'mapping',\n",
       " u'x',\n",
       " u'y',\n",
       " u'observing',\n",
       " u'first',\n",
       " u'sample',\n",
       " u'xl',\n",
       " u'yl',\n",
       " u'e',\n",
       " u'x',\n",
       " u'x',\n",
       " u'y',\n",
       " u'fa',\n",
       " u'reduced',\n",
       " u'fi',\n",
       " u'ixt',\n",
       " u'yl',\n",
       " u'e',\n",
       " u'f',\n",
       " u'observing',\n",
       " u'second',\n",
       " u'sample',\n",
       " u'x2',\n",
       " u'y2',\n",
       " u'e',\n",
       " u'x',\n",
       " u'x',\n",
       " u'y',\n",
       " u'fl',\n",
       " u'reduced',\n",
       " u'f2',\n",
       " u'ixt',\n",
       " u'yl',\n",
       " u'ix2',\n",
       " u'y2',\n",
       " u'e',\n",
       " u'f',\n",
       " u'thus',\n",
       " u'candidate',\n",
       " u'set',\n",
       " u'f',\n",
       " u'becomes',\n",
       " u'gradually',\n",
       " u'small',\n",
       " u'observation',\n",
       " u'sample',\n",
       " u'proceeds',\n",
       " u'observing',\n",
       " u'isamples',\n",
       " u'write',\n",
       " u'one',\n",
       " u'likelihood',\n",
       " u'estimation',\n",
       " u'1',\n",
       " u'selected',\n",
       " u'fi',\n",
       " u'hence',\n",
       " u'contrarily',\n",
       " u'parameter',\n",
       " u'type',\n",
       " u'recursive',\n",
       " u'type',\n",
       " u'guarantee',\n",
       " u'surely',\n",
       " u'j',\n",
       " u'approach',\n",
       " u'1',\n",
       " u'number',\n",
       " u'sample',\n",
       " u'increase',\n",
       " u'recursive',\n",
       " u'type',\n",
       " u'observes',\n",
       " u'sample',\n",
       " u'x',\n",
       " u'yd',\n",
       " u'rewrite',\n",
       " u'value',\n",
       " u'1lxs',\n",
       " u'ix',\n",
       " u'x',\n",
       " u'correlated',\n",
       " u'sample',\n",
       " u'hence',\n",
       " u'type',\n",
       " u'architecture',\n",
       " u'composed',\n",
       " u'rule',\n",
       " u'rewriting',\n",
       " u'free',\n",
       " u'memory',\n",
       " u'space',\n",
       " u'architecture',\n",
       " u'form',\n",
       " u'naturally',\n",
       " u'kind',\n",
       " u'database',\n",
       " u'build',\n",
       " u'management',\n",
       " u'system',\n",
       " u'data',\n",
       " u'selforganizing',\n",
       " u'way',\n",
       " u'however',\n",
       " u'database',\n",
       " u'differs',\n",
       " u'ordinary',\n",
       " u'one',\n",
       " u'following',\n",
       " u'sense',\n",
       " u'record',\n",
       " u'sample',\n",
       " u'already',\n",
       " u'observed',\n",
       " u'computes',\n",
       " u'estimation',\n",
       " u'lx',\n",
       " u'x',\n",
       " u'e',\n",
       " u'x',\n",
       " u'call',\n",
       " u'database',\n",
       " u'associative',\n",
       " u'database',\n",
       " u'first',\n",
       " u'subject',\n",
       " u'constructing',\n",
       " u'associative',\n",
       " u'database',\n",
       " u'establish',\n",
       " u'rule',\n",
       " u'rewri',\n",
       " u'ting',\n",
       " u'purpose',\n",
       " u'adap',\n",
       " u'measure',\n",
       " u'called',\n",
       " u'dissimilari',\n",
       " u'ty',\n",
       " u'here',\n",
       " u'dissimilari',\n",
       " u'ty',\n",
       " u'mean',\n",
       " u'mapping',\n",
       " u'x',\n",
       " u'x',\n",
       " u'x',\n",
       " u'real',\n",
       " u'o',\n",
       " u'x',\n",
       " u'x',\n",
       " u'e',\n",
       " u'x',\n",
       " u'x',\n",
       " u'x',\n",
       " u'dx',\n",
       " u'x',\n",
       " u'0',\n",
       " u'whenever',\n",
       " u'lx',\n",
       " u'x',\n",
       " u'however',\n",
       " u'necessarily',\n",
       " u'defined',\n",
       " u'single',\n",
       " u'formula',\n",
       " u'definable',\n",
       " u'with',\n",
       " u'example',\n",
       " u'collection',\n",
       " u'rule',\n",
       " u'written',\n",
       " u'form',\n",
       " u'if',\n",
       " u'then',\n",
       " u'dissimilarity',\n",
       " u'defines',\n",
       " u'structure',\n",
       " u'1',\n",
       " u'locally',\n",
       " u'x',\n",
       " u'x',\n",
       " u'y',\n",
       " u'hence',\n",
       " u'even',\n",
       " u'though',\n",
       " u'knowledge',\n",
       " u'f',\n",
       " u'imperfect',\n",
       " u'reflect',\n",
       " u'heuristic',\n",
       " u'way',\n",
       " u'hence',\n",
       " u'contrarily',\n",
       " u'neural',\n",
       " u'network',\n",
       " u'possible',\n",
       " u'accelerate',\n",
       " u'speed',\n",
       " u'learning',\n",
       " u'establishing',\n",
       " u'well',\n",
       " u'especially',\n",
       " u'easily',\n",
       " u'find',\n",
       " u'simple',\n",
       " u'd',\n",
       " u'l',\n",
       " u'process',\n",
       " u'analogically',\n",
       " u'information',\n",
       " u'like',\n",
       " u'human',\n",
       " u'see',\n",
       " u'application',\n",
       " u'paper',\n",
       " u'and',\n",
       " u's',\n",
       " u'recursive',\n",
       " u'type',\n",
       " u'show',\n",
       " u'strongly',\n",
       " u'effectiveness',\n",
       " u'denote',\n",
       " u'sequence',\n",
       " u'observed',\n",
       " u'sample',\n",
       " u'xl',\n",
       " u'yd',\n",
       " u'x2',\n",
       " u'y2',\n",
       " u'one',\n",
       " u'simplest',\n",
       " u'construction',\n",
       " u'associative',\n",
       " u'database',\n",
       " u'observing',\n",
       " u'isamples',\n",
       " u'i',\n",
       " u'12',\n",
       " u'follows',\n",
       " u'i',\n",
       " u'i',\n",
       " u'algorithm',\n",
       " u'1',\n",
       " u'initial',\n",
       " u'stage',\n",
       " u'let',\n",
       " u'empty',\n",
       " u'set',\n",
       " u'every',\n",
       " u'12',\n",
       " u'let',\n",
       " u'ilx',\n",
       " u'x',\n",
       " u'e',\n",
       " u'x',\n",
       " u'equal',\n",
       " u'y',\n",
       " u'xy',\n",
       " u'e',\n",
       " u'sl',\n",
       " u'dx',\n",
       " u'x',\n",
       " u'min',\n",
       " u'yest',\n",
       " u'dx',\n",
       " u'x',\n",
       " u'furthermore',\n",
       " u'add',\n",
       " u'x',\n",
       " u'y',\n",
       " u'sl',\n",
       " u'produce',\n",
       " u'sa',\n",
       " u'ie',\n",
       " u's',\n",
       " u'sl',\n",
       " u'u',\n",
       " u'x',\n",
       " u'1',\n",
       " u'yn',\n",
       " u'769',\n",
       " u'another',\n",
       " u'version',\n",
       " u'improved',\n",
       " u'economize',\n",
       " u'memory',\n",
       " u'follows',\n",
       " u'algorithm',\n",
       " u'2',\n",
       " u'initial',\n",
       " u'stage',\n",
       " u'let',\n",
       " u'composed',\n",
       " u'arbitrary',\n",
       " u'element',\n",
       " u'x',\n",
       " u'x',\n",
       " u'y',\n",
       " u'every',\n",
       " u'12',\n",
       " u'let',\n",
       " u'iilex',\n",
       " u'x',\n",
       " u'e',\n",
       " u'x',\n",
       " u'equal',\n",
       " u'y',\n",
       " u'x',\n",
       " u'y',\n",
       " u'e',\n",
       " u'sil',\n",
       " u'dx',\n",
       " u'x',\n",
       " u'min',\n",
       " u'dx',\n",
       " u'x',\n",
       " u'iiesl',\n",
       " u'furthermore',\n",
       " u'iilxi',\n",
       " u'yi',\n",
       " u'let',\n",
       " u'si',\n",
       " u'sil',\n",
       " u'add',\n",
       " u'xi',\n",
       " u'yi',\n",
       " u'sil',\n",
       " u'produce',\n",
       " u'si',\n",
       " u'ie',\n",
       " u'si',\n",
       " u'sil',\n",
       " u'u',\n",
       " u'xi',\n",
       " u'yi',\n",
       " u'either',\n",
       " u'construction',\n",
       " u'ii',\n",
       " u'approach',\n",
       " u'f',\n",
       " u'increase',\n",
       " u'however',\n",
       " u'computation',\n",
       " u'time',\n",
       " u'grows',\n",
       " u'proportionally',\n",
       " u'size',\n",
       " u'si',\n",
       " u'second',\n",
       " u'subject',\n",
       " u'constructing',\n",
       " u'associative',\n",
       " u'database',\n",
       " u'addressing',\n",
       " u'rule',\n",
       " u'employ',\n",
       " u'economize',\n",
       " u'computation',\n",
       " u'time',\n",
       " u'subsequent',\n",
       " u'chapter',\n",
       " u'construction',\n",
       " u'associative',\n",
       " u'database',\n",
       " u'purpose',\n",
       " u'proposed',\n",
       " u'manages',\n",
       " u'data',\n",
       " u'form',\n",
       " u'binary',\n",
       " u'tree',\n",
       " u'selforganization',\n",
       " u'associative',\n",
       " u'database',\n",
       " u'given',\n",
       " u'sample',\n",
       " u'sequence',\n",
       " u'xl',\n",
       " u'yl',\n",
       " u'x2',\n",
       " u'y2',\n",
       " u'algorithm',\n",
       " u'constructing',\n",
       " u'associative',\n",
       " u'database',\n",
       " u'follows',\n",
       " u'algorithm',\n",
       " u'3',\n",
       " u'step',\n",
       " u'iinitialization',\n",
       " u'let',\n",
       " u'xroot',\n",
       " u'yroot',\n",
       " u'xl',\n",
       " u'yd',\n",
       " u'here',\n",
       " u'x',\n",
       " u'y',\n",
       " u'variable',\n",
       " u'assigned',\n",
       " u'respective',\n",
       " u'node',\n",
       " u'memorize',\n",
       " u'data',\n",
       " u'furthermore',\n",
       " u'let',\n",
       " u'1',\n",
       " u'step',\n",
       " u'2',\n",
       " u'increase',\n",
       " u'1',\n",
       " u'put',\n",
       " u'x',\n",
       " u'in',\n",
       " u'reset',\n",
       " u'pointer',\n",
       " u'n',\n",
       " u'root',\n",
       " u'repeat',\n",
       " u'following',\n",
       " u'n',\n",
       " u'arrives',\n",
       " u'terminal',\n",
       " u'node',\n",
       " u'ie',\n",
       " u'leaf',\n",
       " u'notation',\n",
       " u'nand',\n",
       " u'dxt',\n",
       " u'xn',\n",
       " u'let',\n",
       " u'n',\n",
       " u'n',\n",
       " u'mean',\n",
       " u'descendant',\n",
       " u'node',\n",
       " u'n',\n",
       " u'n',\n",
       " u'otherwise',\n",
       " u'let',\n",
       " u'n',\n",
       " u'n',\n",
       " u'dx',\n",
       " u'rn',\n",
       " u'step',\n",
       " u'3',\n",
       " u'display',\n",
       " u'yin',\n",
       " u'related',\n",
       " u'information',\n",
       " u'next',\n",
       " u'put',\n",
       " u'y',\n",
       " u'in',\n",
       " u'yin',\n",
       " u'y',\n",
       " u'back',\n",
       " u'step',\n",
       " u'2',\n",
       " u'otherwise',\n",
       " u'first',\n",
       " u'establish',\n",
       " u'new',\n",
       " u'descendant',\n",
       " u'node',\n",
       " u'n',\n",
       " u'n',\n",
       " u'secondly',\n",
       " u'let',\n",
       " u'xn',\n",
       " u'yin',\n",
       " u'xn',\n",
       " u'yin',\n",
       " u'xn',\n",
       " u'yin',\n",
       " u'xt',\n",
       " u'y',\n",
       " u'2',\n",
       " u'3',\n",
       " u'finally',\n",
       " u'back',\n",
       " u'step',\n",
       " u'2',\n",
       " u'here',\n",
       " u'loop',\n",
       " u'step',\n",
       " u'23',\n",
       " u'stopped',\n",
       " u'time',\n",
       " u'also',\n",
       " u'continued',\n",
       " u'now',\n",
       " u'suppose',\n",
       " u'gate',\n",
       " u'element',\n",
       " u'namely',\n",
       " u'artificial',\n",
       " u'synapsis',\n",
       " u'play',\n",
       " u'role',\n",
       " u'branching',\n",
       " u'prepared',\n",
       " u'then',\n",
       " u'obtain',\n",
       " u'new',\n",
       " u'style',\n",
       " u'neural',\n",
       " u'network',\n",
       " u'gate',\n",
       " u'element',\n",
       " u'randomly',\n",
       " u'connected',\n",
       " u'algorithm',\n",
       " u'letter',\n",
       " u'recognition',\n",
       " u'recen',\n",
       " u'tly',\n",
       " u'vertical',\n",
       " u'slitting',\n",
       " u'method',\n",
       " u'recognizing',\n",
       " u'typographic',\n",
       " u'english',\n",
       " u'letters3',\n",
       " u'elastic',\n",
       " u'matching',\n",
       " u'method',\n",
       " u'recognizing',\n",
       " u'hand',\n",
       " u'written',\n",
       " u'discrete',\n",
       " u'english',\n",
       " u'letters4',\n",
       " u'global',\n",
       " u'training',\n",
       " u'fuzzy',\n",
       " u'logic',\n",
       " u'search',\n",
       " u'method',\n",
       " u'recognizing',\n",
       " u'chinese',\n",
       " u'character',\n",
       " u'written',\n",
       " ...]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_clean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 5),\n",
       " (1, 22),\n",
       " (2, 1),\n",
       " (3, 3),\n",
       " (4, 3),\n",
       " (5, 1),\n",
       " (6, 1),\n",
       " (7, 1),\n",
       " (8, 4),\n",
       " (9, 1),\n",
       " (10, 2),\n",
       " (11, 1),\n",
       " (12, 1),\n",
       " (13, 1),\n",
       " (14, 2),\n",
       " (15, 1),\n",
       " (16, 4),\n",
       " (17, 9),\n",
       " (18, 1),\n",
       " (19, 1),\n",
       " (20, 1),\n",
       " (21, 11),\n",
       " (22, 2),\n",
       " (23, 1),\n",
       " (24, 1),\n",
       " (25, 1),\n",
       " (26, 4),\n",
       " (27, 1),\n",
       " (28, 1),\n",
       " (29, 1),\n",
       " (30, 1),\n",
       " (31, 1),\n",
       " (32, 1),\n",
       " (33, 12),\n",
       " (34, 2),\n",
       " (35, 1),\n",
       " (36, 1),\n",
       " (37, 1),\n",
       " (38, 1),\n",
       " (39, 1),\n",
       " (40, 1),\n",
       " (41, 1),\n",
       " (42, 1),\n",
       " (43, 1),\n",
       " (44, 1),\n",
       " (45, 4),\n",
       " (46, 2),\n",
       " (47, 3),\n",
       " (48, 1),\n",
       " (49, 1),\n",
       " (50, 1),\n",
       " (51, 7),\n",
       " (52, 1),\n",
       " (53, 1),\n",
       " (54, 1),\n",
       " (55, 1),\n",
       " (56, 7),\n",
       " (57, 1),\n",
       " (58, 1),\n",
       " (59, 1),\n",
       " (60, 1),\n",
       " (61, 1),\n",
       " (62, 1),\n",
       " (63, 1),\n",
       " (64, 1),\n",
       " (65, 1),\n",
       " (66, 1),\n",
       " (67, 1),\n",
       " (68, 1),\n",
       " (69, 1),\n",
       " (70, 1),\n",
       " (71, 1),\n",
       " (72, 1),\n",
       " (73, 3),\n",
       " (74, 2),\n",
       " (75, 1),\n",
       " (76, 2),\n",
       " (77, 1),\n",
       " (78, 1),\n",
       " (79, 1),\n",
       " (80, 2),\n",
       " (81, 1),\n",
       " (82, 6),\n",
       " (83, 1),\n",
       " (84, 1),\n",
       " (85, 1),\n",
       " (86, 2),\n",
       " (87, 1),\n",
       " (88, 6),\n",
       " (89, 1),\n",
       " (90, 1),\n",
       " (91, 3),\n",
       " (92, 1),\n",
       " (93, 2),\n",
       " (94, 2),\n",
       " (95, 1),\n",
       " (96, 1),\n",
       " (97, 1),\n",
       " (98, 2),\n",
       " (99, 4),\n",
       " (100, 8),\n",
       " (101, 1),\n",
       " (102, 1),\n",
       " (103, 2),\n",
       " (104, 2),\n",
       " (105, 1),\n",
       " (106, 1),\n",
       " (107, 1),\n",
       " (108, 1),\n",
       " (109, 1),\n",
       " (110, 8),\n",
       " (111, 1),\n",
       " (112, 1),\n",
       " (113, 6),\n",
       " (114, 2),\n",
       " (115, 1),\n",
       " (116, 1),\n",
       " (117, 1),\n",
       " (118, 5),\n",
       " (119, 3),\n",
       " (120, 1),\n",
       " (121, 1),\n",
       " (122, 1),\n",
       " (123, 1),\n",
       " (124, 2),\n",
       " (125, 7),\n",
       " (126, 1),\n",
       " (127, 1),\n",
       " (128, 3),\n",
       " (129, 1),\n",
       " (130, 1),\n",
       " (131, 1),\n",
       " (132, 1),\n",
       " (133, 1),\n",
       " (134, 1),\n",
       " (135, 1),\n",
       " (136, 13),\n",
       " (137, 1),\n",
       " (138, 3),\n",
       " (139, 1),\n",
       " (140, 1),\n",
       " (141, 1),\n",
       " (142, 2),\n",
       " (143, 2),\n",
       " (144, 1),\n",
       " (145, 4),\n",
       " (146, 5),\n",
       " (147, 4),\n",
       " (148, 6),\n",
       " (149, 2),\n",
       " (150, 1),\n",
       " (151, 1),\n",
       " (152, 2),\n",
       " (153, 2),\n",
       " (154, 4),\n",
       " (155, 1),\n",
       " (156, 3),\n",
       " (157, 1),\n",
       " (158, 3),\n",
       " (159, 9),\n",
       " (160, 1),\n",
       " (161, 2),\n",
       " (162, 6),\n",
       " (163, 1),\n",
       " (164, 1),\n",
       " (165, 1),\n",
       " (166, 1),\n",
       " (167, 1),\n",
       " (168, 1),\n",
       " (169, 2),\n",
       " (170, 5),\n",
       " (171, 1),\n",
       " (172, 1),\n",
       " (173, 2),\n",
       " (174, 1),\n",
       " (175, 11),\n",
       " (176, 4),\n",
       " (177, 1),\n",
       " (178, 1),\n",
       " (179, 1),\n",
       " (180, 7),\n",
       " (181, 1),\n",
       " (182, 1),\n",
       " (183, 1),\n",
       " (184, 1),\n",
       " (185, 4),\n",
       " (186, 1),\n",
       " (187, 3),\n",
       " (188, 1),\n",
       " (189, 1),\n",
       " (190, 1),\n",
       " (191, 1),\n",
       " (192, 1),\n",
       " (193, 1),\n",
       " (194, 1),\n",
       " (195, 2),\n",
       " (196, 1),\n",
       " (197, 2),\n",
       " (198, 1),\n",
       " (199, 2),\n",
       " (200, 2),\n",
       " (201, 2),\n",
       " (202, 3),\n",
       " (203, 2),\n",
       " (204, 1),\n",
       " (205, 1),\n",
       " (206, 2),\n",
       " (207, 1),\n",
       " (208, 3),\n",
       " (209, 1),\n",
       " (210, 2),\n",
       " (211, 1),\n",
       " (212, 1),\n",
       " (213, 3),\n",
       " (214, 2),\n",
       " (215, 2),\n",
       " (216, 1),\n",
       " (217, 3),\n",
       " (218, 1),\n",
       " (219, 1),\n",
       " (220, 1),\n",
       " (221, 1),\n",
       " (222, 1),\n",
       " (223, 1),\n",
       " (224, 2),\n",
       " (225, 1),\n",
       " (226, 1),\n",
       " (227, 3),\n",
       " (228, 3),\n",
       " (229, 1),\n",
       " (230, 1),\n",
       " (231, 1),\n",
       " (232, 3),\n",
       " (233, 1),\n",
       " (234, 2),\n",
       " (235, 2),\n",
       " (236, 4),\n",
       " (237, 1),\n",
       " (238, 1),\n",
       " (239, 4),\n",
       " (240, 2),\n",
       " (241, 1),\n",
       " (242, 1),\n",
       " (243, 2),\n",
       " (244, 3),\n",
       " (245, 1),\n",
       " (246, 1),\n",
       " (247, 1),\n",
       " (248, 6),\n",
       " (249, 1),\n",
       " (250, 7),\n",
       " (251, 19),\n",
       " (252, 1),\n",
       " (253, 2),\n",
       " (254, 2),\n",
       " (255, 1),\n",
       " (256, 14),\n",
       " (257, 3),\n",
       " (258, 2),\n",
       " (259, 1),\n",
       " (260, 1),\n",
       " (261, 1),\n",
       " (262, 1),\n",
       " (263, 2),\n",
       " (264, 1),\n",
       " (265, 2),\n",
       " (266, 1),\n",
       " (267, 2),\n",
       " (268, 2),\n",
       " (269, 2),\n",
       " (270, 2),\n",
       " (271, 1),\n",
       " (272, 1),\n",
       " (273, 1),\n",
       " (274, 1),\n",
       " (275, 1),\n",
       " (276, 1),\n",
       " (277, 3),\n",
       " (278, 1),\n",
       " (279, 2),\n",
       " (280, 1),\n",
       " (281, 1),\n",
       " (282, 1),\n",
       " (283, 2),\n",
       " (284, 1),\n",
       " (285, 2),\n",
       " (286, 1),\n",
       " (287, 2),\n",
       " (288, 1),\n",
       " (289, 1),\n",
       " (290, 3),\n",
       " (291, 1),\n",
       " (292, 1),\n",
       " (293, 3),\n",
       " (294, 1),\n",
       " (295, 1),\n",
       " (296, 1),\n",
       " (297, 1),\n",
       " (298, 1),\n",
       " (299, 1),\n",
       " (300, 9),\n",
       " (301, 1),\n",
       " (302, 17),\n",
       " (303, 1),\n",
       " (304, 1),\n",
       " (305, 1),\n",
       " (306, 2),\n",
       " (307, 1),\n",
       " (308, 2),\n",
       " (309, 2),\n",
       " (310, 1),\n",
       " (311, 2),\n",
       " (312, 3),\n",
       " (313, 1),\n",
       " (314, 1),\n",
       " (315, 1),\n",
       " (316, 1),\n",
       " (317, 1),\n",
       " (318, 3),\n",
       " (319, 1),\n",
       " (320, 2),\n",
       " (321, 3),\n",
       " (322, 1),\n",
       " (323, 1),\n",
       " (324, 8),\n",
       " (325, 1),\n",
       " (326, 1),\n",
       " (327, 4),\n",
       " (328, 1),\n",
       " (329, 1),\n",
       " (330, 1),\n",
       " (331, 1),\n",
       " (332, 8),\n",
       " (333, 4),\n",
       " (334, 2),\n",
       " (335, 2),\n",
       " (336, 2),\n",
       " (337, 4),\n",
       " (338, 4),\n",
       " (339, 1),\n",
       " (340, 1),\n",
       " (341, 7),\n",
       " (342, 1),\n",
       " (343, 1),\n",
       " (344, 2),\n",
       " (345, 1),\n",
       " (346, 1),\n",
       " (347, 1),\n",
       " (348, 2),\n",
       " (349, 4),\n",
       " (350, 3),\n",
       " (351, 1),\n",
       " (352, 1),\n",
       " (353, 1),\n",
       " (354, 2),\n",
       " (355, 21),\n",
       " (356, 1),\n",
       " (357, 3),\n",
       " (358, 1),\n",
       " (359, 1),\n",
       " (360, 1),\n",
       " (361, 1),\n",
       " (362, 1),\n",
       " (363, 1),\n",
       " (364, 1),\n",
       " (365, 1),\n",
       " (366, 1),\n",
       " (367, 2),\n",
       " (368, 2),\n",
       " (369, 21),\n",
       " (370, 1),\n",
       " (371, 1),\n",
       " (372, 1),\n",
       " (373, 5),\n",
       " (374, 4),\n",
       " (375, 1),\n",
       " (376, 6),\n",
       " (377, 1),\n",
       " (378, 1),\n",
       " (379, 1),\n",
       " (380, 1),\n",
       " (381, 7),\n",
       " (382, 4),\n",
       " (383, 3),\n",
       " (384, 1),\n",
       " (385, 1),\n",
       " (386, 1),\n",
       " (387, 1),\n",
       " (388, 1),\n",
       " (389, 2),\n",
       " (390, 3),\n",
       " (391, 1),\n",
       " (392, 1),\n",
       " (393, 3),\n",
       " (394, 2),\n",
       " (395, 1),\n",
       " (396, 1),\n",
       " (397, 2),\n",
       " (398, 1),\n",
       " (399, 2),\n",
       " (400, 3),\n",
       " (401, 3),\n",
       " (402, 1),\n",
       " (403, 1),\n",
       " (404, 1),\n",
       " (405, 1),\n",
       " (406, 1),\n",
       " (407, 1),\n",
       " (408, 2),\n",
       " (409, 2),\n",
       " (410, 1),\n",
       " (411, 2),\n",
       " (412, 1),\n",
       " (413, 3),\n",
       " (414, 2),\n",
       " (415, 2),\n",
       " (416, 1),\n",
       " (417, 7),\n",
       " (418, 6),\n",
       " (419, 2),\n",
       " (420, 1),\n",
       " (421, 1),\n",
       " (422, 1),\n",
       " (423, 1),\n",
       " (424, 10),\n",
       " (425, 1),\n",
       " (426, 1),\n",
       " (427, 1),\n",
       " (428, 6),\n",
       " (429, 1),\n",
       " (430, 8),\n",
       " (431, 1),\n",
       " (432, 1),\n",
       " (433, 1),\n",
       " (434, 4),\n",
       " (435, 6),\n",
       " (436, 2),\n",
       " (437, 2),\n",
       " (438, 1),\n",
       " (439, 1),\n",
       " (440, 1),\n",
       " (441, 1),\n",
       " (442, 1),\n",
       " (443, 1),\n",
       " (444, 1),\n",
       " (445, 23),\n",
       " (446, 1),\n",
       " (447, 1),\n",
       " (448, 2),\n",
       " (449, 1),\n",
       " (450, 1),\n",
       " (451, 2),\n",
       " (452, 1),\n",
       " (453, 1),\n",
       " (454, 4),\n",
       " (455, 1),\n",
       " (456, 1),\n",
       " (457, 2),\n",
       " (458, 7),\n",
       " (459, 1),\n",
       " (460, 1),\n",
       " (461, 3),\n",
       " (462, 3),\n",
       " (463, 3),\n",
       " (464, 1),\n",
       " (465, 1),\n",
       " (466, 1),\n",
       " (467, 1),\n",
       " (468, 2),\n",
       " (469, 1),\n",
       " (470, 1),\n",
       " (471, 4),\n",
       " (472, 1),\n",
       " (473, 1),\n",
       " (474, 1),\n",
       " (475, 1),\n",
       " (476, 2),\n",
       " (477, 1),\n",
       " (478, 1),\n",
       " (479, 1),\n",
       " (480, 1),\n",
       " (481, 2),\n",
       " (482, 2),\n",
       " (483, 1),\n",
       " (484, 1),\n",
       " (485, 2),\n",
       " (486, 15),\n",
       " (487, 1),\n",
       " (488, 1),\n",
       " (489, 1),\n",
       " (490, 2),\n",
       " (491, 2),\n",
       " (492, 2),\n",
       " (493, 2),\n",
       " (494, 1),\n",
       " (495, 1),\n",
       " (496, 2),\n",
       " (497, 1),\n",
       " (498, 1),\n",
       " (499, 1),\n",
       " (500, 1),\n",
       " (501, 3),\n",
       " (502, 1),\n",
       " (503, 1),\n",
       " (504, 1),\n",
       " (505, 3),\n",
       " (506, 1),\n",
       " (507, 17),\n",
       " (508, 2),\n",
       " (509, 1),\n",
       " (510, 2),\n",
       " (511, 1),\n",
       " (512, 18),\n",
       " (513, 11),\n",
       " (514, 1),\n",
       " (515, 1),\n",
       " (516, 4),\n",
       " (517, 2),\n",
       " (518, 1),\n",
       " (519, 1),\n",
       " (520, 3),\n",
       " (521, 1),\n",
       " (522, 1),\n",
       " (523, 1),\n",
       " (524, 1),\n",
       " (525, 1),\n",
       " (526, 1),\n",
       " (527, 1),\n",
       " (528, 1),\n",
       " (529, 1),\n",
       " (530, 1),\n",
       " (531, 1),\n",
       " (532, 2),\n",
       " (533, 2),\n",
       " (534, 2),\n",
       " (535, 1),\n",
       " (536, 11),\n",
       " (537, 1),\n",
       " (538, 1),\n",
       " (539, 1),\n",
       " (540, 2),\n",
       " (541, 1),\n",
       " (542, 1),\n",
       " (543, 1),\n",
       " (544, 2),\n",
       " (545, 4),\n",
       " (546, 1),\n",
       " (547, 2),\n",
       " (548, 1),\n",
       " (549, 1),\n",
       " (550, 1),\n",
       " (551, 4),\n",
       " (552, 6),\n",
       " (553, 1),\n",
       " (554, 3),\n",
       " (555, 9),\n",
       " (556, 2),\n",
       " (557, 2),\n",
       " (558, 1),\n",
       " (559, 5),\n",
       " (560, 1),\n",
       " (561, 2),\n",
       " (562, 1),\n",
       " (563, 5),\n",
       " (564, 2),\n",
       " (565, 2),\n",
       " (566, 4),\n",
       " (567, 1),\n",
       " (568, 2),\n",
       " (569, 13),\n",
       " (570, 4),\n",
       " (571, 2),\n",
       " (572, 1),\n",
       " (573, 1),\n",
       " (574, 1),\n",
       " (575, 2),\n",
       " (576, 1),\n",
       " (577, 1),\n",
       " (578, 1),\n",
       " (579, 1),\n",
       " (580, 2),\n",
       " (581, 8),\n",
       " (582, 9),\n",
       " (583, 1),\n",
       " (584, 1),\n",
       " (585, 4),\n",
       " (586, 1),\n",
       " (587, 7),\n",
       " (588, 1),\n",
       " (589, 1),\n",
       " (590, 1),\n",
       " (591, 2),\n",
       " (592, 1),\n",
       " (593, 1),\n",
       " (594, 3),\n",
       " (595, 1),\n",
       " (596, 14),\n",
       " (597, 1),\n",
       " (598, 1),\n",
       " (599, 1),\n",
       " (600, 5),\n",
       " (601, 2),\n",
       " (602, 4),\n",
       " (603, 7),\n",
       " (604, 1),\n",
       " (605, 2),\n",
       " (606, 1),\n",
       " (607, 1),\n",
       " (608, 1),\n",
       " (609, 7),\n",
       " (610, 2),\n",
       " (611, 2),\n",
       " (612, 10),\n",
       " (613, 1),\n",
       " (614, 3),\n",
       " (615, 1),\n",
       " (616, 2),\n",
       " (617, 2),\n",
       " (618, 1),\n",
       " (619, 1),\n",
       " (620, 1),\n",
       " (621, 1),\n",
       " (622, 2),\n",
       " (623, 1),\n",
       " (624, 1),\n",
       " (625, 8),\n",
       " (626, 3),\n",
       " (627, 1),\n",
       " (628, 1),\n",
       " (629, 1),\n",
       " (630, 3),\n",
       " (631, 3),\n",
       " (632, 1),\n",
       " (633, 1),\n",
       " (634, 1),\n",
       " (635, 1),\n",
       " (636, 1),\n",
       " (637, 1),\n",
       " (638, 1),\n",
       " (639, 1),\n",
       " (640, 2),\n",
       " (641, 1),\n",
       " (642, 1),\n",
       " (643, 12),\n",
       " (644, 1),\n",
       " (645, 15),\n",
       " (646, 1),\n",
       " (647, 1),\n",
       " (648, 3),\n",
       " (649, 10),\n",
       " (650, 2),\n",
       " (651, 1),\n",
       " (652, 2),\n",
       " (653, 1),\n",
       " (654, 2),\n",
       " (655, 1),\n",
       " (656, 2),\n",
       " (657, 1),\n",
       " (658, 1),\n",
       " (659, 1),\n",
       " (660, 1),\n",
       " (661, 1),\n",
       " (662, 6),\n",
       " (663, 1),\n",
       " (664, 1),\n",
       " (665, 3),\n",
       " (666, 2),\n",
       " (667, 2),\n",
       " (668, 3),\n",
       " (669, 1),\n",
       " (670, 2),\n",
       " (671, 4),\n",
       " (672, 1),\n",
       " (673, 1),\n",
       " (674, 7),\n",
       " (675, 1),\n",
       " (676, 4),\n",
       " (677, 2),\n",
       " (678, 1),\n",
       " (679, 1),\n",
       " (680, 3),\n",
       " (681, 1),\n",
       " (682, 1),\n",
       " (683, 2),\n",
       " (684, 1),\n",
       " (685, 9),\n",
       " (686, 1),\n",
       " (687, 1),\n",
       " (688, 4),\n",
       " (689, 1),\n",
       " (690, 1),\n",
       " (691, 1),\n",
       " (692, 2),\n",
       " (693, 2),\n",
       " (694, 1),\n",
       " (695, 1),\n",
       " (696, 1),\n",
       " (697, 11),\n",
       " (698, 4),\n",
       " (699, 3),\n",
       " (700, 1),\n",
       " (701, 1),\n",
       " (702, 1),\n",
       " (703, 4),\n",
       " (704, 1),\n",
       " (705, 2),\n",
       " (706, 1),\n",
       " (707, 1),\n",
       " (708, 1),\n",
       " (709, 1),\n",
       " (710, 7),\n",
       " (711, 1),\n",
       " (712, 1),\n",
       " (713, 1),\n",
       " (714, 2),\n",
       " (715, 3),\n",
       " (716, 1),\n",
       " (717, 1),\n",
       " (718, 1),\n",
       " (719, 1),\n",
       " (720, 1),\n",
       " (721, 1),\n",
       " (722, 1),\n",
       " (723, 1),\n",
       " (724, 2),\n",
       " (725, 3),\n",
       " (726, 1),\n",
       " (727, 2),\n",
       " (728, 1),\n",
       " (729, 1),\n",
       " (730, 1),\n",
       " (731, 1),\n",
       " (732, 1),\n",
       " (733, 1),\n",
       " (734, 1),\n",
       " (735, 1),\n",
       " (736, 1),\n",
       " (737, 1),\n",
       " (738, 2),\n",
       " (739, 23),\n",
       " (740, 5),\n",
       " (741, 2),\n",
       " (742, 1),\n",
       " (743, 1),\n",
       " (744, 6),\n",
       " (745, 6),\n",
       " (746, 1),\n",
       " (747, 1),\n",
       " (748, 3),\n",
       " (749, 1),\n",
       " (750, 1),\n",
       " (751, 18),\n",
       " (752, 1),\n",
       " (753, 2),\n",
       " (754, 1),\n",
       " (755, 1),\n",
       " (756, 1),\n",
       " (757, 3),\n",
       " (758, 1),\n",
       " (759, 4),\n",
       " (760, 2),\n",
       " (761, 1),\n",
       " (762, 3),\n",
       " (763, 1),\n",
       " (764, 1),\n",
       " (765, 1),\n",
       " (766, 7),\n",
       " (767, 3),\n",
       " (768, 2),\n",
       " (769, 4),\n",
       " (770, 9),\n",
       " (771, 1),\n",
       " (772, 5),\n",
       " (773, 1),\n",
       " (774, 1),\n",
       " (775, 4),\n",
       " (776, 4),\n",
       " (777, 3),\n",
       " (778, 1),\n",
       " (779, 1),\n",
       " (780, 1),\n",
       " (781, 5),\n",
       " (782, 1),\n",
       " (783, 1),\n",
       " (784, 1),\n",
       " (785, 3),\n",
       " (786, 2),\n",
       " (787, 1),\n",
       " (788, 1),\n",
       " (789, 1),\n",
       " (790, 3),\n",
       " (791, 1),\n",
       " (792, 1),\n",
       " (793, 2),\n",
       " (794, 1),\n",
       " (795, 2),\n",
       " (796, 1),\n",
       " (797, 2),\n",
       " (798, 1),\n",
       " (799, 1),\n",
       " (800, 2),\n",
       " (801, 1),\n",
       " (802, 1),\n",
       " (803, 1),\n",
       " (804, 3),\n",
       " (805, 1),\n",
       " (806, 1),\n",
       " (807, 1),\n",
       " (808, 1),\n",
       " (809, 6),\n",
       " (810, 2),\n",
       " (811, 1),\n",
       " (812, 1),\n",
       " (813, 1),\n",
       " (814, 1),\n",
       " (815, 13),\n",
       " (816, 3),\n",
       " (817, 1),\n",
       " (818, 1),\n",
       " (819, 4),\n",
       " (820, 1),\n",
       " (821, 1),\n",
       " (822, 1),\n",
       " (823, 1),\n",
       " (824, 1),\n",
       " (825, 1),\n",
       " (826, 1),\n",
       " (827, 1),\n",
       " (828, 1),\n",
       " (829, 1),\n",
       " (830, 1),\n",
       " (831, 1),\n",
       " (832, 1),\n",
       " (833, 2),\n",
       " (834, 1),\n",
       " (835, 1),\n",
       " (836, 4),\n",
       " (837, 1),\n",
       " (838, 1),\n",
       " (839, 1),\n",
       " (840, 1),\n",
       " (841, 1),\n",
       " (842, 1),\n",
       " (843, 1),\n",
       " (844, 8),\n",
       " (845, 4),\n",
       " (846, 1),\n",
       " (847, 3),\n",
       " (848, 1),\n",
       " (849, 4),\n",
       " (850, 1),\n",
       " (851, 1),\n",
       " (852, 1),\n",
       " (853, 1),\n",
       " (854, 1),\n",
       " (855, 1),\n",
       " (856, 1),\n",
       " (857, 1),\n",
       " (858, 2),\n",
       " (859, 6),\n",
       " (860, 1),\n",
       " (861, 1),\n",
       " (862, 4),\n",
       " (863, 1),\n",
       " (864, 6),\n",
       " (865, 9),\n",
       " (866, 1),\n",
       " (867, 1),\n",
       " (868, 1),\n",
       " (869, 2),\n",
       " (870, 1),\n",
       " (871, 1),\n",
       " (872, 1),\n",
       " (873, 2),\n",
       " (874, 2),\n",
       " (875, 1),\n",
       " (876, 1),\n",
       " (877, 5),\n",
       " (878, 3),\n",
       " (879, 1),\n",
       " (880, 1),\n",
       " (881, 1),\n",
       " (882, 1),\n",
       " (883, 2),\n",
       " (884, 1),\n",
       " (885, 2),\n",
       " (886, 14),\n",
       " (887, 2),\n",
       " (888, 1),\n",
       " (889, 6),\n",
       " (890, 1),\n",
       " (891, 1),\n",
       " (892, 1),\n",
       " (893, 2),\n",
       " (894, 1),\n",
       " (895, 1),\n",
       " (896, 1),\n",
       " (897, 2),\n",
       " (898, 1),\n",
       " (899, 2),\n",
       " (900, 1),\n",
       " (901, 1),\n",
       " (902, 2),\n",
       " (903, 6),\n",
       " (904, 1),\n",
       " (905, 3),\n",
       " (906, 2),\n",
       " (907, 1),\n",
       " (908, 1),\n",
       " (909, 1),\n",
       " (910, 2),\n",
       " (911, 1),\n",
       " (912, 1),\n",
       " (913, 2),\n",
       " (914, 2),\n",
       " (915, 1),\n",
       " (916, 1),\n",
       " (917, 1),\n",
       " (918, 3),\n",
       " (919, 1),\n",
       " (920, 3),\n",
       " (921, 1),\n",
       " (922, 1),\n",
       " (923, 3),\n",
       " (924, 1),\n",
       " (925, 1),\n",
       " (926, 1),\n",
       " (927, 2),\n",
       " (928, 10),\n",
       " (929, 1),\n",
       " (930, 1),\n",
       " (931, 2),\n",
       " (932, 1),\n",
       " (933, 1),\n",
       " (934, 5),\n",
       " (935, 1),\n",
       " (936, 1),\n",
       " (937, 1),\n",
       " (938, 3),\n",
       " (939, 91),\n",
       " (940, 3),\n",
       " (941, 2),\n",
       " (942, 1),\n",
       " (943, 4),\n",
       " (944, 4),\n",
       " (945, 1),\n",
       " (946, 1),\n",
       " (947, 1),\n",
       " (948, 25),\n",
       " (949, 4),\n",
       " (950, 3),\n",
       " (951, 1),\n",
       " (952, 3),\n",
       " (953, 5),\n",
       " (954, 4),\n",
       " (955, 1),\n",
       " (956, 1),\n",
       " (957, 1)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=10, id2word=dictionary, passes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, u'0.021*\"network\" + 0.009*\"neural\" + 0.008*\"1\" + 0.007*\"input\" + 0.007*\"model\" + 0.005*\"output\" + 0.005*\"unit\" + 0.005*\"time\" + 0.005*\"2\" + 0.004*\"learning\"')\n",
      "(1, u'0.016*\"1\" + 0.012*\"2\" + 0.008*\"model\" + 0.008*\"x\" + 0.006*\"0\" + 0.006*\"p\" + 0.006*\"algorithm\" + 0.006*\"graph\" + 0.005*\"r\" + 0.005*\"n\"')\n",
      "(2, u'0.013*\"image\" + 0.013*\"model\" + 0.010*\"1\" + 0.006*\"2\" + 0.005*\"x\" + 0.005*\"3\" + 0.005*\"feature\" + 0.004*\"a\" + 0.004*\"using\" + 0.004*\"object\"')\n",
      "(3, u'0.014*\"1\" + 0.012*\"model\" + 0.008*\"data\" + 0.008*\"x\" + 0.007*\"learning\" + 0.005*\"distribution\" + 0.005*\"2\" + 0.005*\"k\" + 0.005*\"0\" + 0.005*\"3\"')\n",
      "(4, u'0.022*\"1\" + 0.015*\"2\" + 0.015*\"x\" + 0.010*\"algorithm\" + 0.010*\"k\" + 0.009*\"n\" + 0.009*\"0\" + 0.007*\"f\" + 0.007*\"function\" + 0.006*\"p\"')\n",
      "(5, u'0.016*\"1\" + 0.011*\"0\" + 0.009*\"2\" + 0.009*\"x\" + 0.009*\"n\" + 0.007*\"p\" + 0.007*\"model\" + 0.007*\"j\" + 0.006*\"c\" + 0.006*\"r\"')\n",
      "(6, u'0.012*\"model\" + 0.011*\"1\" + 0.008*\"0\" + 0.008*\"2\" + 0.007*\"data\" + 0.005*\"x\" + 0.005*\"k\" + 0.005*\"p\" + 0.005*\"b\" + 0.004*\"j\"')\n",
      "(7, u'0.010*\"policy\" + 0.009*\"1\" + 0.009*\"learning\" + 0.008*\"state\" + 0.006*\"algorithm\" + 0.006*\"2\" + 0.006*\"action\" + 0.006*\"reward\" + 0.006*\"s\" + 0.005*\"a\"')\n",
      "(8, u'0.010*\"2\" + 0.009*\"1\" + 0.007*\"data\" + 0.006*\"set\" + 0.006*\"learning\" + 0.005*\"method\" + 0.005*\"kernel\" + 0.005*\"training\" + 0.004*\"feature\" + 0.004*\"algorithm\"')\n",
      "(9, u'0.009*\"network\" + 0.008*\"1\" + 0.007*\"model\" + 0.006*\"2\" + 0.005*\"learning\" + 0.004*\"object\" + 0.004*\"3\" + 0.004*\"x\" + 0.004*\"training\" + 0.004*\"j\"')\n"
     ]
    }
   ],
   "source": [
    "for t in ldamodel.print_topics(num_topics=30, num_words=10):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bigrams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_bigrams = [[t1 + '_' + t2 for t1, t2 in zip(doc, doc[1:])] for doc in doc_clean]\n",
    "\n",
    "dictionary = corpora.Dictionary(doc_bigrams)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (4, 1),\n",
       " (5, 2),\n",
       " (6, 1),\n",
       " (7, 1),\n",
       " (8, 2),\n",
       " (9, 1),\n",
       " (10, 1),\n",
       " (11, 1),\n",
       " (12, 1),\n",
       " (13, 1),\n",
       " (14, 1),\n",
       " (15, 2),\n",
       " (16, 1),\n",
       " (17, 1),\n",
       " (18, 1),\n",
       " (19, 1),\n",
       " (20, 1),\n",
       " (21, 1),\n",
       " (22, 2),\n",
       " (23, 1),\n",
       " (24, 4),\n",
       " (25, 3),\n",
       " (26, 6),\n",
       " (27, 1),\n",
       " (28, 1),\n",
       " (29, 1),\n",
       " (30, 1),\n",
       " (31, 1),\n",
       " (32, 1),\n",
       " (33, 1),\n",
       " (34, 1),\n",
       " (35, 1),\n",
       " (36, 1),\n",
       " (37, 1),\n",
       " (38, 1),\n",
       " (39, 1),\n",
       " (40, 1),\n",
       " (41, 1),\n",
       " (42, 1),\n",
       " (43, 1),\n",
       " (44, 1),\n",
       " (45, 1),\n",
       " (46, 1),\n",
       " (47, 1),\n",
       " (48, 1),\n",
       " (49, 1),\n",
       " (50, 1),\n",
       " (51, 1),\n",
       " (52, 2),\n",
       " (53, 1),\n",
       " (54, 1),\n",
       " (55, 1),\n",
       " (56, 1),\n",
       " (57, 1),\n",
       " (58, 1),\n",
       " (59, 1),\n",
       " (60, 1),\n",
       " (61, 1),\n",
       " (62, 1),\n",
       " (63, 1),\n",
       " (64, 1),\n",
       " (65, 1),\n",
       " (66, 1),\n",
       " (67, 1),\n",
       " (68, 1),\n",
       " (69, 1),\n",
       " (70, 1),\n",
       " (71, 1),\n",
       " (72, 1),\n",
       " (73, 1),\n",
       " (74, 1),\n",
       " (75, 1),\n",
       " (76, 2),\n",
       " (77, 1),\n",
       " (78, 1),\n",
       " (79, 1),\n",
       " (80, 1),\n",
       " (81, 1),\n",
       " (82, 1),\n",
       " (83, 1),\n",
       " (84, 1),\n",
       " (85, 1),\n",
       " (86, 1),\n",
       " (87, 1),\n",
       " (88, 1),\n",
       " (89, 1),\n",
       " (90, 1),\n",
       " (91, 1),\n",
       " (92, 1),\n",
       " (93, 1),\n",
       " (94, 1),\n",
       " (95, 1),\n",
       " (96, 1),\n",
       " (97, 1),\n",
       " (98, 1),\n",
       " (99, 1),\n",
       " (100, 1),\n",
       " (101, 1),\n",
       " (102, 1),\n",
       " (103, 1),\n",
       " (104, 1),\n",
       " (105, 1),\n",
       " (106, 1),\n",
       " (107, 1),\n",
       " (108, 1),\n",
       " (109, 1),\n",
       " (110, 1),\n",
       " (111, 1),\n",
       " (112, 1),\n",
       " (113, 1),\n",
       " (114, 1),\n",
       " (115, 1),\n",
       " (116, 2),\n",
       " (117, 1),\n",
       " (118, 1),\n",
       " (119, 1),\n",
       " (120, 1),\n",
       " (121, 1),\n",
       " (122, 1),\n",
       " (123, 1),\n",
       " (124, 1),\n",
       " (125, 1),\n",
       " (126, 1),\n",
       " (127, 2),\n",
       " (128, 1),\n",
       " (129, 1),\n",
       " (130, 1),\n",
       " (131, 1),\n",
       " (132, 1),\n",
       " (133, 1),\n",
       " (134, 1),\n",
       " (135, 1),\n",
       " (136, 1),\n",
       " (137, 1),\n",
       " (138, 1),\n",
       " (139, 1),\n",
       " (140, 1),\n",
       " (141, 1),\n",
       " (142, 1),\n",
       " (143, 1),\n",
       " (144, 1),\n",
       " (145, 1),\n",
       " (146, 1),\n",
       " (147, 1),\n",
       " (148, 1),\n",
       " (149, 1),\n",
       " (150, 1),\n",
       " (151, 1),\n",
       " (152, 1),\n",
       " (153, 1),\n",
       " (154, 1),\n",
       " (155, 1),\n",
       " (156, 1),\n",
       " (157, 1),\n",
       " (158, 2),\n",
       " (159, 1),\n",
       " (160, 1),\n",
       " (161, 1),\n",
       " (162, 1),\n",
       " (163, 1),\n",
       " (164, 1),\n",
       " (165, 1),\n",
       " (166, 1),\n",
       " (167, 1),\n",
       " (168, 2),\n",
       " (169, 1),\n",
       " (170, 1),\n",
       " (171, 1),\n",
       " (172, 1),\n",
       " (173, 1),\n",
       " (174, 1),\n",
       " (175, 1),\n",
       " (176, 2),\n",
       " (177, 1),\n",
       " (178, 1),\n",
       " (179, 1),\n",
       " (180, 2),\n",
       " (181, 1),\n",
       " (182, 1),\n",
       " (183, 1),\n",
       " (184, 1),\n",
       " (185, 1),\n",
       " (186, 1),\n",
       " (187, 1),\n",
       " (188, 1),\n",
       " (189, 1),\n",
       " (190, 1),\n",
       " (191, 1),\n",
       " (192, 3),\n",
       " (193, 1),\n",
       " (194, 1),\n",
       " (195, 1),\n",
       " (196, 1),\n",
       " (197, 1),\n",
       " (198, 1),\n",
       " (199, 1),\n",
       " (200, 2),\n",
       " (201, 1),\n",
       " (202, 1),\n",
       " (203, 1),\n",
       " (204, 1),\n",
       " (205, 1),\n",
       " (206, 2),\n",
       " (207, 1),\n",
       " (208, 1),\n",
       " (209, 1),\n",
       " (210, 1),\n",
       " (211, 1),\n",
       " (212, 1),\n",
       " (213, 1),\n",
       " (214, 1),\n",
       " (215, 1),\n",
       " (216, 1),\n",
       " (217, 1),\n",
       " (218, 1),\n",
       " (219, 1),\n",
       " (220, 1),\n",
       " (221, 1),\n",
       " (222, 1),\n",
       " (223, 1),\n",
       " (224, 1),\n",
       " (225, 1),\n",
       " (226, 1),\n",
       " (227, 1),\n",
       " (228, 1),\n",
       " (229, 2),\n",
       " (230, 1),\n",
       " (231, 2),\n",
       " (232, 1),\n",
       " (233, 1),\n",
       " (234, 1),\n",
       " (235, 1),\n",
       " (236, 1),\n",
       " (237, 1),\n",
       " (238, 1),\n",
       " (239, 1),\n",
       " (240, 1),\n",
       " (241, 1),\n",
       " (242, 1),\n",
       " (243, 1),\n",
       " (244, 1),\n",
       " (245, 1),\n",
       " (246, 1),\n",
       " (247, 1),\n",
       " (248, 2),\n",
       " (249, 1),\n",
       " (250, 1),\n",
       " (251, 1),\n",
       " (252, 1),\n",
       " (253, 1),\n",
       " (254, 1),\n",
       " (255, 1),\n",
       " (256, 13),\n",
       " (257, 1),\n",
       " (258, 1),\n",
       " (259, 2),\n",
       " (260, 1),\n",
       " (261, 1),\n",
       " (262, 1),\n",
       " (263, 1),\n",
       " (264, 1),\n",
       " (265, 1),\n",
       " (266, 1),\n",
       " (267, 1),\n",
       " (268, 3),\n",
       " (269, 1),\n",
       " (270, 1),\n",
       " (271, 3),\n",
       " (272, 1),\n",
       " (273, 4),\n",
       " (274, 1),\n",
       " (275, 1),\n",
       " (276, 1),\n",
       " (277, 1),\n",
       " (278, 2),\n",
       " (279, 2),\n",
       " (280, 1),\n",
       " (281, 1),\n",
       " (282, 1),\n",
       " (283, 1),\n",
       " (284, 1),\n",
       " (285, 1),\n",
       " (286, 1),\n",
       " (287, 1),\n",
       " (288, 1),\n",
       " (289, 1),\n",
       " (290, 1),\n",
       " (291, 1),\n",
       " (292, 2),\n",
       " (293, 1),\n",
       " (294, 1),\n",
       " (295, 1),\n",
       " (296, 1),\n",
       " (297, 1),\n",
       " (298, 8),\n",
       " (299, 1),\n",
       " (300, 1),\n",
       " (301, 1),\n",
       " (302, 1),\n",
       " (303, 1),\n",
       " (304, 1),\n",
       " (305, 1),\n",
       " (306, 1),\n",
       " (307, 1),\n",
       " (308, 1),\n",
       " (309, 1),\n",
       " (310, 1),\n",
       " (311, 1),\n",
       " (312, 1),\n",
       " (313, 1),\n",
       " (314, 1),\n",
       " (315, 1),\n",
       " (316, 1),\n",
       " (317, 1),\n",
       " (318, 1),\n",
       " (319, 1),\n",
       " (320, 1),\n",
       " (321, 1),\n",
       " (322, 1),\n",
       " (323, 1),\n",
       " (324, 1),\n",
       " (325, 1),\n",
       " (326, 1),\n",
       " (327, 1),\n",
       " (328, 6),\n",
       " (329, 1),\n",
       " (330, 1),\n",
       " (331, 1),\n",
       " (332, 1),\n",
       " (333, 1),\n",
       " (334, 1),\n",
       " (335, 1),\n",
       " (336, 1),\n",
       " (337, 1),\n",
       " (338, 1),\n",
       " (339, 1),\n",
       " (340, 1),\n",
       " (341, 1),\n",
       " (342, 3),\n",
       " (343, 1),\n",
       " (344, 1),\n",
       " (345, 1),\n",
       " (346, 1),\n",
       " (347, 1),\n",
       " (348, 1),\n",
       " (349, 1),\n",
       " (350, 1),\n",
       " (351, 1),\n",
       " (352, 1),\n",
       " (353, 1),\n",
       " (354, 1),\n",
       " (355, 1),\n",
       " (356, 1),\n",
       " (357, 1),\n",
       " (358, 1),\n",
       " (359, 1),\n",
       " (360, 1),\n",
       " (361, 1),\n",
       " (362, 1),\n",
       " (363, 1),\n",
       " (364, 1),\n",
       " (365, 1),\n",
       " (366, 1),\n",
       " (367, 1),\n",
       " (368, 1),\n",
       " (369, 1),\n",
       " (370, 1),\n",
       " (371, 1),\n",
       " (372, 1),\n",
       " (373, 1),\n",
       " (374, 1),\n",
       " (375, 1),\n",
       " (376, 1),\n",
       " (377, 1),\n",
       " (378, 1),\n",
       " (379, 1),\n",
       " (380, 1),\n",
       " (381, 1),\n",
       " (382, 1),\n",
       " (383, 1),\n",
       " (384, 1),\n",
       " (385, 1),\n",
       " (386, 1),\n",
       " (387, 1),\n",
       " (388, 1),\n",
       " (389, 2),\n",
       " (390, 1),\n",
       " (391, 1),\n",
       " (392, 1),\n",
       " (393, 1),\n",
       " (394, 1),\n",
       " (395, 1),\n",
       " (396, 1),\n",
       " (397, 1),\n",
       " (398, 1),\n",
       " (399, 1),\n",
       " (400, 3),\n",
       " (401, 1),\n",
       " (402, 1),\n",
       " (403, 1),\n",
       " (404, 1),\n",
       " (405, 1),\n",
       " (406, 1),\n",
       " (407, 1),\n",
       " (408, 1),\n",
       " (409, 1),\n",
       " (410, 1),\n",
       " (411, 3),\n",
       " (412, 2),\n",
       " (413, 1),\n",
       " (414, 1),\n",
       " (415, 1),\n",
       " (416, 1),\n",
       " (417, 1),\n",
       " (418, 1),\n",
       " (419, 1),\n",
       " (420, 1),\n",
       " (421, 1),\n",
       " (422, 1),\n",
       " (423, 1),\n",
       " (424, 1),\n",
       " (425, 1),\n",
       " (426, 1),\n",
       " (427, 1),\n",
       " (428, 1),\n",
       " (429, 1),\n",
       " (430, 1),\n",
       " (431, 3),\n",
       " (432, 1),\n",
       " (433, 1),\n",
       " (434, 1),\n",
       " (435, 1),\n",
       " (436, 1),\n",
       " (437, 1),\n",
       " (438, 1),\n",
       " (439, 1),\n",
       " (440, 2),\n",
       " (441, 1),\n",
       " (442, 1),\n",
       " (443, 1),\n",
       " (444, 1),\n",
       " (445, 1),\n",
       " (446, 1),\n",
       " (447, 1),\n",
       " (448, 1),\n",
       " (449, 1),\n",
       " (450, 1),\n",
       " (451, 1),\n",
       " (452, 1),\n",
       " (453, 1),\n",
       " (454, 1),\n",
       " (455, 1),\n",
       " (456, 1),\n",
       " (457, 1),\n",
       " (458, 1),\n",
       " (459, 1),\n",
       " (460, 1),\n",
       " (461, 1),\n",
       " (462, 1),\n",
       " (463, 1),\n",
       " (464, 1),\n",
       " (465, 1),\n",
       " (466, 1),\n",
       " (467, 1),\n",
       " (468, 1),\n",
       " (469, 1),\n",
       " (470, 1),\n",
       " (471, 1),\n",
       " (472, 2),\n",
       " (473, 1),\n",
       " (474, 1),\n",
       " (475, 1),\n",
       " (476, 1),\n",
       " (477, 1),\n",
       " (478, 1),\n",
       " (479, 1),\n",
       " (480, 1),\n",
       " (481, 1),\n",
       " (482, 1),\n",
       " (483, 3),\n",
       " (484, 3),\n",
       " (485, 1),\n",
       " (486, 1),\n",
       " (487, 1),\n",
       " (488, 1),\n",
       " (489, 3),\n",
       " (490, 1),\n",
       " (491, 1),\n",
       " (492, 1),\n",
       " (493, 1),\n",
       " (494, 1),\n",
       " (495, 1),\n",
       " (496, 1),\n",
       " (497, 1),\n",
       " (498, 1),\n",
       " (499, 1),\n",
       " (500, 1),\n",
       " (501, 1),\n",
       " (502, 1),\n",
       " (503, 1),\n",
       " (504, 1),\n",
       " (505, 2),\n",
       " (506, 1),\n",
       " (507, 1),\n",
       " (508, 1),\n",
       " (509, 1),\n",
       " (510, 1),\n",
       " (511, 1),\n",
       " (512, 1),\n",
       " (513, 1),\n",
       " (514, 1),\n",
       " (515, 1),\n",
       " (516, 1),\n",
       " (517, 1),\n",
       " (518, 1),\n",
       " (519, 1),\n",
       " (520, 1),\n",
       " (521, 1),\n",
       " (522, 1),\n",
       " (523, 1),\n",
       " (524, 1),\n",
       " (525, 1),\n",
       " (526, 1),\n",
       " (527, 1),\n",
       " (528, 1),\n",
       " (529, 1),\n",
       " (530, 2),\n",
       " (531, 1),\n",
       " (532, 2),\n",
       " (533, 1),\n",
       " (534, 1),\n",
       " (535, 1),\n",
       " (536, 1),\n",
       " (537, 1),\n",
       " (538, 1),\n",
       " (539, 1),\n",
       " (540, 1),\n",
       " (541, 2),\n",
       " (542, 1),\n",
       " (543, 1),\n",
       " (544, 1),\n",
       " (545, 1),\n",
       " (546, 1),\n",
       " (547, 1),\n",
       " (548, 1),\n",
       " (549, 8),\n",
       " (550, 1),\n",
       " (551, 1),\n",
       " (552, 1),\n",
       " (553, 1),\n",
       " (554, 1),\n",
       " (555, 2),\n",
       " (556, 1),\n",
       " (557, 1),\n",
       " (558, 1),\n",
       " (559, 9),\n",
       " (560, 1),\n",
       " (561, 1),\n",
       " (562, 1),\n",
       " (563, 1),\n",
       " (564, 1),\n",
       " (565, 2),\n",
       " (566, 1),\n",
       " (567, 1),\n",
       " (568, 1),\n",
       " (569, 2),\n",
       " (570, 1),\n",
       " (571, 1),\n",
       " (572, 1),\n",
       " (573, 1),\n",
       " (574, 1),\n",
       " (575, 1),\n",
       " (576, 1),\n",
       " (577, 1),\n",
       " (578, 1),\n",
       " (579, 1),\n",
       " (580, 1),\n",
       " (581, 1),\n",
       " (582, 1),\n",
       " (583, 1),\n",
       " (584, 1),\n",
       " (585, 2),\n",
       " (586, 1),\n",
       " (587, 1),\n",
       " (588, 1),\n",
       " (589, 1),\n",
       " (590, 1),\n",
       " (591, 1),\n",
       " (592, 1),\n",
       " (593, 1),\n",
       " (594, 1),\n",
       " (595, 1),\n",
       " (596, 1),\n",
       " (597, 1),\n",
       " (598, 1),\n",
       " (599, 1),\n",
       " (600, 1),\n",
       " (601, 1),\n",
       " (602, 1),\n",
       " (603, 1),\n",
       " (604, 1),\n",
       " (605, 1),\n",
       " (606, 1),\n",
       " (607, 6),\n",
       " (608, 1),\n",
       " (609, 4),\n",
       " (610, 1),\n",
       " (611, 1),\n",
       " (612, 1),\n",
       " (613, 1),\n",
       " (614, 2),\n",
       " (615, 1),\n",
       " (616, 3),\n",
       " (617, 2),\n",
       " (618, 2),\n",
       " (619, 1),\n",
       " (620, 1),\n",
       " (621, 1),\n",
       " (622, 1),\n",
       " (623, 1),\n",
       " (624, 1),\n",
       " (625, 1),\n",
       " (626, 1),\n",
       " (627, 1),\n",
       " (628, 1),\n",
       " (629, 1),\n",
       " (630, 1),\n",
       " (631, 1),\n",
       " (632, 1),\n",
       " (633, 1),\n",
       " (634, 1),\n",
       " (635, 1),\n",
       " (636, 1),\n",
       " (637, 1),\n",
       " (638, 1),\n",
       " (639, 1),\n",
       " (640, 1),\n",
       " (641, 2),\n",
       " (642, 1),\n",
       " (643, 1),\n",
       " (644, 1),\n",
       " (645, 1),\n",
       " (646, 2),\n",
       " (647, 1),\n",
       " (648, 2),\n",
       " (649, 1),\n",
       " (650, 2),\n",
       " (651, 1),\n",
       " (652, 1),\n",
       " (653, 1),\n",
       " (654, 1),\n",
       " (655, 1),\n",
       " (656, 1),\n",
       " (657, 1),\n",
       " (658, 1),\n",
       " (659, 1),\n",
       " (660, 1),\n",
       " (661, 1),\n",
       " (662, 1),\n",
       " (663, 1),\n",
       " (664, 1),\n",
       " (665, 1),\n",
       " (666, 1),\n",
       " (667, 1),\n",
       " (668, 1),\n",
       " (669, 1),\n",
       " (670, 1),\n",
       " (671, 1),\n",
       " (672, 1),\n",
       " (673, 1),\n",
       " (674, 1),\n",
       " (675, 1),\n",
       " (676, 1),\n",
       " (677, 1),\n",
       " (678, 1),\n",
       " (679, 1),\n",
       " (680, 1),\n",
       " (681, 1),\n",
       " (682, 1),\n",
       " (683, 4),\n",
       " (684, 2),\n",
       " (685, 4),\n",
       " (686, 2),\n",
       " (687, 5),\n",
       " (688, 2),\n",
       " (689, 2),\n",
       " (690, 1),\n",
       " (691, 1),\n",
       " (692, 1),\n",
       " (693, 1),\n",
       " (694, 2),\n",
       " (695, 1),\n",
       " (696, 1),\n",
       " (697, 1),\n",
       " (698, 2),\n",
       " (699, 1),\n",
       " (700, 1),\n",
       " (701, 1),\n",
       " (702, 1),\n",
       " (703, 1),\n",
       " (704, 1),\n",
       " (705, 1),\n",
       " (706, 1),\n",
       " (707, 1),\n",
       " (708, 1),\n",
       " (709, 1),\n",
       " (710, 1),\n",
       " (711, 1),\n",
       " (712, 1),\n",
       " (713, 1),\n",
       " (714, 1),\n",
       " (715, 1),\n",
       " (716, 1),\n",
       " (717, 1),\n",
       " (718, 2),\n",
       " (719, 1),\n",
       " (720, 1),\n",
       " (721, 1),\n",
       " (722, 1),\n",
       " (723, 1),\n",
       " (724, 1),\n",
       " (725, 1),\n",
       " (726, 1),\n",
       " (727, 1),\n",
       " (728, 1),\n",
       " (729, 1),\n",
       " (730, 1),\n",
       " (731, 1),\n",
       " (732, 2),\n",
       " (733, 1),\n",
       " (734, 1),\n",
       " (735, 1),\n",
       " (736, 1),\n",
       " (737, 1),\n",
       " (738, 1),\n",
       " (739, 1),\n",
       " (740, 1),\n",
       " (741, 1),\n",
       " (742, 2),\n",
       " (743, 1),\n",
       " (744, 1),\n",
       " (745, 1),\n",
       " (746, 1),\n",
       " (747, 1),\n",
       " (748, 1),\n",
       " (749, 1),\n",
       " (750, 2),\n",
       " (751, 1),\n",
       " (752, 1),\n",
       " (753, 1),\n",
       " (754, 1),\n",
       " (755, 1),\n",
       " (756, 1),\n",
       " (757, 2),\n",
       " (758, 1),\n",
       " (759, 1),\n",
       " (760, 1),\n",
       " (761, 1),\n",
       " (762, 1),\n",
       " (763, 1),\n",
       " (764, 1),\n",
       " (765, 1),\n",
       " (766, 1),\n",
       " (767, 2),\n",
       " (768, 1),\n",
       " (769, 1),\n",
       " (770, 1),\n",
       " (771, 1),\n",
       " (772, 1),\n",
       " (773, 2),\n",
       " (774, 1),\n",
       " (775, 1),\n",
       " (776, 1),\n",
       " (777, 1),\n",
       " (778, 1),\n",
       " (779, 1),\n",
       " (780, 1),\n",
       " (781, 2),\n",
       " (782, 1),\n",
       " (783, 1),\n",
       " (784, 1),\n",
       " (785, 1),\n",
       " (786, 1),\n",
       " (787, 1),\n",
       " (788, 1),\n",
       " (789, 1),\n",
       " (790, 1),\n",
       " (791, 1),\n",
       " (792, 1),\n",
       " (793, 1),\n",
       " (794, 1),\n",
       " (795, 1),\n",
       " (796, 1),\n",
       " (797, 1),\n",
       " (798, 1),\n",
       " (799, 1),\n",
       " (800, 1),\n",
       " (801, 1),\n",
       " (802, 1),\n",
       " (803, 1),\n",
       " (804, 3),\n",
       " (805, 1),\n",
       " (806, 1),\n",
       " (807, 1),\n",
       " (808, 1),\n",
       " (809, 1),\n",
       " (810, 1),\n",
       " (811, 2),\n",
       " (812, 1),\n",
       " (813, 1),\n",
       " (814, 1),\n",
       " (815, 1),\n",
       " (816, 1),\n",
       " (817, 1),\n",
       " (818, 1),\n",
       " (819, 1),\n",
       " (820, 1),\n",
       " (821, 3),\n",
       " (822, 1),\n",
       " (823, 1),\n",
       " (824, 1),\n",
       " (825, 1),\n",
       " (826, 1),\n",
       " (827, 1),\n",
       " (828, 1),\n",
       " (829, 1),\n",
       " (830, 1),\n",
       " (831, 1),\n",
       " (832, 1),\n",
       " (833, 1),\n",
       " (834, 1),\n",
       " (835, 1),\n",
       " (836, 1),\n",
       " (837, 1),\n",
       " (838, 1),\n",
       " (839, 1),\n",
       " (840, 1),\n",
       " (841, 1),\n",
       " (842, 1),\n",
       " (843, 1),\n",
       " (844, 2),\n",
       " (845, 1),\n",
       " (846, 1),\n",
       " (847, 1),\n",
       " (848, 1),\n",
       " (849, 1),\n",
       " (850, 1),\n",
       " (851, 6),\n",
       " (852, 1),\n",
       " (853, 1),\n",
       " (854, 1),\n",
       " (855, 1),\n",
       " (856, 1),\n",
       " (857, 1),\n",
       " (858, 1),\n",
       " (859, 1),\n",
       " (860, 1),\n",
       " (861, 1),\n",
       " (862, 1),\n",
       " (863, 1),\n",
       " (864, 1),\n",
       " (865, 1),\n",
       " (866, 1),\n",
       " (867, 1),\n",
       " (868, 1),\n",
       " (869, 1),\n",
       " (870, 1),\n",
       " (871, 1),\n",
       " (872, 1),\n",
       " (873, 1),\n",
       " (874, 1),\n",
       " (875, 1),\n",
       " (876, 1),\n",
       " (877, 1),\n",
       " (878, 1),\n",
       " (879, 1),\n",
       " (880, 2),\n",
       " (881, 1),\n",
       " (882, 1),\n",
       " (883, 1),\n",
       " (884, 3),\n",
       " (885, 1),\n",
       " (886, 1),\n",
       " (887, 1),\n",
       " (888, 1),\n",
       " (889, 1),\n",
       " (890, 1),\n",
       " (891, 1),\n",
       " (892, 1),\n",
       " (893, 3),\n",
       " (894, 1),\n",
       " (895, 1),\n",
       " (896, 1),\n",
       " (897, 1),\n",
       " (898, 1),\n",
       " (899, 1),\n",
       " (900, 1),\n",
       " (901, 1),\n",
       " (902, 1),\n",
       " (903, 1),\n",
       " (904, 1),\n",
       " (905, 1),\n",
       " (906, 1),\n",
       " (907, 1),\n",
       " (908, 1),\n",
       " (909, 1),\n",
       " (910, 1),\n",
       " (911, 2),\n",
       " (912, 1),\n",
       " (913, 2),\n",
       " (914, 1),\n",
       " (915, 1),\n",
       " (916, 1),\n",
       " (917, 3),\n",
       " (918, 1),\n",
       " (919, 1),\n",
       " (920, 1),\n",
       " (921, 1),\n",
       " (922, 1),\n",
       " (923, 1),\n",
       " (924, 1),\n",
       " (925, 1),\n",
       " (926, 1),\n",
       " (927, 1),\n",
       " (928, 1),\n",
       " (929, 1),\n",
       " (930, 1),\n",
       " (931, 2),\n",
       " (932, 1),\n",
       " (933, 1),\n",
       " (934, 1),\n",
       " (935, 1),\n",
       " (936, 1),\n",
       " (937, 1),\n",
       " (938, 1),\n",
       " (939, 1),\n",
       " (940, 1),\n",
       " (941, 1),\n",
       " (942, 2),\n",
       " (943, 1),\n",
       " (944, 1),\n",
       " (945, 1),\n",
       " (946, 1),\n",
       " (947, 1),\n",
       " (948, 1),\n",
       " (949, 1),\n",
       " (950, 1),\n",
       " (951, 1),\n",
       " (952, 1),\n",
       " (953, 1),\n",
       " (954, 9),\n",
       " (955, 1),\n",
       " (956, 1),\n",
       " (957, 1),\n",
       " (958, 1),\n",
       " (959, 1),\n",
       " (960, 1),\n",
       " (961, 1),\n",
       " (962, 1),\n",
       " (963, 1),\n",
       " (964, 1),\n",
       " (965, 1),\n",
       " (966, 1),\n",
       " (967, 1),\n",
       " (968, 1),\n",
       " (969, 2),\n",
       " (970, 1),\n",
       " (971, 1),\n",
       " (972, 1),\n",
       " (973, 1),\n",
       " (974, 2),\n",
       " (975, 1),\n",
       " (976, 1),\n",
       " (977, 1),\n",
       " (978, 2),\n",
       " (979, 1),\n",
       " (980, 1),\n",
       " (981, 1),\n",
       " (982, 1),\n",
       " (983, 1),\n",
       " (984, 1),\n",
       " (985, 1),\n",
       " (986, 3),\n",
       " (987, 1),\n",
       " (988, 1),\n",
       " (989, 1),\n",
       " (990, 1),\n",
       " (991, 1),\n",
       " (992, 1),\n",
       " (993, 1),\n",
       " (994, 2),\n",
       " (995, 1),\n",
       " (996, 1),\n",
       " (997, 1),\n",
       " (998, 1),\n",
       " (999, 1),\n",
       " ...]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=10, id2word=dictionary, passes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, u'0.001*\"reinforcement_learning\" + 0.001*\"et_al\" + 0.001*\"s_a\" + 0.001*\"1_2\" + 0.000*\"machine_learning\" + 0.000*\"value_function\" + 0.000*\"reward_function\" + 0.000*\"0_1\" + 0.000*\"0_0\" + 0.000*\"neural_network\"')\n",
      "(1, u'0.001*\"1_1\" + 0.001*\"et_al\" + 0.001*\"0_1\" + 0.000*\"1_2\" + 0.000*\"machine_learning\" + 0.000*\"x_x\" + 0.000*\"figure_1\" + 0.000*\"2_2\" + 0.000*\"k_k\" + 0.000*\"figure_2\"')\n",
      "(2, u'0.001*\"1_1\" + 0.001*\"1_2\" + 0.001*\"machine_learning\" + 0.001*\"et_al\" + 0.001*\"0_0\" + 0.001*\"x_x\" + 0.001*\"neural_information\" + 0.001*\"2_2\" + 0.001*\"0_1\" + 0.001*\"information_processing\"')\n",
      "(3, u'0.001*\"et_al\" + 0.001*\"machine_learning\" + 0.001*\"1_2\" + 0.001*\"x_x\" + 0.001*\"1_1\" + 0.001*\"0_1\" + 0.001*\"processing_system\" + 0.001*\"0_0\" + 0.001*\"neural_network\" + 0.000*\"figure_1\"')\n",
      "(4, u'0.001*\"et_al\" + 0.001*\"x_x\" + 0.001*\"1_2\" + 0.001*\"0_0\" + 0.001*\"information_processing\" + 0.001*\"processing_system\" + 0.001*\"0_1\" + 0.000*\"machine_learning\" + 0.000*\"neural_network\" + 0.000*\"gaussian_process\"')\n",
      "(5, u'0.001*\"et_al\" + 0.001*\"neural_network\" + 0.000*\"figure_2\" + 0.000*\"computer_vision\" + 0.000*\"arxiv_preprint\" + 0.000*\"1_2\" + 0.000*\"information_processing\" + 0.000*\"figure_3\" + 0.000*\"figure_1\" + 0.000*\"ground_truth\"')\n",
      "(6, u'0.003*\"neural_network\" + 0.001*\"et_al\" + 0.001*\"0_0\" + 0.001*\"hidden_unit\" + 0.001*\"1_1\" + 0.001*\"1_2\" + 0.001*\"figure_1\" + 0.001*\"information_processing\" + 0.001*\"figure_2\" + 0.000*\"figure_3\"')\n",
      "(7, u'0.002*\"neural_network\" + 0.001*\"et_al\" + 0.001*\"neural_information\" + 0.001*\"figure_1\" + 0.000*\"1_2\" + 0.000*\"0_0\" + 0.000*\"information_processing\" + 0.000*\"figure_2\" + 0.000*\"table_1\" + 0.000*\"training_set\"')\n",
      "(8, u'0.001*\"neural_network\" + 0.001*\"et_al\" + 0.001*\"machine_learning\" + 0.001*\"1_1\" + 0.001*\"0_1\" + 0.001*\"0_0\" + 0.001*\"1_2\" + 0.001*\"training_set\" + 0.000*\"table_1\" + 0.000*\"x_x\"')\n",
      "(9, u'0.001*\"f_x\" + 0.001*\"1_2\" + 0.001*\"1_1\" + 0.001*\"0_1\" + 0.001*\"2_2\" + 0.001*\"x_x\" + 0.001*\"machine_learning\" + 0.001*\"et_al\" + 0.001*\"lower_bound\" + 0.001*\"loss_function\"')\n"
     ]
    }
   ],
   "source": [
    "for t in ldamodel.print_topics(num_topics=30, num_words=10):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save model\n",
    "# ldamodel.save('nips.bigrams')\n",
    "\n",
    "#Load model\n",
    "ldamodel = Lda.load('nips.bigrams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, u'0.001*\"reinforcement_learning\" + 0.001*\"et_al\" + 0.001*\"s_a\" + 0.001*\"1_2\" + 0.000*\"machine_learning\"')\n",
      "(1, u'0.001*\"1_1\" + 0.001*\"et_al\" + 0.001*\"0_1\" + 0.000*\"1_2\" + 0.000*\"machine_learning\"')\n",
      "(2, u'0.001*\"1_1\" + 0.001*\"1_2\" + 0.001*\"machine_learning\" + 0.001*\"et_al\" + 0.001*\"0_0\"')\n",
      "(3, u'0.001*\"et_al\" + 0.001*\"machine_learning\" + 0.001*\"1_2\" + 0.001*\"x_x\" + 0.001*\"1_1\"')\n",
      "(4, u'0.001*\"et_al\" + 0.001*\"x_x\" + 0.001*\"1_2\" + 0.001*\"0_0\" + 0.001*\"information_processing\"')\n",
      "(5, u'0.001*\"et_al\" + 0.001*\"neural_network\" + 0.000*\"figure_2\" + 0.000*\"computer_vision\" + 0.000*\"arxiv_preprint\"')\n",
      "(6, u'0.003*\"neural_network\" + 0.001*\"et_al\" + 0.001*\"0_0\" + 0.001*\"hidden_unit\" + 0.001*\"1_1\"')\n",
      "(7, u'0.002*\"neural_network\" + 0.001*\"et_al\" + 0.001*\"neural_information\" + 0.001*\"figure_1\" + 0.000*\"1_2\"')\n",
      "(8, u'0.001*\"neural_network\" + 0.001*\"et_al\" + 0.001*\"machine_learning\" + 0.001*\"1_1\" + 0.001*\"0_1\"')\n",
      "(9, u'0.001*\"f_x\" + 0.001*\"1_2\" + 0.001*\"1_1\" + 0.001*\"0_1\" + 0.001*\"2_2\"')\n"
     ]
    }
   ],
   "source": [
    "for t in ldamodel.print_topics(num_topics=10, num_words=5):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://www.kaggle.com/mrisdal/fake-news/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6852     Comments \\nRepublican nominee Donald Trump is ...\n",
       "3557     It is no longer a question of whether or not f...\n",
       "6654     Tuesday 1 November 2016 by James W School to t...\n",
       "12753    posted by Eddie One of the world’s largest tra...\n",
       "51       November 13, 2016 By 21wire Leave a Comment \\n...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ds = pd.read_csv('fake.csv', usecols = ['text'])\n",
    "ds.dropna(axis=0, inplace=True, subset=['text'])\n",
    "ds = ds.sample(frac=1.0)\n",
    "ds['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word.decode('utf-8')) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(doc).split() for doc in ds['text']]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'longer',\n",
       " u'question',\n",
       " u'whether',\n",
       " u'financial',\n",
       " u'market',\n",
       " u'u',\n",
       " u'economy',\n",
       " u'collapse',\n",
       " u'that',\n",
       " u'according',\n",
       " u'host',\n",
       " u'expert',\n",
       " u'mainstream',\n",
       " u'alternative',\n",
       " u'given',\n",
       " u'question',\n",
       " u'\\u201cwhen\\u201d',\n",
       " u'moment',\n",
       " u'come',\n",
       " u'according',\n",
       " u'christine',\n",
       " u'hughes',\n",
       " u'chief',\n",
       " u'investment',\n",
       " u'strategist',\n",
       " u'otterwood',\n",
       " u'capital',\n",
       " u'soon',\n",
       " u'basing',\n",
       " u'assessment',\n",
       " u'historically',\n",
       " u'deadon',\n",
       " u'yield',\n",
       " u'curve',\n",
       " u'analysis',\n",
       " u'hughes',\n",
       " u'say',\n",
       " u'latest',\n",
       " u'update',\n",
       " u'client',\n",
       " u'we\\u2019re',\n",
       " u'looking',\n",
       " u'maximum',\n",
       " u'breaking',\n",
       " u'point',\n",
       " u'2020',\n",
       " u'time',\n",
       " u'next',\n",
       " u'12',\n",
       " u'\\u2013',\n",
       " u'15',\n",
       " u'month',\n",
       " u'likely',\n",
       " u'scenario',\n",
       " u'peg',\n",
       " u'next',\n",
       " u'crisis',\n",
       " u'right',\n",
       " u'beginning',\n",
       " u'2018',\n",
       " u'first',\n",
       " u'chart',\n",
       " u'near',\n",
       " u'perfect',\n",
       " u'accuracy',\n",
       " u'thus',\n",
       " u'far',\n",
       " u'show',\n",
       " u'rapidly',\n",
       " u'yield',\n",
       " u'curve',\n",
       " u'collapsed',\n",
       " u'last',\n",
       " u'12',\n",
       " u'month',\n",
       " u'hughes',\n",
       " u'explains',\n",
       " u'mean',\n",
       " u'expect',\n",
       " u'2018',\n",
       " u'year',\n",
       " u'reckoning',\n",
       " u'bond',\n",
       " u'market',\n",
       " u'see',\n",
       " u'recession',\n",
       " u'slower',\n",
       " u'growth',\n",
       " u'mean',\n",
       " u'lower',\n",
       " u'interest',\n",
       " u'rate',\n",
       " u'the',\n",
       " u'yield',\n",
       " u'curve',\n",
       " u'collapse',\n",
       " u'let\\u2019s',\n",
       " u'assume',\n",
       " u'we\\u2019re',\n",
       " u'like',\n",
       " u'every',\n",
       " u'time',\n",
       " u'history',\n",
       " u'happens',\n",
       " u'move',\n",
       " u'forward',\n",
       " u'2018\\u2026',\n",
       " u'so',\n",
       " u'2018',\n",
       " u'according',\n",
       " u'yield',\n",
       " u'curve',\n",
       " u'pretty',\n",
       " u'much',\n",
       " u'last',\n",
       " u'gasp',\n",
       " u'economic',\n",
       " u'cycle',\n",
       " u'we\\u2019re',\n",
       " u'closing',\n",
       " u'2016',\n",
       " u'now\\u2026',\n",
       " u'basically',\n",
       " u'year\\u2026',\n",
       " u'maybe',\n",
       " u'year',\n",
       " u'15',\n",
       " u'month',\n",
       " u'next',\n",
       " u'crisis',\n",
       " u'hand',\n",
       " u'levered',\n",
       " u'personally',\n",
       " u'corporately\\u2026',\n",
       " u'lot',\n",
       " u'asset',\n",
       " u'illiquid',\n",
       " u'stuff\\u2026',\n",
       " u'canadian',\n",
       " u'housing',\n",
       " u'market',\n",
       " u'come',\n",
       " u'mind\\u2026',\n",
       " u'might',\n",
       " u'want',\n",
       " u'think',\n",
       " u'existing',\n",
       " u'liquefying',\n",
       " u'yourself',\n",
       " u'watch',\n",
       " u'video',\n",
       " u'report',\n",
       " u'wolf',\n",
       " u'richter',\n",
       " u'wolf',\n",
       " u'street',\n",
       " u'explains',\n",
       " u'treasury',\n",
       " u'yield',\n",
       " u'curve',\n",
       " u'important',\n",
       " u'since',\n",
       " u'early',\n",
       " u'july',\n",
       " u'30year',\n",
       " u'u',\n",
       " u'treasury',\n",
       " u'bond',\n",
       " u'price',\n",
       " u'index',\n",
       " u'plunged',\n",
       " u'83',\n",
       " u'it\\u2019s',\n",
       " u'called',\n",
       " u'\\u201cthe',\n",
       " u'rout\\u201d',\n",
       " u'longerdated',\n",
       " u'government',\n",
       " u'bond',\n",
       " u'one',\n",
       " u'specter',\n",
       " u'rising',\n",
       " u'inflation',\n",
       " u'time',\n",
       " u'ultralow',\n",
       " u'yield',\n",
       " u'become',\n",
       " u'number',\n",
       " u'one',\n",
       " u'predictor',\n",
       " u'bear',\n",
       " u'market',\n",
       " u'stock',\n",
       " u'past',\n",
       " u'many',\n",
       " u'decade',\n",
       " u'u',\n",
       " u'treasury',\n",
       " u'yield',\n",
       " u'curve',\n",
       " u'drive',\n",
       " u'bank',\n",
       " u'lending',\n",
       " u'\\u2013',\n",
       " u'strangle',\n",
       " u'economy',\n",
       " u'time',\n",
       " u'risk',\n",
       " u'much',\n",
       " u'higher',\n",
       " u'potential',\n",
       " u'economic',\n",
       " u'consequence',\n",
       " u'steeper',\n",
       " u'know',\n",
       " u'matter',\n",
       " u'time',\n",
       " u'point',\n",
       " u'greg',\n",
       " u'mannarino',\n",
       " u'trader',\n",
       " u'choice',\n",
       " u'made',\n",
       " u'similar',\n",
       " u'warning',\n",
       " u'noting',\n",
       " u'bond',\n",
       " u'market',\n",
       " u'signaling',\n",
       " u'massive',\n",
       " u'crash',\n",
       " u'ahead',\n",
       " u'crash',\n",
       " u'finally',\n",
       " u'take',\n",
       " u'place',\n",
       " u'fall',\n",
       " u'debt',\n",
       " u'bubble',\n",
       " u'burst',\n",
       " u'according',\n",
       " u'mannarino',\n",
       " u'could',\n",
       " u'lead',\n",
       " u'extremely',\n",
       " u'serious',\n",
       " u'consequence',\n",
       " u'so',\n",
       " u'debt',\n",
       " u'bubble',\n",
       " u'burst',\n",
       " u'we\\u2019re',\n",
       " u'going',\n",
       " u'get',\n",
       " u'correction',\n",
       " u'population',\n",
       " u'it\\u2019s',\n",
       " u'mathematical',\n",
       " u'certainty',\n",
       " u'million',\n",
       " u'upon',\n",
       " u'million',\n",
       " u'people',\n",
       " u'going',\n",
       " u'die',\n",
       " u'worldwide',\n",
       " u'scale',\n",
       " u'debt',\n",
       " u'bubble',\n",
       " u'burst',\n",
       " u'i\\u2019m',\n",
       " u'saying',\n",
       " u'if\\u2026',\n",
       " u'\\u2026',\n",
       " u'resource',\n",
       " u'become',\n",
       " u'scarce',\n",
       " u'we\\u2019re',\n",
       " u'going',\n",
       " u'see',\n",
       " u'country',\n",
       " u'war',\n",
       " u'other',\n",
       " u'people',\n",
       " u'scrambling\\u2026',\n",
       " u'worst',\n",
       " u'case',\n",
       " u'scenario\\u2026',\n",
       " u'everything',\n",
       " u'survive\\u2026',\n",
       " u'provide',\n",
       " u'family',\n",
       " u'themselves',\n",
       " u'there\\u2019s',\n",
       " u'way',\n",
       " u'it',\n",
       " u'mannarino',\n",
       " u'hughes',\n",
       " u'right',\n",
       " u'year',\n",
       " u'get',\n",
       " u'ready',\n",
       " u'next',\n",
       " u'leg',\n",
       " u'collapse',\n",
       " u'article',\n",
       " u'reposted',\n",
       " u'permission',\n",
       " u'shtf',\n",
       " u'plan',\n",
       " u'dont',\n",
       " u'forget',\n",
       " u'like',\n",
       " u'freedom',\n",
       " u'outpost',\n",
       " u'facebook',\n",
       " u'google',\n",
       " u'plus',\n",
       " u'twitter',\n",
       " u'also',\n",
       " u'get',\n",
       " u'freedom',\n",
       " u'outpost',\n",
       " u'delivered',\n",
       " u'amazon',\n",
       " u'kindle',\n",
       " u'device']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_clean[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "# use nltk fdist to get a frequency distribution of all words\n",
    "fdist = FreqDist(word for d in doc_clean for word in d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "213611"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'pontevedra', 4),\n",
       " (u'bayou', 4),\n",
       " (u'boomed', 4),\n",
       " (u'przeciwko', 4),\n",
       " (u'tazed', 4),\n",
       " (u'cabinet\\u2019s', 4),\n",
       " (u'permettra', 4),\n",
       " (u'\\u0442\\u043e\\u043b\\u0447\\u043a\\u043e\\u0432', 4),\n",
       " (u'\\u043f\\u0440\\u0438\\u0432\\u043b\\u0435\\u0447\\u044c', 4),\n",
       " (u'suburbia', 4)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 50000\n",
    "top_k_words = fdist.most_common(k)\n",
    "top_k_words[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'oak', 27),\n",
       " (u'trustworthy', 27),\n",
       " (u'\\u0432\\u0430\\u043c', 27),\n",
       " (u'ersten', 27),\n",
       " (u'22nd', 27),\n",
       " (u'aspiring', 27),\n",
       " (u'scoundrel', 27),\n",
       " (u'lao', 27),\n",
       " (u'\\u042f\\u043f\\u043e\\u043d\\u0438\\u0438', 27),\n",
       " (u'don\\xe2\\u20ac\\u2122t', 27)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 15000\n",
    "top_k_words = fdist.most_common(k)\n",
    "top_k_words[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_k_words = dict(top_k_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_clean_freqs = [[w for w in doc if w in top_k_words] for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'comment',\n",
       " u'republican',\n",
       " u'nominee',\n",
       " u'donald',\n",
       " u'trump',\n",
       " u'admitted',\n",
       " u'serial',\n",
       " u'sexual',\n",
       " u'predator',\n",
       " u'recorded',\n",
       " u'word',\n",
       " u'confirm',\n",
       " u'much',\n",
       " u'dozen',\n",
       " u'woman',\n",
       " u'come',\n",
       " u'forward',\n",
       " u'accuse',\n",
       " u'sexual',\n",
       " u'misconduct',\n",
       " u'campaign',\n",
       " u'desperately',\n",
       " u'fighting',\n",
       " u'put',\n",
       " u'lid',\n",
       " u'growing',\n",
       " u'awareness',\n",
       " u'trump',\n",
       " u'testifying',\n",
       " u'oath',\n",
       " u'trial',\n",
       " u'federal',\n",
       " u'court',\n",
       " u'accusation',\n",
       " u'raping',\n",
       " u'girl',\n",
       " u'previously',\n",
       " u'undisclosed',\n",
       " u'second',\n",
       " u'girl',\n",
       " u'even',\n",
       " u'younger',\n",
       " u'case',\n",
       " u'thrown',\n",
       " u'may',\n",
       " u'due',\n",
       " u'error',\n",
       " u'june',\n",
       " u'two',\n",
       " u'new',\n",
       " u'witness',\n",
       " u'say',\n",
       " u'worked',\n",
       " u'convicted',\n",
       " u'child',\n",
       " u'rapist',\n",
       " u'billionaire',\n",
       " u'epstein',\n",
       " u'part',\n",
       " u'appears',\n",
       " u'girl',\n",
       " u'party',\n",
       " u'revealed',\n",
       " u'deposition',\n",
       " u'convinced',\n",
       " u'victim',\n",
       " u'attend',\n",
       " u'four',\n",
       " u'different',\n",
       " u'party',\n",
       " u'promise',\n",
       " u'money',\n",
       " u'industry',\n",
       " u'\\u2013',\n",
       " u'story',\n",
       " u'match',\n",
       " u'account',\n",
       " u'another',\n",
       " u'person',\n",
       " u'would',\n",
       " u'arrange',\n",
       " u'underage',\n",
       " u'woman',\n",
       " u'attend',\n",
       " u'trump\\u2019s',\n",
       " u'party',\n",
       " u'one',\n",
       " u'party',\n",
       " u'forced',\n",
       " u'perform',\n",
       " u'oral',\n",
       " u'sex',\n",
       " u'trump',\n",
       " u'old',\n",
       " u'named',\n",
       " u'fourth',\n",
       " u'final',\n",
       " u'encounter',\n",
       " u'tiffany',\n",
       " u'doe',\n",
       " u'say',\n",
       " u'\\u201c',\n",
       " u'personally',\n",
       " u'witnessed',\n",
       " u'defendant',\n",
       " u'trump',\n",
       " u'telling',\n",
       " u'plaintiff',\n",
       " u'shouldn\\u2019t',\n",
       " u'ever',\n",
       " u'say',\n",
       " u'anything',\n",
       " u'didn\\u2019t',\n",
       " u'want',\n",
       " u'disappear',\n",
       " u'like',\n",
       " u'12yearold',\n",
       " u'female',\n",
       " u'maria',\n",
       " u'capable',\n",
       " u'whole',\n",
       " u'family',\n",
       " u'hadn\\u2019t',\n",
       " u'seen',\n",
       " u'since',\n",
       " u'previous',\n",
       " u'encounter',\n",
       " u'trump',\n",
       " u'forcibly',\n",
       " u'raped',\n",
       " u'raped',\n",
       " u'epstein',\n",
       " u'apparently',\n",
       " u'furious',\n",
       " u'trump',\n",
       " u'ta',\n",
       " u'ken',\n",
       " u'him',\n",
       " u'beating',\n",
       " u'fury',\n",
       " u'donald',\n",
       " u'trump',\n",
       " u'told',\n",
       " u'new',\n",
       " u'york',\n",
       " u'magazine',\n",
       " u'2002',\n",
       " u'\\u201ci\\u2019ve',\n",
       " u'known',\n",
       " u'jeff',\n",
       " u'15',\n",
       " u'year',\n",
       " u'terrific',\n",
       " u'guy',\n",
       " u'he\\u2019s',\n",
       " u'lot',\n",
       " u'fun',\n",
       " u'with',\n",
       " u'even',\n",
       " u'said',\n",
       " u'like',\n",
       " u'beautiful',\n",
       " u'woman',\n",
       " u'much',\n",
       " u'do',\n",
       " u'many',\n",
       " u'younger',\n",
       " u'side',\n",
       " u'doubt',\n",
       " u'\\u2014',\n",
       " u'jeffrey',\n",
       " u'enjoys',\n",
       " u'social',\n",
       " u'life\\u201d',\n",
       " u'epstein\\u2019s',\n",
       " u'brother',\n",
       " u'mark',\n",
       " u'also',\n",
       " u'testified',\n",
       " u'\\u201ctrump',\n",
       " u'flew',\n",
       " u'private',\n",
       " u'jet',\n",
       " u'least',\n",
       " u'record',\n",
       " u'indicate',\n",
       " u'trump',\n",
       " u'called',\n",
       " u'epstein',\n",
       " u'twice',\n",
       " u'november',\n",
       " u'epstein',\n",
       " u'eventually',\n",
       " u'slap',\n",
       " u'wrist',\n",
       " u'\\u2013',\n",
       " u'13',\n",
       " u'month',\n",
       " u'prison',\n",
       " u'registration',\n",
       " u'sex',\n",
       " u'offender',\n",
       " u'\\u2013',\n",
       " u'decade',\n",
       " u'abuse',\n",
       " u'\\u201caccording',\n",
       " u'lawenforcement',\n",
       " u'official',\n",
       " u'alleged',\n",
       " u'victim',\n",
       " u'year',\n",
       " u'1998',\n",
       " u'possibly',\n",
       " u'even',\n",
       " u'ran',\n",
       " u'particularly',\n",
       " u'vile',\n",
       " u'pyramid',\n",
       " u'scheme',\n",
       " u'involved',\n",
       " u'paying',\n",
       " u'minor',\n",
       " u'around',\n",
       " u'200',\n",
       " u'time',\n",
       " u'perform',\n",
       " u'sexual',\n",
       " u'massage',\n",
       " u'nearly',\n",
       " u'every',\n",
       " u'day',\n",
       " u'recruit',\n",
       " u'even',\n",
       " u'younger',\n",
       " u'girl',\n",
       " u'same',\n",
       " u'\\u201cthe',\n",
       " u'do',\n",
       " u'one',\n",
       " u'said',\n",
       " u'massage',\n",
       " u'girl',\n",
       " u'young',\n",
       " u'13',\n",
       " u'told',\n",
       " u'police',\n",
       " u'instructed',\n",
       " u'get',\n",
       " u'epstein',\n",
       " u'would',\n",
       " u'them',\n",
       " u'finger',\n",
       " u'allegedly',\n",
       " u'detail',\n",
       " u'keep',\n",
       " u'coming',\n",
       " u'picture',\n",
       " u'paint',\n",
       " u'grows',\n",
       " u'darker',\n",
       " u'horrifying',\n",
       " u'day',\n",
       " u'much',\n",
       " u'evidence',\n",
       " u'america',\n",
       " u'need',\n",
       " u'many',\n",
       " u'story',\n",
       " u'need',\n",
       " u'told',\n",
       " u'word',\n",
       " u'one',\n",
       " u'rich',\n",
       " u'man',\n",
       " u'literally',\n",
       " u'dozen',\n",
       " u'account',\n",
       " u'donald',\n",
       " u'trump',\n",
       " u'sexual',\n",
       " u'predator',\n",
       " u'possible',\n",
       " u'pedophile',\n",
       " u'deserves',\n",
       " u'jail',\n",
       " u'cell',\n",
       " u'island',\n",
       " u'white',\n",
       " u'house']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_clean_freqs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "doc_bigrams = [[t1 + '_' + t2 for t1, t2 in zip(doc, doc[1:])] for doc in doc_clean_freqs]\n",
    "\n",
    "dictionary = corpora.Dictionary(doc_bigrams)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (4, 1),\n",
       " (5, 1),\n",
       " (6, 1),\n",
       " (7, 1),\n",
       " (8, 1),\n",
       " (9, 1),\n",
       " (10, 1),\n",
       " (11, 1),\n",
       " (12, 1),\n",
       " (13, 1),\n",
       " (14, 1),\n",
       " (15, 1),\n",
       " (16, 1),\n",
       " (17, 1),\n",
       " (18, 1),\n",
       " (19, 1),\n",
       " (20, 1),\n",
       " (21, 1),\n",
       " (22, 1),\n",
       " (23, 1),\n",
       " (24, 1),\n",
       " (25, 1),\n",
       " (26, 1),\n",
       " (27, 1),\n",
       " (28, 1),\n",
       " (29, 1),\n",
       " (30, 1),\n",
       " (31, 1),\n",
       " (32, 1),\n",
       " (33, 1),\n",
       " (34, 1),\n",
       " (35, 1),\n",
       " (36, 1),\n",
       " (37, 1),\n",
       " (38, 1),\n",
       " (39, 1),\n",
       " (40, 1),\n",
       " (41, 1),\n",
       " (42, 1),\n",
       " (43, 1),\n",
       " (44, 1),\n",
       " (45, 1),\n",
       " (46, 1),\n",
       " (47, 1),\n",
       " (48, 1),\n",
       " (49, 1),\n",
       " (50, 1),\n",
       " (51, 1),\n",
       " (52, 1),\n",
       " (53, 1),\n",
       " (54, 1),\n",
       " (55, 1),\n",
       " (56, 1),\n",
       " (57, 1),\n",
       " (58, 3),\n",
       " (59, 1),\n",
       " (60, 1),\n",
       " (61, 1),\n",
       " (62, 1),\n",
       " (63, 1),\n",
       " (64, 1),\n",
       " (65, 1),\n",
       " (66, 1),\n",
       " (67, 1),\n",
       " (68, 1),\n",
       " (69, 1),\n",
       " (70, 1),\n",
       " (71, 1),\n",
       " (72, 1),\n",
       " (73, 1),\n",
       " (74, 1),\n",
       " (75, 2),\n",
       " (76, 1),\n",
       " (77, 1),\n",
       " (78, 1),\n",
       " (79, 1),\n",
       " (80, 1),\n",
       " (81, 1),\n",
       " (82, 1),\n",
       " (83, 1),\n",
       " (84, 1),\n",
       " (85, 1),\n",
       " (86, 1),\n",
       " (87, 1),\n",
       " (88, 1),\n",
       " (89, 1),\n",
       " (90, 1),\n",
       " (91, 1),\n",
       " (92, 1),\n",
       " (93, 1),\n",
       " (94, 1),\n",
       " (95, 1),\n",
       " (96, 1),\n",
       " (97, 1),\n",
       " (98, 1),\n",
       " (99, 1),\n",
       " (100, 1),\n",
       " (101, 1),\n",
       " (102, 1),\n",
       " (103, 1),\n",
       " (104, 1),\n",
       " (105, 1),\n",
       " (106, 1),\n",
       " (107, 1),\n",
       " (108, 1),\n",
       " (109, 1),\n",
       " (110, 1),\n",
       " (111, 1),\n",
       " (112, 1),\n",
       " (113, 1),\n",
       " (114, 1),\n",
       " (115, 1),\n",
       " (116, 1),\n",
       " (117, 1),\n",
       " (118, 1),\n",
       " (119, 1),\n",
       " (120, 1),\n",
       " (121, 1),\n",
       " (122, 1),\n",
       " (123, 1),\n",
       " (124, 1),\n",
       " (125, 1),\n",
       " (126, 1),\n",
       " (127, 1),\n",
       " (128, 1),\n",
       " (129, 1),\n",
       " (130, 1),\n",
       " (131, 1),\n",
       " (132, 1),\n",
       " (133, 1),\n",
       " (134, 1),\n",
       " (135, 1),\n",
       " (136, 1),\n",
       " (137, 1),\n",
       " (138, 1),\n",
       " (139, 1),\n",
       " (140, 1),\n",
       " (141, 1),\n",
       " (142, 1),\n",
       " (143, 1),\n",
       " (144, 1),\n",
       " (145, 1),\n",
       " (146, 1),\n",
       " (147, 1),\n",
       " (148, 1),\n",
       " (149, 1),\n",
       " (150, 1),\n",
       " (151, 1),\n",
       " (152, 1),\n",
       " (153, 1),\n",
       " (154, 1),\n",
       " (155, 1),\n",
       " (156, 1),\n",
       " (157, 1),\n",
       " (158, 1),\n",
       " (159, 1),\n",
       " (160, 1),\n",
       " (161, 1),\n",
       " (162, 1),\n",
       " (163, 1),\n",
       " (164, 1),\n",
       " (165, 1),\n",
       " (166, 1),\n",
       " (167, 1),\n",
       " (168, 1),\n",
       " (169, 1),\n",
       " (170, 1),\n",
       " (171, 1),\n",
       " (172, 1),\n",
       " (173, 1),\n",
       " (174, 1),\n",
       " (175, 1),\n",
       " (176, 1),\n",
       " (177, 1),\n",
       " (178, 1),\n",
       " (179, 1),\n",
       " (180, 1),\n",
       " (181, 1),\n",
       " (182, 1),\n",
       " (183, 1),\n",
       " (184, 1),\n",
       " (185, 1),\n",
       " (186, 1),\n",
       " (187, 1),\n",
       " (188, 1),\n",
       " (189, 1),\n",
       " (190, 1),\n",
       " (191, 1),\n",
       " (192, 1),\n",
       " (193, 1),\n",
       " (194, 1),\n",
       " (195, 1),\n",
       " (196, 1),\n",
       " (197, 1),\n",
       " (198, 1),\n",
       " (199, 1),\n",
       " (200, 1),\n",
       " (201, 1),\n",
       " (202, 1),\n",
       " (203, 1),\n",
       " (204, 1),\n",
       " (205, 1),\n",
       " (206, 1),\n",
       " (207, 1),\n",
       " (208, 1),\n",
       " (209, 1),\n",
       " (210, 1),\n",
       " (211, 1),\n",
       " (212, 1),\n",
       " (213, 1),\n",
       " (214, 1),\n",
       " (215, 2),\n",
       " (216, 1),\n",
       " (217, 1),\n",
       " (218, 1),\n",
       " (219, 1),\n",
       " (220, 1),\n",
       " (221, 1),\n",
       " (222, 1),\n",
       " (223, 1),\n",
       " (224, 1),\n",
       " (225, 1),\n",
       " (226, 1),\n",
       " (227, 1),\n",
       " (228, 1),\n",
       " (229, 1),\n",
       " (230, 1),\n",
       " (231, 1),\n",
       " (232, 1),\n",
       " (233, 1),\n",
       " (234, 1),\n",
       " (235, 1),\n",
       " (236, 1),\n",
       " (237, 1),\n",
       " (238, 1),\n",
       " (239, 1),\n",
       " (240, 1),\n",
       " (241, 1),\n",
       " (242, 1),\n",
       " (243, 1),\n",
       " (244, 1),\n",
       " (245, 1),\n",
       " (246, 1),\n",
       " (247, 1),\n",
       " (248, 1),\n",
       " (249, 1),\n",
       " (250, 1),\n",
       " (251, 1),\n",
       " (252, 1),\n",
       " (253, 1),\n",
       " (254, 1),\n",
       " (255, 1),\n",
       " (256, 1),\n",
       " (257, 1),\n",
       " (258, 1),\n",
       " (259, 1),\n",
       " (260, 1),\n",
       " (261, 1),\n",
       " (262, 1),\n",
       " (263, 1),\n",
       " (264, 1),\n",
       " (265, 1),\n",
       " (266, 1),\n",
       " (267, 1),\n",
       " (268, 1),\n",
       " (269, 1),\n",
       " (270, 1),\n",
       " (271, 1),\n",
       " (272, 1),\n",
       " (273, 1),\n",
       " (274, 1),\n",
       " (275, 1),\n",
       " (276, 1),\n",
       " (277, 1),\n",
       " (278, 1),\n",
       " (279, 1),\n",
       " (280, 1),\n",
       " (281, 1),\n",
       " (282, 1),\n",
       " (283, 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "\n",
    "num_topics = 100\n",
    "chunksize = 300\n",
    "\n",
    "\n",
    "# low alpha means each document is only represented by a small number of topics, and vice versa\n",
    "# low eta means each topic is only represented by a small number of words, and vice versa\n",
    "\n",
    "ldamodel = Lda(\n",
    "    doc_term_matrix, \n",
    "    num_topics=num_topics, \n",
    "    id2word=dictionary, \n",
    "    alpha=1e-2, \n",
    "    eta=0.5e-2, \n",
    "    chunksize=chunksize, \n",
    "    minimum_probability=0.0, \n",
    "    passes=2, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save model\n",
    "ldamodel.save('fake.bigrams')\n",
    "\n",
    "#Load model\n",
    "# ldamodel = Lda.load('fake.bigrams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for t in ldamodel.print_topics(num_topics=100, num_words=5):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldamodel.show_topic(topicid=4, topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldamodel.get_document_topics(doc_term_matrix[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VK walls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "def get_res_arr(filename):\n",
    "    res_arr = []\n",
    "    data = json.load(open(filename))\n",
    "    for id in data:\n",
    "        res_arr.append(data[id])\n",
    "    #print(len(res_arr))\n",
    "    return res_arr\n",
    "\n",
    "path='./user_posts/'\n",
    "super_arr = []\n",
    "for filename in os.listdir(path):\n",
    "    #print(filename)\n",
    "    super_arr.extend(get_res_arr(path + filename))\n",
    "\n",
    "print(len(super_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "\n",
    "stop = set(stopwords.words('russian'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def clean(doc):\n",
    "    #print(doc)\n",
    "    doc = str(doc)\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(doc).split() for doc in super_arr]   \n",
    "print(doc_clean[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_term_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=10, id2word=dictionary, passes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ldamodel.save('vk.unigrams')\n",
    "ldamodel = Lda.load('vk.unigrams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.014*\"n\" + 0.004*\"это\" + 0.004*\"бизнес\" + 0.003*\"компании\" + 0.003*\"деньги\" + 0.003*\"1\" + 0.003*\"рублей\" + 0.002*\"10\" + 0.002*\"день\" + 0.002*\"bitcoin\"\n",
      "0.013*\"помоги\" + 0.012*\"3\" + 0.012*\"вк\" + 0.012*\"зайди\" + 0.011*\"пройти\" + 0.011*\"ссылке\" + 0.008*\"игру\" + 0.007*\"помощь\" + 0.007*\"нужна\" + 0.007*\"интерны\"\n",
      "0.024*\"с\" + 0.019*\"♥\" + 0.017*\"тебе\" + 0.016*\"рождения\" + 0.015*\"днем\" + 0.015*\"открытки\" + 0.013*\"😉\" + 0.012*\"узнай\" + 0.012*\"n\" + 0.011*\"❤\"\n",
      "0.016*\"это\" + 0.005*\"очень\" + 0.005*\"просто\" + 0.004*\"я\" + 0.004*\"тебе\" + 0.003*\"—\" + 0.003*\"всё\" + 0.002*\"хочу\" + 0.002*\"всем\" + 0.002*\"ещё\"\n",
      "0.011*\"n\" + 0.008*\"nи\" + 0.006*\"—\" + 0.004*\"это\" + 0.004*\"жизнь\" + 0.004*\"жизни\" + 0.004*\"любовь\" + 0.003*\"день\" + 0.003*\"пусть\" + 0.003*\"–\"\n",
      "0.017*\"the\" + 0.011*\"to\" + 0.011*\"a\" + 0.010*\"and\" + 0.010*\"you\" + 0.009*\"of\" + 0.009*\"i\" + 0.008*\"in\" + 0.007*\"my\" + 0.006*\"for\"\n",
      "0.006*\"і\" + 0.005*\"з\" + 0.004*\"люблю\" + 0.004*\"❤️\" + 0.003*\"😂\" + 0.003*\"спасибо\" + 0.003*\"день\" + 0.003*\"russia\" + 0.003*\"😍\" + 0.003*\"😊\"\n",
      "0.015*\"n\" + 0.012*\"это\" + 0.007*\"—\" + 0.007*\"–\" + 0.003*\"которые\" + 0.003*\"время\" + 0.003*\"жизни\" + 0.002*\"людей\" + 0.002*\"очень\" + 0.002*\"то\"\n",
      "0.006*\"спасибо\" + 0.005*\"я\" + 0.005*\"сегодня\" + 0.004*\"друзья\" + 0.004*\"всем\" + 0.004*\"видео\" + 0.004*\"новый\" + 0.004*\"очень\" + 0.004*\"фото\" + 0.003*\"день\"\n",
      "0.008*\"n╬═╬\" + 0.005*\"n\" + 0.004*\"●\" + 0.004*\"1\" + 0.003*\"2\" + 0.002*\"волос\" + 0.002*\"наличии\" + 0.002*\"г\" + 0.002*\"салат\" + 0.001*\"волосы\"\n"
     ]
    }
   ],
   "source": [
    "for t in ldamodel.print_topics(num_topics=30, num_words=10):\n",
    "    print(t[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### биграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_bigrams = [[t1 + '_' + t2 for t1, t2 in zip(doc, doc[1:])] for doc in doc_clean]\n",
    "\n",
    "dictionary = corpora.Dictionary(doc_bigrams)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=10, id2word=dictionary, passes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ldamodel.save('vk.bigrams')\n",
    "ldamodel = Lda.load('vk.bigrams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for t in ldamodel.print_topics(num_topics=30, num_words=10):\n",
    "    print(t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.machinelearning.ru/wiki/images/8/82/BMMO11_14.pdf\n",
    "http://www.machinelearning.ru/wiki/images/f/f7/DirichletProcessNotes.pdf \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
