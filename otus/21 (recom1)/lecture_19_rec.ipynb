{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='otus.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import math, random\n",
    "from collections import defaultdict, Counter\n",
    "from scipy import spatial\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Опрос в начале лекции:\n",
    "https://docs.google.com/forms/d/e/1FAIpQLScoGNFmWENp4nMoxlpdGiALl4hwcVgFHomds8ldJqgUDSRpsQ/viewform?usp=sf_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Рекомендательные системы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заинтересованность компаний в разработке рекомендательных систем:\n",
    "\n",
    "* покупка некоторого товара\n",
    "* потребление некоторого контента и удержание пользователя"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Покупка товара:\n",
    "\n",
    "* Amazon\n",
    "* Ozon\n",
    "* Lamoda\n",
    "* ...\n",
    "* Ваши варианты?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lamoda\n",
    "<img src='lamoda.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon\n",
    "<img src='amazon.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удержание пользователя:\n",
    "* Spotify\n",
    "* Netflix\n",
    "* LinkedIn\n",
    "* Facebook\n",
    "* ...\n",
    "* Ваши варианты?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinkedIn\n",
    "\n",
    "<img src='linkedin.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LastFM\n",
    "\n",
    "<img src='lastfm.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judging by Amazon’s success, the recommendation system works. The company reported a 29% sales increase to \\$12.83 billion during its second fiscal quarter, up from $9.9 billion during the same time last year. \n",
    "\n",
    "http://fortune.com/2012/07/30/amazons-recommendation-secret/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уровни работы рекомендательных систем\n",
    "\n",
    "* Долгосрочные интересы - медленно меняющиеся предпочтения\n",
    "* Краткосрочные интересы - некоторые кратковременные тренды\n",
    "\n",
    "Будем рассматривать модели для построения долгосрочных рекомендаций"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим пример из книги Data Science from scratch\n",
    "\n",
    "https://github.com/joelgrus/data-science-from-scratch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Big Data', 'C++', 'Cassandra', 'HBase', 'Hadoop', 'Haskell', 'Java', 'Mahout', 'MapReduce', 'MongoDB', 'MySQL', 'NoSQL', 'Postgres', 'Python', 'R', 'Spark', 'Storm', 'artificial intelligence', 'databases', 'decision trees', 'deep learning', 'libsvm', 'machine learning', 'mathematics', 'neural networks', 'numpy', 'pandas', 'probability', 'programming languages', 'regression', 'scikit-learn', 'scipy', 'statistics', 'statsmodels', 'support vector machines', 'theory']\n",
      "36\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# темы повторяются у разных пользователей\n",
    "\n",
    "users_interests = [\n",
    "    [\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\"],\n",
    "    [\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"],\n",
    "    [\"Python\", \"scikit-learn\", \"scipy\", \"numpy\", \"statsmodels\", \"pandas\"],\n",
    "    [\"R\", \"Python\", \"statistics\", \"regression\", \"probability\"],\n",
    "    [\"machine learning\", \"regression\", \"decision trees\", \"libsvm\"],\n",
    "    [\"Python\", \"R\", \"Java\", \"C++\", \"Haskell\", \"programming languages\"],\n",
    "    [\"statistics\", \"probability\", \"mathematics\", \"theory\"],\n",
    "    [\"machine learning\", \"scikit-learn\", \"Mahout\", \"neural networks\"],\n",
    "    [\"neural networks\", \"deep learning\", \"Big Data\", \"artificial intelligence\"],\n",
    "    [\"Hadoop\", \"Java\", \"MapReduce\", \"Big Data\"],\n",
    "    [\"statistics\", \"R\", \"statsmodels\"],\n",
    "    [\"C++\", \"deep learning\", \"artificial intelligence\", \"probability\"],\n",
    "    [\"pandas\", \"R\", \"Python\"],\n",
    "    [\"databases\", \"HBase\", \"Postgres\", \"MySQL\", \"MongoDB\"],\n",
    "    [\"libsvm\", \"regression\", \"support vector machines\"]\n",
    "]\n",
    "\n",
    "unique_interests = sorted({interest for user_interests in users_interests for interest in user_interests})\n",
    "\n",
    "print(unique_interests)\n",
    "print(len(unique_interests))\n",
    "print(len(users_interests))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если пользователей и тем мало, то справится эксперт, и систему разрабатывать не нужно.  \n",
    "Когда количество пользователей и тем/товаров исчисляется миллионами, давать рекомендацию нужно автоматически."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Популярность темы\n",
    "\n",
    "Простой подход - рекомендуем пользователю самые популярные темы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Popular Interests\n",
      "[('Python', 4), ('R', 4), ('Big Data', 3), ('HBase', 3), ('Java', 3), ('statistics', 3), ('regression', 3), ('probability', 3), ('Hadoop', 2), ('Cassandra', 2), ('MongoDB', 2), ('Postgres', 2), ('scikit-learn', 2), ('statsmodels', 2), ('pandas', 2), ('machine learning', 2), ('libsvm', 2), ('C++', 2), ('neural networks', 2), ('deep learning', 2), ('artificial intelligence', 2), ('Spark', 1), ('Storm', 1), ('NoSQL', 1), ('scipy', 1), ('numpy', 1), ('decision trees', 1), ('Haskell', 1), ('programming languages', 1), ('mathematics', 1), ('theory', 1), ('Mahout', 1), ('MapReduce', 1), ('databases', 1), ('MySQL', 1), ('support vector machines', 1)]\n"
     ]
    }
   ],
   "source": [
    "popular_interests = Counter(\n",
    "    interest for user_interests in users_interests for interest in user_interests\n",
    ").most_common()\n",
    "\n",
    "print(\"Popular Interests\")\n",
    "print(popular_interests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Popular New Interests\n",
      "already like: ['NoSQL', 'MongoDB', 'Cassandra', 'HBase', 'Postgres']\n",
      "[('Python', 4), ('R', 4), ('Big Data', 3), ('Java', 3), ('statistics', 3)]\n"
     ]
    }
   ],
   "source": [
    "def most_popular_new_interests(user_interests, max_results=5):\n",
    "    suggestions = [(interest, frequency) \n",
    "                   for interest, frequency in popular_interests\n",
    "                   if interest not in user_interests]\n",
    "    return suggestions[:max_results]\n",
    "\n",
    "\n",
    "print(\"Most Popular New Interests\")\n",
    "print(\"already like:\", [\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"])\n",
    "print(most_popular_new_interests([\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такой подход применим для пользователей, о которых ничего не известно. Так называемый \"холодный старт\".\n",
    "Почему он плохо работает для всех?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Hadoop': 1, 'Big Data': 1, 'HBase': 1, 'Java': 1, 'Spark': 1, 'Storm': 1, 'Cassandra': 1}, {'NoSQL': 1, 'MongoDB': 1, 'Cassandra': 1, 'HBase': 1, 'Postgres': 1}, {'Python': 1, 'scikit-learn': 1, 'scipy': 1, 'numpy': 1, 'statsmodels': 1, 'pandas': 1}, {'R': 1, 'Python': 1, 'statistics': 1, 'regression': 1, 'probability': 1}, {'machine learning': 1, 'regression': 1, 'decision trees': 1, 'libsvm': 1}, {'Python': 1, 'R': 1, 'Java': 1, 'C++': 1, 'Haskell': 1, 'programming languages': 1}, {'statistics': 1, 'probability': 1, 'mathematics': 1, 'theory': 1}, {'machine learning': 1, 'scikit-learn': 1, 'Mahout': 1, 'neural networks': 1}, {'neural networks': 1, 'deep learning': 1, 'Big Data': 1, 'artificial intelligence': 1}, {'Hadoop': 1, 'Java': 1, 'MapReduce': 1, 'Big Data': 1}, {'statistics': 1, 'R': 1, 'statsmodels': 1}, {'C++': 1, 'deep learning': 1, 'artificial intelligence': 1, 'probability': 1}, {'pandas': 1, 'R': 1, 'Python': 1}, {'databases': 1, 'HBase': 1, 'Postgres': 1, 'MySQL': 1, 'MongoDB': 1}, {'libsvm': 1, 'regression': 1, 'support vector machines': 1}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1.,\n",
       "        0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
       "        1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# составим матрицу интересов пользователей\n",
    "\n",
    "v = DictVectorizer(sparse=False)\n",
    "\n",
    "d = [{item: 1 for item in ui} for ui in users_interests]\n",
    "print(d)\n",
    "user_interest_matrix = v.fit_transform(d)\n",
    "\n",
    "user_interest_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Коллаборативная фильтрация на основе пользователей\n",
    "User-Based Collaborative Filtering\n",
    "\n",
    "Идея - для пользователя найти других пользователей, которые на него похожи, и предложить те темы, которые они предпочитают\n",
    "\n",
    "<img src=\"cfuser.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как посчитать похожесть пользователей?\n",
    "\n",
    "* Косинусная мера - косинус угла между векторами рейтингов. Если два вектора равнонаправлены, их косинусный коэффициент равен 1. Если направление противоположно, то -1.\n",
    "\n",
    "$$ \\text{similarity}=\\cos(\\theta )={\\mathbf {A} \\cdot \\mathbf {B}  \\over \\|\\mathbf {A} \\|_{2}\\|\\mathbf {B} \\|_{2}}={\\frac {\\sum \\limits _{i=1}^{n}{A_{i}B_{i}}}{{\\sqrt {\\sum \\limits _{i=1}^{n}{A_{i}^{2}}}}{\\sqrt {\\sum \\limits _{i=1}^{n}{B_{i}^{2}}}}}} $$\n",
    "\n",
    "В нашем случае присутствуют все \"оценки\" - пользователь интересуется темой (1) или не интересуется (0).  \n",
    "В случае отсуствия оценок - брать пересечение предметов пользователей\n",
    "\n",
    "* Коэффициент Пирсона\n",
    "\n",
    "Пусть даны две выборки $x^m=\\left( x_1, \\cdots ,x_m  \\right)$,  $y^m=\\left( y_1, \\cdots ,y_m  \\right)$\n",
    "\n",
    "$$r_{xy} = \\frac {\\sum_{i=1}^{m} \\left( x_i-\\bar{x} \\right)\\left( y_i-\\bar{y} \\right)}{\\sqrt{\\sum_{i=1}^{m} \\left( x_i-\\bar{x} \\right)^2 \\sum_{i=1}^{m} \\left( y_i-\\bar{y} \\right)^2}} = \\frac {cov(x,y)}{\\sqrt{s_x^2 s_y^2}}$$\n",
    "\n",
    "где $\\bar{x}$, $\\bar{y}$ – выборочные средние $x^m$ и $y^m$, $s_x^2$,  $s_y^2$ – выборочные дисперсии, $r_{xy} \\in \\left[-1,1\\right]$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.3380617 , 0.        , 0.        , 0.        ,\n",
       "       0.15430335, 0.        , 0.        , 0.18898224, 0.56694671,\n",
       "       0.        , 0.        , 0.        , 0.16903085, 0.        ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# User-Based Collaborative Filtering\n",
    "#\n",
    "\n",
    "user_similarities = cosine_similarity(user_interest_matrix, user_interest_matrix)\n",
    "# схожесть первого пользователя с остальными\n",
    "user_similarities[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Простейший способ рекомендации - просуммировать коэффициенты схожести пользователей по каждой теме (для нашего случая это подходит, так как оценки 1 или 0 и нам нужно ранжирование."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User based similarity\n",
      "most similar to 0\n",
      "['Hadoop', 'Big Data', 'HBase', 'Java', 'Spark', 'Storm', 'Cassandra']\n",
      "['Hadoop', 'Java', 'MapReduce', 'Big Data']\n",
      "[(9, 0.5669467095138407), (1, 0.3380617018914066), (8, 0.1889822365046136), (13, 0.1690308509457033), (5, 0.1543033499620919)]\n",
      "Suggestions for 0\n",
      "[('MapReduce', 0.5669467095138407), ('MongoDB', 0.50709255283711), ('Postgres', 0.50709255283711), ('NoSQL', 0.3380617018914066), ('neural networks', 0.1889822365046136), ('deep learning', 0.1889822365046136), ('artificial intelligence', 0.1889822365046136), ('databases', 0.1690308509457033), ('MySQL', 0.1690308509457033), ('Python', 0.1543033499620919), ('R', 0.1543033499620919), ('C++', 0.1543033499620919), ('Haskell', 0.1543033499620919), ('programming languages', 0.1543033499620919)]\n"
     ]
    }
   ],
   "source": [
    "def most_similar_users_to(user_id):\n",
    "    user = user_similarities[user_id]\n",
    "    \n",
    "    pairs = [(other_user_id, similarity)                      # find other\n",
    "             for other_user_id, similarity in enumerate(user) # users with\n",
    "             if user_id != other_user_id and similarity > 0]  # nonzero similarity\n",
    "\n",
    "    return sorted(pairs, key=lambda x: x[1], reverse=True)    # sort them most similar first \n",
    "\n",
    "\n",
    "def user_based_suggestions(user_id, include_current_interests=False):\n",
    "    # sum up the similarities\n",
    "    suggestions = defaultdict(float)\n",
    "    for other_user_id, similarity in most_similar_users_to(user_id):\n",
    "        for interest in users_interests[other_user_id]:\n",
    "            suggestions[interest] += similarity\n",
    "\n",
    "    # convert them to a sorted list\n",
    "    suggestions = sorted(suggestions.items(),\n",
    "                         key=lambda weight: weight[1],\n",
    "                         reverse=True)\n",
    "\n",
    "    # and (maybe) exclude already-interests\n",
    "    if include_current_interests:\n",
    "        return suggestions\n",
    "    else:\n",
    "        return [(suggestion, weight) \n",
    "                for suggestion, weight in suggestions\n",
    "                if suggestion not in users_interests[user_id]]\n",
    "    \n",
    "print(\"User based similarity\")\n",
    "print(\"most similar to 0\")\n",
    "print(users_interests[0])\n",
    "print(users_interests[9])\n",
    "print(most_similar_users_to(0))\n",
    "\n",
    "print(\"Suggestions for 0\")\n",
    "print(user_based_suggestions(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если нужно предсказать рейтинг (u - пользователь, i - предмет):\n",
    "\n",
    "$$\\hat{r}_{u,i} = \\bar{r_i} + \\frac{\\sum_v s_{u,v} (r_{v, i} -  \\bar{r_v})}{\\sum_v |s_{u,v}|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Замечения:\n",
    "\n",
    "1. Пользователи часто ставят пессимистичные или оптимистичные оценки, поэтому считаем $\\bar{r_u}$ - средняя оценка пользователя, и делаем поправку\n",
    "\n",
    "<img src='star_ratings.png'>\n",
    "\n",
    "2. Искать подробную информацию можно по \"GroupLens algorithm\"\n",
    "\n",
    "3. Можно не суммировать по всем пользователям, а ограничиться ближайшими соседями\n",
    "\n",
    "4. При росте размерности похожие пользователи перестают на самом деле быть похожими (проклятие размерности) и рекомендация работает плохо\n",
    "\n",
    "5. Естественно предположить, что продукты, которые любят или не любят практически все пользователи, не слишком полезны в определении ближайшего соседа. Поэтому естественно взвесить продукты по тому, как часто их уже оценивали пользователи; такая метрика называется iuf – inverse user frequency, обратная частота пользователей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Коллаборативная фильтрация по схожести предметов\n",
    "Item-Based Collaborative Filtering\n",
    "\n",
    "Будем считать сходство непосредственно между темами. После этого можно рекомендовать пользователю темы, похожие на те, которые его уже интересуют.\n",
    "\n",
    "<img src=\"content.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.        , 0.40824829, 0.33333333, 0.81649658,\n",
       "       0.        , 0.66666667, 0.        , 0.57735027, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.57735027, 0.57735027, 0.40824829, 0.        , 0.        ,\n",
       "       0.40824829, 0.        , 0.        , 0.        , 0.40824829,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Item-Based Collaborative Filtering\n",
    "#\n",
    "\n",
    "interest_user_matrix = user_interest_matrix.T\n",
    "\n",
    "interest_similarities = cosine_similarity(interest_user_matrix, interest_user_matrix)\n",
    "interest_similarities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.40824829, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 1.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.40824829, 0.        , 1.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 1.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 1.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interest_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item based similarity\n",
      "most similar to 'Big Data'\n",
      "[('Hadoop', 0.816496580927726), ('Java', 0.6666666666666669), ('MapReduce', 0.5773502691896258), ('Spark', 0.5773502691896258), ('Storm', 0.5773502691896258), ('Cassandra', 0.408248290463863), ('artificial intelligence', 0.408248290463863), ('deep learning', 0.408248290463863), ('neural networks', 0.408248290463863), ('HBase', 0.3333333333333334)]\n",
      "suggestions for user 0\n",
      "[('MapReduce', 1.861807319565799), ('MongoDB', 1.3164965809277258), ('Postgres', 1.3164965809277258), ('NoSQL', 1.2844570503761732), ('MySQL', 0.5773502691896258), ('databases', 0.5773502691896258), ('Haskell', 0.5773502691896258), ('programming languages', 0.5773502691896258), ('artificial intelligence', 0.408248290463863), ('deep learning', 0.408248290463863), ('neural networks', 0.408248290463863), ('C++', 0.408248290463863), ('Python', 0.2886751345948129), ('R', 0.2886751345948129)]\n"
     ]
    }
   ],
   "source": [
    "def most_similar_interests_to(interest_id):\n",
    "    similarities = interest_similarities[interest_id]\n",
    "    pairs = [(unique_interests[other_interest_id], similarity)\n",
    "             for other_interest_id, similarity in enumerate(similarities)\n",
    "             if interest_id != other_interest_id and similarity > 0]\n",
    "    return sorted(pairs,\n",
    "                  key=lambda similarity: similarity[1],\n",
    "                  reverse=True)\n",
    "\n",
    "def item_based_suggestions(user_id, include_current_interests=False):\n",
    "    suggestions = defaultdict(float)\n",
    "    user_interest_vector = user_interest_matrix[user_id]\n",
    "    for interest_id, is_interested in enumerate(user_interest_vector):\n",
    "        if is_interested == 1:\n",
    "            similar_interests = most_similar_interests_to(interest_id)\n",
    "            for interest, similarity in similar_interests:\n",
    "                suggestions[interest] += similarity\n",
    "\n",
    "    suggestions = sorted(suggestions.items(),\n",
    "                         key=lambda similarity: similarity[1],\n",
    "                         reverse=True)\n",
    "\n",
    "    if include_current_interests:\n",
    "        return suggestions\n",
    "    else:\n",
    "        return [(suggestion, weight) \n",
    "                for suggestion, weight in suggestions\n",
    "                if suggestion not in users_interests[user_id]]\n",
    "\n",
    "\n",
    "print(\"Item based similarity\")\n",
    "print(\"most similar to 'Big Data'\")\n",
    "print(most_similar_interests_to(0))\n",
    "\n",
    "print(\"suggestions for user 0\")\n",
    "print(item_based_suggestions(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Замечания\n",
    "* Можно считать оффлайн, так как предметы появляются редко, оценка нескольких пользователей не влияет сильно на предмет\n",
    "* Для новых предметов нужно каким-либо образом получить несколько оценок\n",
    "* Эффект пузыря - никогда не получим рекомендацию новой, не известной нам ранее темы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Общие замечания\n",
    "* Вычисления ресурсоемкие - все оценки нужно хранить в памяти\n",
    "* Иногда пользователь кликнул, но ему не понравилось (!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://grouplens.org/datasets/movielens/100k/\n",
    "\n",
    "или https://www.kaggle.com/prajitdatta/movielens-100k-dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "u.data     -- The full u data set, 100000 ratings by 943 users on 1682 items.  \n",
    "              Each user has rated at least 20 movies.  Users and items are  \n",
    "              numbered consecutively from 1.  The data is randomly  \n",
    "              ordered. This is a tab separated list of   \n",
    "\t         user id | item id | rating | timestamp.    \n",
    "              The time stamps are unix seconds since 1/1/1970 UTC     \n",
    "              \n",
    "u.item     -- Information about the items (movies); this is a tab separated\n",
    "              list of\n",
    "              movie id | movie title | release date | video release date |\n",
    "              IMDb URL | unknown | Action | Adventure | Animation |\n",
    "              Children's | Comedy | Crime | Documentary | Drama | Fantasy |\n",
    "              Film-Noir | Horror | Musical | Mystery | Romance | Sci-Fi |\n",
    "              Thriller | War | Western |\n",
    "              The last 19 fields are the genres, a 1 indicates the movie\n",
    "              is of that genre, a 0 indicates it is not; movies can be in\n",
    "              several genres at once.\n",
    "              The movie ids are the ones used in the u.data data set.\n",
    "              \n",
    "u.user     -- Demographic information about the users; this is a tab\n",
    "              separated list of\n",
    "              user id | age | gender | occupation | zip code\n",
    "              The user ids are the ones used in the u.data data set.\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise import Reader, Dataset, SVD, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ml-100k could not be found. Do you want to download it? [Y/n] "
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to download dataset from http://files.grouplens.org/datasets/movielens/ml-100k.zip...\n",
      "Done! Dataset ml-100k has been saved to C:\\Users\\1/.surprise_data/ml-100k\n",
      "Evaluating RMSE, MAE of algorithm SVD on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.9338  0.9392  0.9422  0.9319  0.9421  0.9378  0.0043  \n",
      "MAE (testset)     0.7358  0.7428  0.7441  0.7352  0.7387  0.7393  0.0036  \n",
      "Fit time          3.34    3.35    3.37    3.33    3.36    3.35    0.02    \n",
      "Test time         0.10    0.12    0.12    0.12    0.10    0.11    0.01    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_rmse': array([0.9337621 , 0.93920189, 0.94220364, 0.93190383, 0.94207721]),\n",
       " 'test_mae': array([0.73577947, 0.7427816 , 0.74408276, 0.73516385, 0.73867588]),\n",
       " 'fit_time': (3.3380048274993896,\n",
       "  3.354022741317749,\n",
       "  3.372028112411499,\n",
       "  3.3250298500061035,\n",
       "  3.357023000717163),\n",
       " 'test_time': (0.09999966621398926,\n",
       "  0.12199997901916504,\n",
       "  0.12099456787109375,\n",
       "  0.12299251556396484,\n",
       "  0.10200142860412598)}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the movielens-100k dataset (download it if needed).\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "\n",
    "# Use the famous SVD algorithm.\n",
    "algo = SVD()\n",
    "\n",
    "# Run 5-fold cross-validation and print results.\n",
    "cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\surprise\\evaluate.py:66: UserWarning: The evaluate() method is deprecated. Please use model_selection.cross_validate() instead.\n",
      "  'model_selection.cross_validate() instead.', UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\surprise\\dataset.py:193: UserWarning: Using data.split() or using load_from_folds() without using a CV iterator is now deprecated. \n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm SVD.\n",
      "\n",
      "------------\n",
      "Fold 1\n",
      "RMSE: 0.9294\n",
      "MAE:  0.7292\n",
      "------------\n",
      "Fold 2\n",
      "RMSE: 0.9402\n",
      "MAE:  0.7423\n",
      "------------\n",
      "Fold 3\n",
      "RMSE: 0.9399\n",
      "MAE:  0.7420\n",
      "------------\n",
      "Fold 4\n",
      "RMSE: 0.9306\n",
      "MAE:  0.7358\n",
      "------------\n",
      "Fold 5\n",
      "RMSE: 0.9397\n",
      "MAE:  0.7422\n",
      "------------\n",
      "------------\n",
      "Mean RMSE: 0.9360\n",
      "Mean MAE : 0.7383\n",
      "------------\n",
      "------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CaseInsensitiveDefaultDict(list,\n",
       "                           {'rmse': [0.9294025822150249,\n",
       "                             0.9401501180123344,\n",
       "                             0.9399164452047318,\n",
       "                             0.9305826158519822,\n",
       "                             0.9397186155937872],\n",
       "                            'mae': [0.7291808884208323,\n",
       "                             0.7423092717731407,\n",
       "                             0.7420277531139287,\n",
       "                             0.7358404353513306,\n",
       "                             0.742235510168149]})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd = SVD()\n",
    "evaluate(svd, data, measures=['RMSE', 'MAE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\surprise\\prediction_algorithms\\algo_base.py:51: UserWarning: train() is deprecated. Use fit() instead\n",
      "  warnings.warn('train() is deprecated. Use fit() instead', UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x13953c59550>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset = data.build_full_trainset()\n",
    "svd.train(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(uid=1, iid=302, r_ui=5, est=3.52986, details={'was_impossible': False})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd.predict(1, 302, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='mf.png'>\n",
    "\n",
    "Для любого пользователя можно предсказать оценки, которые он еще не ставил. Обучать модель нужно так, чтобы она давала наименьшие ошибки для уже известных моделей.\n",
    "\n",
    "Предсказание - скалярное произведение вектора для пользователя и вектора для объекта.\n",
    "\n",
    "При таком подходе необходимо обучить намного меньше параметров модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD напрямую применить нельзя, так как очень много отсутствующих значений (точнее можно, как-то заполнив их, но получается не очень хорошее качество).  \n",
    "Поэтому формулируется задача оптимизации по известным оценкам $K$:\n",
    "\n",
    "$$ min_{p, q} \\sum_{u,i \\in K} (r_{ui} - q_i^T p_u)^2 + \\lambda (||q_i||^2 + ||p_u||^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Оценка качества\n",
    "\n",
    "* Качество рейтингов\n",
    "    * MAE, MSE\n",
    "* Качество событий\n",
    "    * F-score, ROC-AUC, PR-AUC\n",
    "    * precision@k, recall@k\n",
    "* Качество ранжирования\n",
    "    * $DCG@k(u) = \\sum\\limits_{p=1}^k \\frac{val(i,p)}{\\log{(p+1)}}$\n",
    "    * $nDCG@k(u) = \\frac{DCG@k(u)}{\\max{(DCG@k(u))}}$\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://habrahabr.ru/company/surfingbird/blog/139518/\n",
    "2. https://d4datascience.wordpress.com/category/predictive-analytics/\n",
    "3. Data Science from scratch\n",
    "4. https://habrahabr.ru/company/yandex/blog/241455/\n",
    "5. https://www.kaggle.com/rounakbanik/movie-recommender-systems/notebook\n",
    "6. http://www.cs.ubbcluj.ro/~gabis/DocDiplome/SistemeDeRecomandare/Recommender_systems_handbook.pdf\n",
    "7. https://www.coursera.org/specializations/recommender-systems\n",
    "8. https://www.cs.umd.edu/~samir/498/Amazon-Recommendations.pdf\n",
    "9. https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf\n",
    "10. http://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
